session 01 
-------------------------------------------------------------------------------------------------------------------------------
What is DevOps?
Waterfall vs Agile vs DevOps
SDLC

create organisation in Github
create repo in  new org in github
create folder with 82s and create folder with repo name locally
Execute the below commands to  intialise repo in local and push to remote
--------------------------------------------------------------------------
git init
git branch -M main
git remote add origin https://github.com/DAWS-2025-82S/notes.git
git add .
git commit -m "session-01"
git push origin main

For author identity configure below
------------------------------------
git config --global user.email "rajasekharb974@gmail.com"
git config --global user.name "rajasekhar"

To unstage a new file after using git add . use the command below
git rm --cached DAWS-82S-notes.txt

Before commiting the changes if the file is modified again after git add . then to revert the changes made, use the command below
 git restore DAWS-82S-notes.txt		---  to discard changes in working directory
 git add .							--- to include the file changes
 
 git restore --staged DAWS-82S-notes.txt  -- to unstage the already existing file in staging area

Test updated in github
-------------------------------------------------------------------------------------------------------------------------------------------------
session 02 
-------------------------------------------------------------------------------------------------------------------------------------------------
What is a Computer?
Client Server Architecture

AWS account with N-virginia region because of less cost
AWS AMI setup
AWS security group --- allow-all
Allow all Inbound and outbound
AWS Instance with devops-practice image

Linus Torvalds  -- Inventor of linux
Linux kernel
Linux -> C language == Kernel 
OS == Kernel + User Interface == Open Source

Authentication mechanisms
What you know --> Username and Password
What you have --> Username and token/OTP
What you are --> Fingerprints, Retina, Palm, etc.

Git bash Installation

Public key and Private key

public key -- copied to the server
Private key -- shared only with the users who need access, not to all

Lock and Key 
Lock --> Public 
Key --> Private

ssh-keygen -f <filename>

Server == IP(Public)

Enable file extensions in windows
ControlPanel--->FileExplorer--->Uncheck option Hide extensions

keyPairs---> Import keypairs in AWS



-------------------------------------------------------------------------------------------------------------------------------------------------
session 08 
-------------------------------------------------------------------------------------------------------------------------------------------------
DNS 
How DNS works

Browser--->OS-->ISP(DNS Resolver)-->ICANN(RootServer)-->TLD Server-->NameServer

ICANN --> Internet Corporation for assigned names and numbers --> countries, reputed organistions
ICANN maintains root servers

there are 13 root servers in the world
https://www.iana.org/domains/root/servers

top level domains(ICANN root servers will have this top level domain)
.telugu , .com , .in , .uk, .net, .edu, .gov, .us, .au, .org, .ai, .online

.gov.in, .co.in --> sub level domain

ICANN --> To start new domain i.e., top level domain  .telugu domain we need to approach ICANN. I need to complete all the process

domain registars(mediators) --> godaddy, hostinger, aws, gcp, azure

https://en.wikipedia.org/wiki/List_of_Internet_top-level_domains

Create Domain using hostinger
Domain -- raj82s.online

Register the domain in AWS using hosted zone
Copy the nameservers from AWS and update/change nameservers in Hostinger

change in NS --> Hostinger updates the change of Nameservers to .online TLD
now aws manages my domain

Hostinger updates Radix Registry(Top level TLD) about raj82s.online --> who bought this domain and nameservers

nameservers = who managed this domain = records to the DNS

A record = IP address

Create A record for backend and DB with Private IP
Create A record for frontend with public IP

DB A record       -- mysql.raj82s.online
backend A record  -- backend.raj82s.online
frontend A record -- raj82s.online

mysql.raj82s.online --> DNS resolver --> .online TLD --> provides nameservers to raj82s.online --> mysql.raj82s.online A record

nslookup -- to check the domain

Record types
=============
A --> points to IP address
CNAME --> points to another domain
MX --> mail records (info@joindevops.com)
TXT --> Domain ownership validaton purpose
NS --> nameservers
SOA --> who is the authority of this domain

What happens when we book domain?
What happens when someone enter our domain in browser?
How to become TLD?

[Unit]
Description = Backend Service

[Service]
User=expense
Environment=DB_HOST="mysql.raj82s.online"
ExecStart=/bin/node /app/index.js
SyslogIdentifier=backend

[Install]
WantedBy=multi-user.target


proxy_http_version 1.1;

location /api/ { proxy_pass http://backend.raj82s.online:8080/; }

location /health {
  stub_status on;
  access_log off;
}

http://raj82s.online/api/transaction

http://backend.raj82s.online:8080/transaction


http://raj82s.online/api/transaction --> send request to backend --> backend responds with data

inode, symlink/softlink and hardlink

what is inode?

inode stores the file type(file or folder), permissions, ownership, file size, timestamp, disk location(memory location)

lrwxrwxrwx  1 root root    11 Dec 26 03:10 DbConfig1.js -> DbConfig.js
l represents link file

symlink is like shortcut it points to the original file. symlink inode and actual file inode is different. symlink breaks when actual file is deleted. symlink can be created to folders/directories

hardlink inode is same as actual file. hardlink is useful for backup of the file. if original file is deleted hardlink remains same. we can't create hardlinks to folders/directories

how do you findout hardlinks for a particular file?

find / -inum "<inode-number>"

-------------------------------------------------------------------------------------------------------------------------------------------------
session 16 
-------------------------------------------------------------------------------------------------------------------------------------------------
Shell disadvantages
======================
Error Handling
Not idempotent
Homogenous --> only works for a particular distro
Not scalable to many servers
Password security
syntax is not easy

user: ec2-user
Pwd: DevOps321
AMI Image: RHEL devops-practice

ssh ec2-user@Public-IP
ssh connection will not work with private IP

ssh ec2-user@98.81.90.117 'echo "Hello Word" > /tmp/hello.txt'

What is configuration management
Configuration Management tools --> Chef, puppet, rundeck, Ansible
Ansible architecture
push vs pull
Ansible Installation
----------------------
sudo dnf install ansible -y

Ansible uses SSH protocol, no need of agent i.e agentless
Ansible also implmented pull based for few usecases.

XML vs JSON vs YAML

adhoc commands
====================
ansible all -i <IP>, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m ping

ansible all -i <PrivateIP>, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m ping  --- Use Private IP or Public IP when connecting  from AWS instance to AWS instance

ansible all -i <PublicIP>, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m ping --- Use Public IP when connecting  from External instance to AWS instance

ansible all -i 172.31.23.37, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m dnf -a "name=nginx state=installed" -b

ansible all -i 172.31.23.37, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m service -a "name=nginx state=started" -b

Playbook
---------

ansible-playbook -i inventory.ini -e ansible_user=ec2-user -e ansible_password=DevOps321 01-playbook.yaml

git pull --- pull the changes from github

-------------------------------------------------------------------------------------------------------------------------------------------------
session 17
-------------------------------------------------------------------------------------------------------------------------------------------------
Ansible Multiple Plays

Types of Variables
	Play 
	Task
	Files
	Prompt
	Inventory
	command line or args
Data types
Conditions

variable have a name we can define, it can hold value. You can use it wherever you want. if you change the value it will reflect everywhere. it is DRY
In shell script:
COURSE=DevOps
$COURSE or ${COURSE}

vars:
    COURSE: "DevOps with AWS"
    DURATION: 120HRS
    TRAINER: Sivakumar
	
In Ansible
"{{COURSE}}"

#1. Command line or args
#2. Task level
#3. Files
#4. Prompt
#5. Play
#6. Inventory
#7. Roles

Using -e to pass extra vars
ansible -u carol -e 'ansible_user=brian' -a whoami all

 ansible-playbook -i inventory.ini 11-vars-args.yaml -e 'COURSES="Devops with AWS"' -e DURATIONTime=120HRS -e TRAINERS=RajasekharReddy

 ansible-playbook -i inventory.ini 11-vars-args.yaml -e "COURSES='Devops with AWS'" -e "DURATIONTime=120HRS" -e "TRAINERS=Rajasekhar"
 
  ansible-playbook -i inventory.ini 11-vars-args.yaml -e "COURSES='Devops with AWS'" -e DURATIONTime=120HRS -e TRAINERS=RajasekharReddy
 
  ansible-playbook -i inventory.ini 11-vars-args.yaml -e COURSES="Devops with AWS" -e DURATIONTime=120HRS -e TRAINERS=Rajasekhar  -- wrong output becuase of space
  need to use both double and single quotes
  
 Ansible challenge : Files is a reserved word and ansible won't work as expected  if used as variable value
vars:
    DURATION: Files
-------------------------------------------------------------------------------------------------------------------------------------------------
session 18
------------------------------------------------------------------------------------------------------------------------------------------------- 
https://techviewleo.com/list-of-ansible-os-family-distributions-facts/
 
Loops
Filters
No Functions in ansible use filters
Ansible facts

ANSIBLE_DISPLAY_SKIPPED_HOSTS=false ansible-playbook -i inventory.ini 14-conditions.yaml

for ipaddr install python library 'netaddr'
To install python packages use pip
pip3.9 install netaddr

shell and command modules
ansible.builtin.shell vs ansible.builtin.command
shell --> it is like you are logging inside the server and executing command... We can access variables, we can use redirections, we can use pipes
command --> this is like running commands from outside, you will not get access to shell variables, redirections, pipes, etc.
simple command --> you can use command module, it is more secure. shell is for complex commands and less secure

Expense MySQL setup using ansible 

install cryptography,PyMySql packages using pip 3.9 

ansible-playbook -i inventory.ini mysql.yaml
Mysql fails with erroe if db root user password is not configured

fatal: [172.31.20.49]: FAILED! => {"changed": false, "msg": "unable to connect to database using pymysql 1.1.1, 
check login_user and login_password are correct or /root/.my.cnf has the credentials. 
Exception message: (1130, \"Host 'ip-172-31-20-49.ec2.internal' is not allowed to connect to this MySQL server\")"}

-------------------------------------------------------------------------------------------------------------------------------------------------
session 19
-------------------------------------------------------------------------------------------------------------------------------------------------
 Expense Backend(Nodejs) setup using ansible
 
 ansible-playbook -i inventory.ini backend.yaml
 
  Expense Fronend(Nginx) setup using ansible

 ansible-playbook -i inventory.ini frontend.yaml

mysql -h backend.raj82s.online -u root -pExpenseApp@1

Failed after creating A record with error 

rajasekhar@ManDev:~/GitHub/DAWS-2025-82S/repos/04-expense-ansible$  nslookup frontend.raj82s.online
Server:         10.255.255.254
Address:        10.255.255.254#53

** server can't find frontend.raj82s.online: NXDOMAIN

The above error is thrown when A record is still not updated

ansible.cfg file

-------------------------------------------------------------------------------------------------------------------------------------------------
session 20
-------------------------------------------------------------------------------------------------------------------------------------------------
Ansible Roles

ansible-playbook mysql.yaml -- no need to use -i to mention inventory file because it is configured in ansible.cfg file
code reuse
DRY --> Don't repeat yourself
A standard structure of writing playbooks that contains tasks, variables, dependencies, files, templates, libraries. We can reuse roles.

roles/role-name

tasks --> You can keep all your tasks here, ansible automatically loads them
	main.yaml 
vars --> variables required for this role
	main.yaml
templates --> you can keep variables in the file, ansible replace the value at runtime.
	any file
files --> We can keep files in this folder
	any file names
Handlers --> notifiers when some change event is happened
defaults/         #
	main.yml      #  <-- default lower priority variables for this role
meta  --> dependencies of this role
	main.yaml
library/          # roles can also include custom modules
	
An exception occurred during task execution. To see the full traceback, use -vvv. The error was: If you are using a module and expect the file to exist on the remote, see the remote_src option
fatal: [backend.daws82s.online]: FAILED! => {"changed": false, "msg": "Could not find or access 'backend.service'\nSearched in:\n\t/home/ec2-user/expense-ansible-roles/roles/backend/files/backend.service\n\t/home/ec2-user/expense-ansible-roles/roles/backend/backend.service\n\t/home/ec2-user/expense-ansible-roles/roles/backend/tasks/files/backend.service\n\t/home/ec2-user/expense-ansible-roles/roles/backend/tasks/backend.service\n\t/home/ec2-user/expense-ansible-roles/files/backend.service\n\t/home/ec2-user/expense-ansible-roles/backend.service on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option"}

ansible handlers are notifiers. when some change event happened in one task, we can trigger other tasks through handlers

ansible-playbook mysql.yaml
ansible-playbook backend.yaml
ansible-playbook frontend.yaml

deployment
===========
there will be one folder where code exist
stop the server
remove old code
download new code
restart the server

1. remove directory
2. re create directory
3. download code

How do you controll tasks in ansible.. few tasks should run, few tasks should not run...

ansible tags
ansible-playbook backend.yaml -t deployment
ansible-playbook frontend.yaml -t deployment
-------------------------------------------------------------------------------------------------------------------------------------------------
session 21
-------------------------------------------------------------------------------------------------------------------------------------------------
create AWS IAM user for ansible with access key and admin role

aws cli needs to be installed

aws configure  -- configure this ansible server
[ ec2-user@ip-172-31-19-238 ~/03-ansible ]$ aws configure

Expense ec2 and route53 creation using ansible
A record creation using ansible

ansible-playbook 22-create-ec2-r53.yaml  -i inventory.ini

Ansible vault

ansible-vault  view /home/rajasekhar/GitHub/DAWS-2025-82S/repos/05-expense-roles/roles/mysql/vars/vault.yaml --ask-vault-pass
admin123

--ask-vault-pass
--vault-password-file

ansible-playbook mysql.yaml --ask-vault-pass
ansible-playbook mysql.yaml --vault-password-file

ansible.cfg file
-------------------
[defaults]

inventory      = inventory.ini
#vault_password_file = /home/ec2-user/mysql_vault_pass
ask_vault_pass = True

de commissiong ansible vault and replace with secret manager or paramter store
secret manager is more cost than parameter store
ansible aws ssm parameter
-------------------------------------------------------------------------------------------------------------------------------------------------
session 22 - PENDING
-------------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------------
session 23
-------------------------------------------------------------------------------------------------------------------------------------------------
ssh connection with key in ansible.cfg

terraform
-----------
shell, ansible, terraform,sql, perl, customised scriptings
1. syntax --> variables, data types, conditions, functions and loops, error handling
2. process understanding

CRUD --> Ansible is failed to manage the infrastrucutre. For example if some manual edits happen ansible can't recognise those.
It creates duplicate resources. So ansible is not perfect in state management

Ansible --> with in the server, Ansible is perfect

Manual infra
=============
1. time
2. human errors
3. cost
4. reusable effort
5. don't know who did mistake
6. modifications are not easy
7. not scalable

IaaC --> Infra as a code

version control --> you can version control your infra. expense-infra-1.0 expense-infra-2.0
restore if something goes wrong --> easy to restore if something goes wrong
Consistent infra --> you can create same infra across all environments
Inventory management --> you can understand the resources by seeing terraform files
dependency management --> terraform can understand the dependency between resources. First it will create dependencies and then actual resources
code reusability --> terraform modules, you can create similar infra for any number of projects using modules
Cost --> automation of CRUD --> we can save costs
state management
declarative --> easy syntax
Git --> you can review code before apply

terraform manual Installation
Install AWS CLI in windows
aws configure

aws s3  ls

ansible modules
terraform providers --> provider means, a system which terraform can connect and create resources

terraform aws provider
terraform file extensions are .tf
terraform uses HCL --> Hashicorp configuration language

resource "type-of-resource" "name-of-resource" {
	your-parameters
}

resource "aws_instance" "this" {

}

terraform init   --- Downloads and install provider
terraform fmt    --- format the files properly
terraform validate --- validate the configuration
terraform plan
terraform apply  --- to create resources
terraform apply -auto-approve --- to create resources without confirmation
terraform destroy  --- to destroy resources
terraform destroy -auto-approve  --- to destroy resources without confirmation 

terraform show  --- Inspect the current state
terraform state list ---  to list the resources in your project's state

1. create user in IAM with admin access
2. install aws cli v2
3. run aws configure
-------------------------------------------------------------------------------------------------------------------------------------------------
session 24
-------------------------------------------------------------------------------------------------------------------------------------------------
1. create user in IAM with admin access
2. install aws cli v2
3. run aws configure

variables
data types
conditions
functions
loops

variables.tf --> same name is not mandatory

variable "<var-name>" {
	type = 
	default = ""
}
number, string, map, list, bool

Project = Expense
Component = Backend
Environment = DEV/UAT/QA/PROD

How do you override default variable values in terraform
terraform.tfvars --> You can override default values in terraform

cmd, tfvars, env variables, default

TF_VAR_<var-name>

Below is the order terraform load variables with high to low
-------------------------------------------------------------
1. command line --> -var "<var-name>=<var-value>"
2. tfvars
3. env var
4. default values
5. user prompt

// By default terraform will load vars from *.auto.tfvars and terraform.tfvars files
// If the file name  is different to load vars from different file name use -var-file=<filename> option
// terraform plan -var-file=test.tfvars
// terraform apply -var-file=test.tfvars

command line var:
-----------------
terraform plan -var "instance_type=t3.large"
terraform plan -var "instance_type=t3.large" -var "from_port=99"

env var:
--------
export TF_VAR_instance_type="t3.xlarge"
unset TF_VAR_instance_type

if else, when

if(expression){
	this statement run if expression is true
}
else{
	this statement run if expression is false
}

condition
---------
expression ? "this runs if true" : "this runs if false"

if dev environment t3.micro, if prod env you can run t3.small

loops
=========
1. count based loops --> iterate over list type of variables
2. for each loops
3. dynamic blocks

I want to create
1. 3 ec2 instances --> mysql, backend, frontend
2. 3 r53 private ip records
3. 1 r53 public ip record

to loop list use count.index
count.index --> 0, 1, 2 --> size=3

mysql.daws82s.online
backend.daws82s.online
frontend.daws82s.online

interpolation --> you can concat variables with text

you can't create custom functions in terraform

length,max,lookup,split

terraform console

merge -- to merge 2 maps 
course1 = {
	name = "devops",
	duration = "120hrs"
}

course2 = {
	name = "terraform",
	duration = "120hrs"
}

merge(course1, course2)

name = "terraform"
duration = "120hrs

target resources similar to tags in ansible
-------------------------------------------------------------------------------------------------------------------------------------------------
session 25
-------------------------------------------------------------------------------------------------------------------------------------------------
data sources
==================
data sources are used to query existing information from the provider. 
devops-practice --> ami-356dgtr4367yt --> ami id changes when new updates are posted.

data "<type>" "<name>" {
	
}

output blocks are used to print the information. It will be used in module development too.

locals
==================
locals are used to run the expressions or functions and save the results to variable

locals are used to store expressions, it can even store simple key value pairs just like variables.
variables can't store expressions. variable can't refer other variable. locals can refer other locals or variables
variables can be overriden. locals can't be overriden

state management
==================
declared/desired infra ==> .tf files. Whatever user write in tf files, that is what user wants
actual infra ==> what terraform is creating in provider

desired infra == actual infra

terraform.tfstate ==> it is a file terraform creates to know what it is created. this is actual infra created by terraform

someone changed the name of ec2 manually inside console

terraform plan or terraform apply

terraform reads state file and then compare that with actual infra using provider

if you update tf files....

terraform.tfstate --> expense-dev-backend
compared with tf files --> expense-dev-backend-changed

if you update few paramters, resources will not be created again it will just update
but few parameters if you update, we are forced to recreate resource

tfstate is very important file, we need to secure it

clone terraform repo --> terraform apply
duplicate resources or errors

in colloboration environment we must main state file remotely. locking is also important, so that we can prevent parellel executions

terraform.tfstate -- terraform state file
.terraform.tfstate.lock.info -- local terraform lock file

AWS S3 bucket  -- to store state file remotely and securely

DynamoDB  -- for locking the state file
create DynamoDB table with Partition Key as "LockID" other it will fail for other partition key
backend block
Default backend
Terraform uses a backend called local by default. The local backend type stores state as a local file on disk.

-------------------------------------------------------------------------------------------------------------------------------------------------
session 26
-------------------------------------------------------------------------------------------------------------------------------------------------
dynamic blocks
================
loops are used to create multiple resources.
dynamic blocks are used to create multiple blocks inside a resource..

for_each loops
===============
1. count based loop --> use it to iterate lists
2. for each loop --> use it to iterate maps
3. dynamic blocks

mysql --> t3.small
backend --> t3.micro
frontend --> t3.micro

mysql -> mysql.daws82s.online --> privateip
backend --> backend.daws82s.online --> privateip
frontend --> daws82s.online --> public ip

provisioners
===============
provisioners are used to take some action either locally or remote when terraform created servers.. 2 types of provisioners are there
1. local-exec
2. remote-exec

we can use provisioners either creation time or destroy time

local --> where terraform command is running that is local..
remote --> inside the server created by terraform

ansible-playbook -i inventory backend.yaml

multiple environments using terraform
=====================================
1. tfvars --> used to override default values
2. workspaces
3. separate codebase

DEV and PROD, remote state use

expense-dev-mysql --> mysql-dev.daws82s.online
expense-dev-backend --> backend-dev.daws82s.online
expense-dev-frontend --> frontend-dev.daws82s.online

expense-prod-mysql --> mysql-prod.daws82s.online
expense-prod-backend --> backend-prod.daws82s.online
expense-prod-frontend --> frontend-prod.daws82s.online

expense-dev or expense-prod
create separate buckets and dynamodb tables for dev and prod environments

For dev:

terraform init -backend-config=dev/backend.tf -reconfigure
terraform plan -var-file=dev/dev.tfvars
terraform apply -auto-approve -var-file=dev/dev.tfvars
terraform destroy -auto-approve -var-file=dev/dev.tfvars

For Prod:
terraform init -backend-config=prod/backend.tf -reconfigure
terraform plan -var-file=prod/prod.tfvars
terraform apply -auto-approve -var-file=prod/prod.tfvars
terraform destroy -auto-approve -var-file=prod/prod.tfvars
-------------------------------------------------------------------------------------------------------------------------------------------------
session 27
-------------------------------------------------------------------------------------------------------------------------------------------------
Why terraform? advantages
variables
	variables.tf
	terraform.tfvars
	command line
	ENV variables
conditions -> expression ? "true-value" : "false-value"
loops --> count based, for each, dynamic
functions
data sources --> query existing information
output
locals --> store expressions in a variable
state and remote state with locking
provisioners --> local-exec and remote-exec
multiple environments --> tfvars

3 ec2, 3 r53 records
====================
expense-mysql-dev --> mysql-dev.daws82s.online
expense-backend-dev --> backend-dev.daws82s.online
expense-frontend-dev --> frontend-dev.daws82s.online
allow-tls-dev

expense-mysql-prod --> mysql-prod.daws82s.online
expense-backend-prod --> backend-prod.daws82s.online
expense-frontend-prod --> daws82s.online --> public IP
allow-tls-prod

variables and tfvars

diff bucket and diff dynamodb table  for diff environments

terraform init -reconfigure -backend-config=dev/backend.tf

if instance_name is frontend and environment is prod then name should be daws82s.online
instance_name-environment.daws82s.online

instance_name is frontend and environment is prod --> and condition

terraform workspaces
====================
terraform.workspace --variable--> prod

terraform workspace new  <workspacename>--- to create new work space
terraform workspace list -- to list workspaces
terraform workspace select <workspacename>  --  to switch workspace

terraform plan
terraform apply -auto-approve

lookup() function

1. tfvars
2. workspaces
3. maintain different repos for diff environment

terraform-expense-dev
terraform-expense-prod

disadvantages
================
should be very careful --> because same code to prod also, any mistake in dev causes confusion in prod
should have full expertise, too much testing

advantages
================
code reuse

disadvantages
================
multiple repos to manage
may be we need more employees

advantages
================
clear isolation between environments, no confusion


terraform modules
========================
variables, functions, ansible roles, locals

modules --> it is like functions, you can pass inputs you will get infra
code reuse
enforce standards and best practices
centralised place to updates

validation block to validate variables
-------------------------------------------------------------------------------------------------------------------------------------------------
session 28
-------------------------------------------------------------------------------------------------------------------------------------------------
reusability
maintainability
standards
consistent infra across organisation

VPC
=====
Virtual private cloud. A isolated project space where we can create services for a project. We wil have full control and access on this

on-premise data centers
physical space
physical security
networking
power
firewalls
linux admins
n/w admins
storage backups

We need proper society space to construct house. 

village --> name, pincode
street --> name, street number
house --> C/O name, house number. Main gate

VPC --> Virtual private cloud
CIDR
subnets
igw --> to provide internet connection to VPC
databases --> no outside person should have access to DB
frontend application --> public, anyone internet can open
routes
HA --> High availability
region --> min 2AZ(data center)

IP Address --> 2^32
0.0.0.0
.
.
.
.
255.255.255.255 --> each octate 8 bits

192.145.34.56

500082 --> It represents one entire village

CIDR --> Classless inter domain routing

10.0.0.0/16 -> 16 means first 2 octates are blocked for network bits. and ip address will start as below

IPs for 10.0.0.0/16

10.0.0.0
10.0.0.1
10.0.0.2
10.0.0.3
..
.
.
10.0.0.255

10.0.1.0
10.0.1.1
.
.
.
10.0.1.255

256*256 -> 65,536

subnet --> CIDR --> 10.0.0.0/24 --> 24 bits or 3 octates blocked. 256 IP addresses are possible
1a --> 10.0.0.0/24 --> 256 IP addresses
1b --> 10.0.1.0/24 --> 256 IP addresses

IPs for 10.0.0.0/24

10.0.0.0
10.0.0.1
10.0.0.2
.
.
.
10.0.0.255

IPs for 10.0.1.0/24

10.0.1.0
10.0.1.1
10.0.1.2
.
.
.
10.0.1.255

10.0.22.145 --> database 1b

10.0.0.0/28 -- blocks 28 bits only 4 bits are available and we can have only 2power4i.e., 16 IP addresses

192.145.34.56/32 -- means only one can be used

NAT --> if you want to enable egress internet to the servers in private subnet, you should create NAT gateway in public subnet and provide route to private subnets
if traffic coming to server --> ingress
if server is sending traffic to internet --> egress


static(elastic) IP --> 1000

VPC --> 10.0.0.0/16
IGW --> Attach to VPC
Public subnets --> 10.0.1.0/24 10.0.2.0/24
Public Subnets --> Public Route --> 0.0.0.0/0 --> IGW

Private Subnets --> 10.0.11.0/24 10.0.12.0/24
Private Subnets --> Private Route --> 0.0.0.0/0 --> NAT

Database Subnets --> 10.0.21.0/24 10.0.22.0/24
Database Subnets --> Database Route --> 0.0.0.0/0 --> NAT

for humans - or _
for programs _

joindevops --> expense, roboshop

project-name-environment

Project = expense
Environment = dev
Terraform = true
Name = expense-dev

40.25.35.98 --> Home N/W --> Google N/W

IP = N/W + Host ID

dnf install mysql-server --> my EC2 is requesting internet to provide mysql-server

-------------------------------------------------------------------------------------------------------------------------------------------------
session 29
-------------------------------------------------------------------------------------------------------------------------------------------------
VPC --> CIDR
Subnets
	public
	private
	database
IGW
Route table
	Public
	Private
	Database
NAT
Routes
	Public --> 0.0.0.0/0 --> IGW
	Private --> 0.0.0.0/0 --> NAT
	Database --> 0.0.0.0/0 --> NAT
	
2 subnets
data.aws_availability_zones.available.names

[
          + "us-east-1a",
          + "us-east-1b",
          + "us-east-1c",
          + "us-east-1d",
          + "us-east-1e",
          + "us-east-1f",
        ]

Peering
==========
village-A pincode
village-B pincode
pins should be different. road should be there between 2 villages.

by default 2 VPC in AWS are not connected.
VPC peering is the way of connecting 2 VPC. CIDR should be different. Routes also should be there between 2 VPC route tables.

same account, same region 2 VPC
same account, different region VPC
diff accounts, same region
diff accounts, diff region

expense-dev --> requestor
default --> acceptor

village-A --> village-B
village-B pincode is the destinatio
-------------------------------------------------------------------------------------------------------------------------------------------------
session 30
-------------------------------------------------------------------------------------------------------------------------------------------------
count = var.is_peering_required ? 1 : 0

if is_peering_required is true the count=1 and peering will be created
if is_peering_required is false the count=o and peering will not create

2 types of modules
1. custom
2. open source

custom

advantages
==========
we know what we created and what we want

disadvantages
==========
you need to write everything from the scratch

open source
===========
advantages
-----------
everything is ready

disadvantages
----------
we dont know what is inside
we have to wait for the fix if something is wrong, we must update when they updated.

referring  git terraform module
    #source = "github.com/DAWS-2025-82S/10-terraform-aws-vpc"
    source = "git::https://github.com/DAWS-2025-82S/10-terraform-aws-vpc.git?ref=main"

to get module updates
terraform get -update

AWS security group module 

# expense-dev-mysql
resource "aws_security_group" "main" {
  name        = local.sg_final_name
  description = var.sg_description
  vpc_id      = var.vpc_id
  
  ingress {
    from_port        = 80
    to_port          = 80
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  tags = merge(
    var.common_tags,
    var.sg_tags,
    {
        Name = local.sg_final_name
    }
  )
}


resource "aws_security_group_rule" "example" {
  type              = "ingress"
  from_port         = 0
  to_port           = 65535
  protocol          = "tcp"
  cidr_blocks       = [aws_vpc.example.cidr_block]
  ipv6_cidr_blocks  = [aws_vpc.example.ipv6_cidr_block]
  security_group_id = "sg-123456"
}

Our junior engineer developed a sg module with ingress rules from the user. Module users started using it, 
for another requirement they added new firewall seperately. When they run module again our module deleted newly added rule. 
So we decided not to add sg ingress rules in module development.
-------------------------------------------------------------------------------------------------------------------------------------------------
session 31
-------------------------------------------------------------------------------------------------------------------------------------------------
Expense project infra setup using modules

terraform list aws ssm parameter string list
use join function  -- to convert

["subnet-76dghye567vf","subnet-76dghye567vf"] --> terraform(List)

subnet-76dghye567vf,subnet-76dghye567vf --> AWS(StringList), Terraform(String)

We cannot connect to instances in private and database subnets from our local laptop because they will not have public IP
to connect to them we have to use Bastion/jump host
Bastion/jump host -- one instance in public subnet -- from this we can connect to private subnets

From bastion connect using private ip
ssh ec2-user@PrivateIP

stringList ---to--> aws list
string to list
use split function

Load Balancer
Target Group

AutoScaling
Launch Template

Load Balancer,Listener,Rule,Target Group,Instance
-------------------------------------------------------------------------------------------------------------------------------------------------
session 32
-------------------------------------------------------------------------------------------------------------------------------------------------
ALB --> Listener --> Evaluate Rules --> target group --> server
Application Load Balancer
terraform aws_alb

In AWS load balancer we cannot create listener with fixed response from AWS (always have to provide target group)
while it is possible in Terraform to provide fixed response(without target group)

https://github.com/terraform-aws-modules/terraform-aws-alb    -- opensource modules
Listener and Rules
aws_lb_listener

Project infra   -- infra that doesn't change ex: Load Balancer
Application infra -- infra that changes frequently ex: ec2

one time creation -- project infra

stateful vs statless
---------------------
state means data

database --- stateful
frontend and backend -- stateless

Access App Load balancer using Bastion host
terraform aws security group rule

aws_security_group_rule

bastion host --> app ALB
ALB SG Rule
port no: 80 --> bastion host IP

To hit/access load balancer we need to allow bastion IP and port in Private(APP) Load balancer inbound security group rule
But Bastion host IP may change so instead of bastion IP we will allow access from Bastion security group
so all the instances with Bastion security group will have access to ALB irresepctive of IP

Hit the load balancer from bastion instance to check connectivity
connect to Bastion from local and then
curl http://<Loadbalancer DNS Names>

create route53 record so that alb can be accessed without load balancer dns name

*.app-dev.raj82s.online  -- route 53 record
enable alias and then select load balancer

curl http://backend.app-dev.raj82s.online
curl http://something.app-dev.raj82s.online

-------------------------------------------------------------------------------------------------------------------------------------------------
session 33
-------------------------------------------------------------------------------------------------------------------------------------------------
VPN setup
VPN is forward proxy

Instead of accessing resources in private subnets with Bastion we can setup VPN
we can access http://backend.app-dev.raj82s.online only from Bastion
To access from your local browser setup VPN

VPN server setup should be done manually only but ec2 instance creation can be automated
VPN server configuration will start when connect for the first time with user openvpnas
The password can be configured with the setup for the first time

Security Group for VPN Server
ports 22, 443, 1194, 943 --> VPN ports
22 -- for ssh access
443 -- for access from browser
1194 & 943 -- VPN internal ports
security group rule for vpn to allow the ports

openvpn access server community -- image for VPN server
OpenVPN Access Server Community Image-fe8020db-5343-4c43-9e65-5ed4a825c931

connect to VPN server from terminal using below:
	ssh -i privatekey openvpnas@PublicIP
	
ssh -i D:/GitHub/DAWS-2025-82S/testvpnfromaws.pem openvpnas@52.91.49.147	

	Note: Change the permission of pem file to 600 

openvpnas -- VPN Server SSH Username

VPN Client credentials
----------------------
openvpn
Openvpn@123	

To Access VPN server from browser and  connect to client
--------------------------------------------------------
You can now continue configuring OpenVPN Access Server by
directing your Web browser to this URL:

https://52.91.49.147:943/admin

During normal operation, OpenVPN AS can be accessed via these URLs:
Admin  UI: https://52.91.49.147:943/admin
Client UI: https://52.91.49.147:943/
To login please use the "openvpn" account with  the password you specified during the setup.

OpenVPN Connect -- client software to connect to VPN server

aws_key_pair
file function -- to read file

DB
====
on-premise

DB installation
DB upgrades
DB backups
DB changes
DB Cluster setup
DB restoration test

RDS --> 1$/hours, multi-AZ(HA) 2$/hr

DB subnet group --> group of DB subnets

set up RDS(MYSQL) DB manually
RDS(Mysql DB Setup) using AWS
creating mysql db
creating db subnet group with database subnets
aws_db_subnet_group
security group for backend(mysql DB)

DB_HOST_URL: expense-dev.czn6yzxlcsiv.us-east-1.rds.amazonaws.com
root
ExpenseApp1
3306

connect RDS using Bastion
--------------------------
sudo dnf install mysql -y
mysql -h DB_HOST_URL -u root -pExpenseApp1


terraform-aws-rds
https://github.com/terraform-aws-modules/terraform-aws-rds

Note: Mysql password in AWS should be ExpenseApp1 without @

-------------------------------------------------------------------------------------------------------------------------------------------------
session 34
-------------------------------------------------------------------------------------------------------------------------------------------------
Architecture diagram with VPC,Availability Zones,subnets,Route Tables,Bastion,VPN,Load Balancer

VPN server full automation including user setup and VPN server setup can be done automatically using user_data option
This will avoid connecting to VPN server using below command and do user setup and VPN configuration
 ssh -i Private_KEY  openvpnas@IP

user_data 
 
For rds(MYSQL) follow the steps below
--------------------------------------
Create aws_db_subnet_group group
export it as aws ssm parameter
Allow the access to DB(security group) from Bastion(Bastion security group)
Allow the access to DB(security group) from VPN(VPN security group)
create rds resource using open source rds module  "terraform-aws-modules/rds/aws"
create  route 53 record

HeidiSQL
Install client tool HeidiSQL to connect MYSQL DB

connect to DB using rds endpoint

create  route 53 record since the endpoint is not user friendly
create CNAME record since the rds endpoint is another domain

mysql-dev.raj82s.online

For APP Load Balancer Alias Route 53 record(*.app-dev.raj82s.online) is created

Destroy all resources
----------------------
rajas@ManDev MINGW64 /d/GitHub/DAWS-2025-82S/repos/14-expense-infra-dev (main)
$  ls -dr *
50-app-alb/  40-rds/  30-vpn/  20-bastion/  10-sg/  00-vpc/

for i in $(ls -dr *);do cd $i;terraform destroy -auto-approve;cd ..;done

-------------------------------------------------------------------------------------------------------------------------------------------------
session 35
-------------------------------------------------------------------------------------------------------------------------------------------------
new release
=============
1. remove old code
2. download new code
3. restart the server

changes in existing servers
============================
20 servers --> downtime should be there

connect to the servers using ansible
fetch the ip using dynamic inventory
run ansible playbook against all the servers

another method
============================
provision new instance
configure it using ansible
	connect to it
	run the playbook

stop the instance
take AMI

update auto scaling group --> 5 old version application instances
Rolling update
provision one new instance with new AMI, delete one old version server
provision second new instance with new AMI, delete second old version server
.
.
provision fifth new instance with new AMI, delete fifth old version server

Launch template --> instance creation inputs
AMI
SG ID
Subnet
Which target group

null resource will not create any new resource, it is used to connect to the instances, copy the scripts, execute the scripts through provisioners.
It has a trigger attribute to take actions when something is changed like instance id.
 
Ansible environment specific variable files in roles
 
Ansible pull
ansible-pull  -i localhost, -U https://github.com/DAWS-2025-82S/15-expense-roles-terraform.git -e COMPONENT=backend -e ENVIRONMENT=$1

file provisioner 

terraform variables --> shell --> ansible
nslookup.io

1. instance creation
2. connect to server using connection block
3. copy the shell script into server using file provisioner block
	install ansible
	ansible-pull -i localhost, -U URL main.yaml -e COMPONENT=backend -e ENVIRONMENT=dev
4. remote-exec
	chmod +x backend.sh
	sudo sh backend.sh
5. ansible configures backend
6. stop instance
7. take the AMI
8. create target group
9. launch template --> AMI
10. ASG -->> launch template

cat -A script.sh
sed -i 's/\r//g' backend.sh
-------------------------------------------------------------------------------------------------------------------------------------------------
session 36
-------------------------------------------------------------------------------------------------------------------------------------------------
If there are less environment specific variables then instead of environment specific files change the variable directly.

To do healthcheck use the url below
curl http://localhost:8080/health

check the API endpoints
allow the port 8080 for VPN
curl http://PrivateIP:8080/transaction

stop the ec2 instance
aws_ec2_instance_state
Create AMI from ec2 instance
aws_ami_from_instance
delete the stopped instance using aws cli

create target group with target type as instances and 8080 port and health check-- without registering targets
create launch template using AMI Image
create auto scaling group with launch template and select the target group
Dynamic scaling policy
Attach target group to load balancer i.e., Add the Listener rule to the listener with condition
host-header backend.app-dev.raj82s.online and select target group

create security group from load balancer to backend on port 8080
aws_lb_target_group

http://backend.app-dev.raj82s.online/health
http://backend.app-dev.raj82s.online/transaction

With private IP use 8080 port and with domain use 80 since ALB listens on 80 port
http://10.0.12.74:8080/transaction
http://backend.app-dev.raj82s.online/transaction
aws_launch_template
aws_autoscaling_group
aws_lb_listener_rule
-------------------------------------------------------------------------------------------------------------------------------------------------
session 37
-------------------------------------------------------------------------------------------------------------------------------------------------
Backend Load Balancer and autoscaling -- recap

DM, He listens to client, his manager, etc.
DM --> Load balancer
client --> port no 80 http
Rules
	*.app-dev.daws82s.online --> LB
	fdhasdkfh.app-dev.daws82s.online --> Yes Iam LB (default fixed response)
	http://backend.app-dev.daws82s.online --> forward that to backend target group
	http://analytics.app-dev.daws82s.online --> forward that to analytics target group
	health check
		every 10sec
		2 health success --> instance is healthy
		2 health failure --> instance is failed
		
Autoscaling
==============
Terraform+Shell+Ansible+Autoscaling

HR --> JD --> Which team?
JD --> Launch template
AMI --> updated latest version backend AMI

install program runtime, create user, create app folder, download code, install dependencies, create sytemctl services, configure DB_URLS, restart application

terraform --> Instance launch --> copy file using provisioner --> connected to instance through remote-exec --> run playbook

stop the instance
take AMI
delete the instance

create backend target group
create launch template --> latest AMI ID, update launch template version
ASG --> launch template latest version, instance refresh
	rolling update
	new instance create, old instance delete

listener rule --> backend.app-dev.daws82s.online --> forward that to backend target group
ASG Policy --> if AVG CPU Utilisation is crossing 70% create instances
Scale out --> create new instances
Scale in --> remove instances

if instance id changes
trigger provisoner
stop instance
take AMI

launch template changes
ASG instance refresh

either host or context

amazon.daws82s.online --> amazon
daws82s.online/amazon --> path based or context based

m.facebook.com --> mobile target group
netbanking.icicibank.com
corporatebanking.icicibank.com

joindevops --> golden AMI

backend
	ansible+nodejs --> joindevops AMI + Ansible + nodejs
shipping
	ansible+java --> joindevops AMI + Ansible + java

host or context based forwarding
amazon.daws82s.online -- host
daws82s.online/amazon -- path or context

SSL/TLS certificates
----------------------
daws82s.online --> get certificates

certificate authority --> verisign, letsencrypt

.crt file --> domain, country, type of business, location, address, company name
private key

joindevops.com --> verisign
browser sends data to servers using encrypted key, joindevops has private key so it can decrypt

*.daws82s.online

SSL termination -- after hitting the public ALB we will terminate SSL

SSL certificate creation using AWS ACM
certificate validation

aws_acm_certificate  -- create ssl certificate
aws_route53_record -- to confirm the owner of the domain
aws_acm_certificate_validation -- validate certificate

create frontend(public) Load balancer for frontend using SSL certificate
security group for frontend(public) load balancer
listener for frontend load balancer with fixed response
create route 53 record for frontend ALB

https://frontend-dev.raj82s.online:443  --- to get fixed response from frontend(public) ALB


-------------------------------------------------------------------------------------------------------------------------------------------------
session 38 - 41 PENDING
-------------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------------
session 42
-------------------------------------------------------------------------------------------------------------------------------------------------
Refer for full details - https://github.com/DAWS-82S/notes/blob/main/session-42.txt
Physical Server vs VM vs Container
Old enterprise vs Monolithic vs Microservices
Docker
Docker installation
Adding user to the docker group

docker install --> docker group created by default
usermod -aG docker ec2-user
logout and login

VM --> AMI --> Instance
Docker --> Image --> Container
AMI ==> BaseOS + App runtime + User creation + Folder creation + config files + app code + dependency installation + start/restart app --> 2GB-4GB

Image ==> Bare min OS(10MB-500MB) + App runtime + User creation + Folder creation + config files + app code + dependency installation + start/restart app --> 150 - 500MB

docker images --> displays the images available

docker pull image-name  --- to download latest image from dockerhub
docker pull image-name:tag/version -- specific tag/version

docker images -a -q -- to get all images ids
docker rmi <Image-id/name> -- delete Image
docker rmi `docker images -a -q` -- to delete all images


docker ps --> displays running containers
docker ps -a --> displays all containers(runnin+stopped)

docker create <image> --> container will be created
docker start <container-id> -- to start the created container
docker rm <container-id>  -- delete stopped container
docker rm -f <container-id> -- delete running container

docker exec -it 1e74ed63d8d6 bash -- to connect to container and get  the interactive terminal/bash access
 
docker pull + create + start == docker run
docker run nginx
docker run -d nginx -- to run container in detached mode

Container will also have 65,535 ports
docker run -p host-port:container-port  --- docker will open host port and forward request to container

nginx
======
alpine OS + Install nginx --> nginx image

cd /usr/share/nginx/html
echo "I am fro Docker Container" > index.html

docker inspect
docker log

-------------------------------------------------------------------------------------------------------------------------------------------------
session 43
-------------------------------------------------------------------------------------------------------------------------------------------------
AWS EC2 instance for docker using terraform

How to create custom docker images?

Dockerfile --> used to create our custom images using instructions provided by docker

RHEL9 == Centos9 == Almalinux9
FROM
======
FROM <BASE-OS>:<version>

docker build -t from:1.0.0 . -- build image

docker build -t dath1/from:1.0.0 . -- build with docker hub username

how to build docker image?

docker build -t <image-name>:<version> . --> Dockerfile

docker build -t <docker-hub-username>/<image-name>:<version> .

docker login -u <username>

docker push <image-name>:<version>

RUN
======
RUN instruction used to install packages, configurations on top of base os. It executes at the time of image building

docker build -t run:1.0.0 .
docker run -d run:1.0.0 -- container will exit

docker run -itd run:1.0.0 -- need to check
docker ps

CMD
======
systemctl will not work in containers.. /etc/systemd/system/*.service

docker build -t cmd:1.0.0 .
docker run -d cmd:1.0.0 -- container will not exit because of cmd command of nginx
docker ps

docker run -d -p 8080:80 cmd:1.0.0

RUN vs CMD
=========
RUN instruction executes at the time of image building
CMD instruction executes at the time of container creation

COPY vs ADD
=========
COPY and ADD both copies the files to images. but ADD has 2 extra capabilities
1. copying from directly from internet to the image
2. extracts tar file directly into image

docker build -t copy:1.0.0 .
docker run -d -p 8080:80 copy:1.0.0
docker ps

docker build -t add:1.0.0 .
docker run -d -p 8080:80 add:1.0.0
docker ps


LABEL -- adds metadata to the image but can be used in filteration

docker build -t label:1.0.0 .
docker images -f "LABEL=project=expense"

EXPOSE -- let the users know/provide the info the ports container listens  but it will not open any port
docker build -t expose:1.0.0 .
docker inspect expose:1.0.0

ENV -- sets the environment variables

To check the env variables connect to container using exec command and give "env" command to list all environment variables

docker build -t env:1.0.0 .
docker run -d env:1.0.0  -- container will exit
docker run -d env:1.0.0 sleep 30 -- use sleep to stop the container exit

docker run -itd env:1.0.0 -- container will not exit
docker exec -it e01aceb bash
env

-------------------------------------------------------------------------------------------------------------------------------------------------
session 44
-------------------------------------------------------------------------------------------------------------------------------------------------
CMD vs ENTRYPOINT
==================
1. CMD instruction can be overriden
2. You can't override ENTRYPOINT --> ping google.com ping facebook.com. If you try to override entrypoint it will not override, but it will append
3. for best results we can use CMD and ENTRYPOINT together.
4. We can mention command in ENTRYPOINT, default options/inputs can be supplied through CMD. User can always override default options.
5. Only one CMD and one ENTRYPOINT should be used in Dockerfile

USER -> set the user of container
WORKDIR --> set the working directory of container/image

ARG
------
1. ENV variables can be access at the image building and in container also
2. ARG variables are only accessed inside image build, not in container
3. ARG can be the first instruction only to provide the version for base image. It can't be useful after FROM instruction

set the ARG value to ENV variable inside Dockerfile




