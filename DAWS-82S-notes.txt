session 01 
-------------------------------------------------------------------------------------------------------------------------------
What is DevOps?
Waterfall vs Agile vs DevOps
SDLC

create organisation in Github
create repo in  new org in github
create folder with 82s and create folder with repo name locally
Execute the below commands to  intialise repo in local and push to remote
--------------------------------------------------------------------------
git init
git branch -M main
git remote add origin https://github.com/DAWS-2025-82S/notes.git
git add .
git commit -m "session-01"
git push origin main

For author identity configure below
------------------------------------
git config --global user.email "rajasekharb974@gmail.com"
git config --global user.name "rajasekhar"

To unstage a new file after using git add . use the command below
git rm --cached DAWS-82S-notes.txt

Before commiting the changes if the file is modified again after git add . then to revert the changes made, use the command below
 git restore DAWS-82S-notes.txt		---  to discard changes in working directory
 git add .							--- to include the file changes
 
 git restore --staged DAWS-82S-notes.txt  -- to unstage the already existing file in staging area

Test updated in github
-------------------------------------------------------------------------------------------------------------------------------------------------
session 02 
-------------------------------------------------------------------------------------------------------------------------------------------------
What is a Computer?
Client Server Architecture

AWS account with N-virginia region because of less cost
AWS AMI setup
AWS security group --- allow-all
Allow all Inbound and outbound
AWS Instance with devops-practice image

Linus Torvalds  -- Inventor of linux
Linux kernel
Linux -> C language == Kernel 
OS == Kernel + User Interface == Open Source

Authentication mechanisms
What you know --> Username and Password
What you have --> Username and token/OTP
What you are --> Fingerprints, Retina, Palm, etc.

Git bash Installation

Public key and Private key

public key -- copied to the server
Private key -- shared only with the users who need access, not to all

Lock and Key 
Lock --> Public 
Key --> Private

ssh-keygen -f <filename>

Server == IP(Public)

Enable file extensions in windows
ControlPanel--->FileExplorer--->Uncheck option Hide extensions

keyPairs---> Import keypairs in AWS



-------------------------------------------------------------------------------------------------------------------------------------------------
session 08 
-------------------------------------------------------------------------------------------------------------------------------------------------
DNS 
How DNS works

Browser--->OS-->ISP(DNS Resolver)-->ICANN(RootServer)-->TLD Server-->NameServer

ICANN --> Internet Corporation for assigned names and numbers --> countries, reputed organistions
ICANN maintains root servers

there are 13 root servers in the world
https://www.iana.org/domains/root/servers

top level domains(ICANN root servers will have this top level domain)
.telugu , .com , .in , .uk, .net, .edu, .gov, .us, .au, .org, .ai, .online

.gov.in, .co.in --> sub level domain

ICANN --> To start new domain i.e., top level domain  .telugu domain we need to approach ICANN. I need to complete all the process

domain registars(mediators) --> godaddy, hostinger, aws, gcp, azure

https://en.wikipedia.org/wiki/List_of_Internet_top-level_domains

Create Domain using hostinger
Domain -- raj82s.online

Register the domain in AWS using hosted zone
Copy the nameservers from AWS and update/change nameservers in Hostinger

change in NS --> Hostinger updates the change of Nameservers to .online TLD
now aws manages my domain

Hostinger updates Radix Registry(Top level TLD) about raj82s.online --> who bought this domain and nameservers

nameservers = who managed this domain = records to the DNS

A record = IP address

Create A record for backend and DB with Private IP
Create A record for frontend with public IP

DB A record       -- mysql.raj82s.online
backend A record  -- backend.raj82s.online
frontend A record -- raj82s.online

mysql.raj82s.online --> DNS resolver --> .online TLD --> provides nameservers to raj82s.online --> mysql.raj82s.online A record

nslookup -- to check the domain

Record types
=============
A --> points to IP address
CNAME --> points to another domain
MX --> mail records (info@joindevops.com)
TXT --> Domain ownership validaton purpose
NS --> nameservers
SOA --> who is the authority of this domain

What happens when we book domain?
What happens when someone enter our domain in browser?
How to become TLD?

[Unit]
Description = Backend Service

[Service]
User=expense
Environment=DB_HOST="mysql.raj82s.online"
ExecStart=/bin/node /app/index.js
SyslogIdentifier=backend

[Install]
WantedBy=multi-user.target


proxy_http_version 1.1;

location /api/ { proxy_pass http://backend.raj82s.online:8080/; }

location /health {
  stub_status on;
  access_log off;
}

http://raj82s.online/api/transaction

http://backend.raj82s.online:8080/transaction


http://raj82s.online/api/transaction --> send request to backend --> backend responds with data

inode, symlink/softlink and hardlink

what is inode?

inode stores the file type(file or folder), permissions, ownership, file size, timestamp, disk location(memory location)

lrwxrwxrwx  1 root root    11 Dec 26 03:10 DbConfig1.js -> DbConfig.js
l represents link file

symlink is like shortcut it points to the original file. symlink inode and actual file inode is different. symlink breaks when actual file is deleted. symlink can be created to folders/directories

hardlink inode is same as actual file. hardlink is useful for backup of the file. if original file is deleted hardlink remains same. we can't create hardlinks to folders/directories

how do you findout hardlinks for a particular file?

find / -inum "<inode-number>"

-------------------------------------------------------------------------------------------------------------------------------------------------
session 16 
-------------------------------------------------------------------------------------------------------------------------------------------------
Shell disadvantages
======================
Error Handling
Not idempotent
Homogenous --> only works for a particular distro
Not scalable to many servers
Password security
syntax is not easy

user: ec2-user
Pwd: DevOps321
AMI Image: RHEL devops-practice

ssh ec2-user@Public-IP
ssh connection will not work with private IP

ssh ec2-user@98.81.90.117 'echo "Hello Word" > /tmp/hello.txt'

What is configuration management
Configuration Management tools --> Chef, puppet, rundeck, Ansible
Ansible architecture
push vs pull
Ansible Installation
----------------------
sudo dnf install ansible -y

Ansible uses SSH protocol, no need of agent i.e agentless
Ansible also implmented pull based for few usecases.

XML vs JSON vs YAML

adhoc commands
====================
ansible all -i <IP>, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m ping

ansible all -i <PrivateIP>, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m ping  --- Use Private IP or Public IP when connecting  from AWS instance to AWS instance

ansible all -i <PublicIP>, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m ping --- Use Public IP when connecting  from External instance to AWS instance

ansible all -i 172.31.23.37, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m dnf -a "name=nginx state=installed" -b

ansible all -i 172.31.23.37, -e ansible_user=ec2-user -e ansible_password=DevOps321 -m service -a "name=nginx state=started" -b

Playbook
---------

ansible-playbook -i inventory.ini -e ansible_user=ec2-user -e ansible_password=DevOps321 01-playbook.yaml

git pull --- pull the changes from github

-------------------------------------------------------------------------------------------------------------------------------------------------
session 17
-------------------------------------------------------------------------------------------------------------------------------------------------
Ansible Multiple Plays

Types of Variables
	Play 
	Task
	Files
	Prompt
	Inventory
	command line or args
Data types
Conditions

variable have a name we can define, it can hold value. You can use it wherever you want. if you change the value it will reflect everywhere. it is DRY
In shell script:
COURSE=DevOps
$COURSE or ${COURSE}

vars:
    COURSE: "DevOps with AWS"
    DURATION: 120HRS
    TRAINER: Sivakumar
	
In Ansible
"{{COURSE}}"

#1. Command line or args
#2. Task level
#3. Files
#4. Prompt
#5. Play
#6. Inventory
#7. Roles

Using -e to pass extra vars
ansible -u carol -e 'ansible_user=brian' -a whoami all

 ansible-playbook -i inventory.ini 11-vars-args.yaml -e 'COURSES="Devops with AWS"' -e DURATIONTime=120HRS -e TRAINERS=RajasekharReddy

 ansible-playbook -i inventory.ini 11-vars-args.yaml -e "COURSES='Devops with AWS'" -e "DURATIONTime=120HRS" -e "TRAINERS=Rajasekhar"
 
  ansible-playbook -i inventory.ini 11-vars-args.yaml -e "COURSES='Devops with AWS'" -e DURATIONTime=120HRS -e TRAINERS=RajasekharReddy
 
  ansible-playbook -i inventory.ini 11-vars-args.yaml -e COURSES="Devops with AWS" -e DURATIONTime=120HRS -e TRAINERS=Rajasekhar  -- wrong output becuase of space
  need to use both double and single quotes
  
 Ansible challenge : Files is a reserved word and ansible won't work as expected  if used as variable value
vars:
    DURATION: Files
-------------------------------------------------------------------------------------------------------------------------------------------------
session 18
------------------------------------------------------------------------------------------------------------------------------------------------- 
https://techviewleo.com/list-of-ansible-os-family-distributions-facts/
 
Loops
Filters
No Functions in ansible use filters
Ansible facts

ANSIBLE_DISPLAY_SKIPPED_HOSTS=false ansible-playbook -i inventory.ini 14-conditions.yaml

for ipaddr install python library 'netaddr'
To install python packages use pip
pip3.9 install netaddr

shell and command modules
ansible.builtin.shell vs ansible.builtin.command
shell --> it is like you are logging inside the server and executing command... We can access variables, we can use redirections, we can use pipes
command --> this is like running commands from outside, you will not get access to shell variables, redirections, pipes, etc.
simple command --> you can use command module, it is more secure. shell is for complex commands and less secure

Expense MySQL setup using ansible 

install cryptography,PyMySql packages using pip 3.9 

ansible-playbook -i inventory.ini mysql.yaml
Mysql fails with erroe if db root user password is not configured

fatal: [172.31.20.49]: FAILED! => {"changed": false, "msg": "unable to connect to database using pymysql 1.1.1, 
check login_user and login_password are correct or /root/.my.cnf has the credentials. 
Exception message: (1130, \"Host 'ip-172-31-20-49.ec2.internal' is not allowed to connect to this MySQL server\")"}

-------------------------------------------------------------------------------------------------------------------------------------------------
session 19
-------------------------------------------------------------------------------------------------------------------------------------------------
 Expense Backend(Nodejs) setup using ansible
 
 ansible-playbook -i inventory.ini backend.yaml
 
  Expense Fronend(Nginx) setup using ansible

 ansible-playbook -i inventory.ini frontend.yaml

mysql -h backend.raj82s.online -u root -pExpenseApp@1

Failed after creating A record with error 

rajasekhar@ManDev:~/GitHub/DAWS-2025-82S/repos/04-expense-ansible$  nslookup frontend.raj82s.online
Server:         10.255.255.254
Address:        10.255.255.254#53

** server can't find frontend.raj82s.online: NXDOMAIN

The above error is thrown when A record is still not updated

ansible.cfg file

-------------------------------------------------------------------------------------------------------------------------------------------------
session 20
-------------------------------------------------------------------------------------------------------------------------------------------------
Ansible Roles

ansible-playbook mysql.yaml -- no need to use -i to mention inventory file because it is configured in ansible.cfg file
code reuse
DRY --> Don't repeat yourself
A standard structure of writing playbooks that contains tasks, variables, dependencies, files, templates, libraries. We can reuse roles.

roles/role-name

tasks --> You can keep all your tasks here, ansible automatically loads them
	main.yaml 
vars --> variables required for this role
	main.yaml
templates --> you can keep variables in the file, ansible replace the value at runtime.
	any file
files --> We can keep files in this folder
	any file names
Handlers --> notifiers when some change event is happened
defaults/         #
	main.yml      #  <-- default lower priority variables for this role
meta  --> dependencies of this role
	main.yaml
library/          # roles can also include custom modules
	
An exception occurred during task execution. To see the full traceback, use -vvv. The error was: If you are using a module and expect the file to exist on the remote, see the remote_src option
fatal: [backend.daws82s.online]: FAILED! => {"changed": false, "msg": "Could not find or access 'backend.service'\nSearched in:\n\t/home/ec2-user/expense-ansible-roles/roles/backend/files/backend.service\n\t/home/ec2-user/expense-ansible-roles/roles/backend/backend.service\n\t/home/ec2-user/expense-ansible-roles/roles/backend/tasks/files/backend.service\n\t/home/ec2-user/expense-ansible-roles/roles/backend/tasks/backend.service\n\t/home/ec2-user/expense-ansible-roles/files/backend.service\n\t/home/ec2-user/expense-ansible-roles/backend.service on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option"}

ansible handlers are notifiers. when some change event happened in one task, we can trigger other tasks through handlers

ansible-playbook mysql.yaml
ansible-playbook backend.yaml
ansible-playbook frontend.yaml

deployment
===========
there will be one folder where code exist
stop the server
remove old code
download new code
restart the server

1. remove directory
2. re create directory
3. download code

How do you controll tasks in ansible.. few tasks should run, few tasks should not run...

ansible tags
ansible-playbook backend.yaml -t deployment
ansible-playbook frontend.yaml -t deployment
-------------------------------------------------------------------------------------------------------------------------------------------------
session 21
-------------------------------------------------------------------------------------------------------------------------------------------------
create AWS IAM user for ansible with access key and admin role

aws cli needs to be installed

aws configure  -- configure this ansible server
[ ec2-user@ip-172-31-19-238 ~/03-ansible ]$ aws configure

Expense ec2 and route53 creation using ansible
A record creation using ansible

ansible-playbook 22-create-ec2-r53.yaml  -i inventory.ini

Ansible vault

ansible-vault  view /home/rajasekhar/GitHub/DAWS-2025-82S/repos/05-expense-roles/roles/mysql/vars/vault.yaml --ask-vault-pass
admin123

--ask-vault-pass
--vault-password-file

ansible-playbook mysql.yaml --ask-vault-pass
ansible-playbook mysql.yaml --vault-password-file

ansible.cfg file
-------------------
[defaults]

inventory      = inventory.ini
#vault_password_file = /home/ec2-user/mysql_vault_pass
ask_vault_pass = True

de commissiong ansible vault and replace with secret manager or paramter store
secret manager is more cost than parameter store
ansible aws ssm parameter
-------------------------------------------------------------------------------------------------------------------------------------------------
session 22 - PENDING
-------------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------------
session 23
-------------------------------------------------------------------------------------------------------------------------------------------------
ssh connection with key in ansible.cfg

terraform
-----------
shell, ansible, terraform,sql, perl, customised scriptings
1. syntax --> variables, data types, conditions, functions and loops, error handling
2. process understanding

CRUD --> Ansible is failed to manage the infrastrucutre. For example if some manual edits happen ansible can't recognise those.
It creates duplicate resources. So ansible is not perfect in state management

Ansible --> with in the server, Ansible is perfect

Manual infra
=============
1. time
2. human errors
3. cost
4. reusable effort
5. don't know who did mistake
6. modifications are not easy
7. not scalable

IaaC --> Infra as a code

version control --> you can version control your infra. expense-infra-1.0 expense-infra-2.0
restore if something goes wrong --> easy to restore if something goes wrong
Consistent infra --> you can create same infra across all environments
Inventory management --> you can understand the resources by seeing terraform files
dependency management --> terraform can understand the dependency between resources. First it will create dependencies and then actual resources
code reusability --> terraform modules, you can create similar infra for any number of projects using modules
Cost --> automation of CRUD --> we can save costs
state management
declarative --> easy syntax
Git --> you can review code before apply

terraform manual Installation
Install AWS CLI in windows
aws configure

aws s3  ls

ansible modules
terraform providers --> provider means, a system which terraform can connect and create resources

terraform aws provider
terraform file extensions are .tf
terraform uses HCL --> Hashicorp configuration language

resource "type-of-resource" "name-of-resource" {
	your-parameters
}

resource "aws_instance" "this" {

}

terraform init   --- Downloads and install provider
terraform fmt    --- format the files properly
terraform validate --- validate the configuration
terraform plan
terraform apply  --- to create resources
terraform apply -auto-approve --- to create resources without confirmation
terraform destroy  --- to destroy resources
terraform destroy -auto-approve  --- to destroy resources without confirmation 

terraform show  --- Inspect the current state
terraform state list ---  to list the resources in your project's state

1. create user in IAM with admin access
2. install aws cli v2
3. run aws configure
-------------------------------------------------------------------------------------------------------------------------------------------------
session 24
-------------------------------------------------------------------------------------------------------------------------------------------------
1. create user in IAM with admin access
2. install aws cli v2
3. run aws configure

variables
data types
conditions
functions
loops

variables.tf --> same name is not mandatory

variable "<var-name>" {
	type = 
	default = ""
}
number, string, map, list, bool

Project = Expense
Component = Backend
Environment = DEV/UAT/QA/PROD

How do you override default variable values in terraform
terraform.tfvars --> You can override default values in terraform

cmd, tfvars, env variables, default

TF_VAR_<var-name>

Below is the order terraform load variables with high to low
-------------------------------------------------------------
1. command line --> -var "<var-name>=<var-value>"
2. tfvars
3. env var
4. default values
5. user prompt

// By default terraform will load vars from *.auto.tfvars and terraform.tfvars files
// If the file name  is different to load vars from different file name use -var-file=<filename> option
// terraform plan -var-file=test.tfvars
// terraform apply -var-file=test.tfvars

command line var:
-----------------
terraform plan -var "instance_type=t3.large"
terraform plan -var "instance_type=t3.large" -var "from_port=99"

env var:
--------
export TF_VAR_instance_type="t3.xlarge"
unset TF_VAR_instance_type

if else, when

if(expression){
	this statement run if expression is true
}
else{
	this statement run if expression is false
}

condition
---------
expression ? "this runs if true" : "this runs if false"

if dev environment t3.micro, if prod env you can run t3.small

loops
=========
1. count based loops --> iterate over list type of variables
2. for each loops
3. dynamic blocks

I want to create
1. 3 ec2 instances --> mysql, backend, frontend
2. 3 r53 private ip records
3. 1 r53 public ip record

to loop list use count.index
count.index --> 0, 1, 2 --> size=3

mysql.daws82s.online
backend.daws82s.online
frontend.daws82s.online

interpolation --> you can concat variables with text

you can't create custom functions in terraform

length,max,lookup,split

terraform console

merge -- to merge 2 maps 
course1 = {
	name = "devops",
	duration = "120hrs"
}

course2 = {
	name = "terraform",
	duration = "120hrs"
}

merge(course1, course2)

name = "terraform"
duration = "120hrs

target resources similar to tags in ansible
-------------------------------------------------------------------------------------------------------------------------------------------------
session 25
-------------------------------------------------------------------------------------------------------------------------------------------------
data sources
==================
data sources are used to query existing information from the provider. 
devops-practice --> ami-356dgtr4367yt --> ami id changes when new updates are posted.

data "<type>" "<name>" {
	
}

output blocks are used to print the information. It will be used in module development too.

locals
==================
locals are used to run the expressions or functions and save the results to variable

locals are used to store expressions, it can even store simple key value pairs just like variables.
variables can't store expressions. variable can't refer other variable. locals can refer other locals or variables
variables can be overriden. locals can't be overriden

state management
==================
declared/desired infra ==> .tf files. Whatever user write in tf files, that is what user wants
actual infra ==> what terraform is creating in provider

desired infra == actual infra

terraform.tfstate ==> it is a file terraform creates to know what it is created. this is actual infra created by terraform

someone changed the name of ec2 manually inside console

terraform plan or terraform apply

terraform reads state file and then compare that with actual infra using provider

if you update tf files....

terraform.tfstate --> expense-dev-backend
compared with tf files --> expense-dev-backend-changed

if you update few paramters, resources will not be created again it will just update
but few parameters if you update, we are forced to recreate resource

tfstate is very important file, we need to secure it

clone terraform repo --> terraform apply
duplicate resources or errors

in colloboration environment we must main state file remotely. locking is also important, so that we can prevent parellel executions

terraform.tfstate -- terraform state file
.terraform.tfstate.lock.info -- local terraform lock file

AWS S3 bucket  -- to store state file remotely and securely

DynamoDB  -- for locking the state file
create DynamoDB table with Partition Key as "LockID" other it will fail for other partition key
backend block
Default backend
Terraform uses a backend called local by default. The local backend type stores state as a local file on disk.

-------------------------------------------------------------------------------------------------------------------------------------------------
session 26
-------------------------------------------------------------------------------------------------------------------------------------------------
dynamic blocks
================
loops are used to create multiple resources.
dynamic blocks are used to create multiple blocks inside a resource..

for_each loops
===============
1. count based loop --> use it to iterate lists
2. for each loop --> use it to iterate maps
3. dynamic blocks

mysql --> t3.small
backend --> t3.micro
frontend --> t3.micro

mysql -> mysql.daws82s.online --> privateip
backend --> backend.daws82s.online --> privateip
frontend --> daws82s.online --> public ip

provisioners
===============
provisioners are used to take some action either locally or remote when terraform created servers.. 2 types of provisioners are there
1. local-exec
2. remote-exec

we can use provisioners either creation time or destroy time

local --> where terraform command is running that is local..
remote --> inside the server created by terraform

ansible-playbook -i inventory backend.yaml

multiple environments using terraform
=====================================
1. tfvars --> used to override default values
2. workspaces
3. separate codebase

DEV and PROD, remote state use

expense-dev-mysql --> mysql-dev.daws82s.online
expense-dev-backend --> backend-dev.daws82s.online
expense-dev-frontend --> frontend-dev.daws82s.online

expense-prod-mysql --> mysql-prod.daws82s.online
expense-prod-backend --> backend-prod.daws82s.online
expense-prod-frontend --> frontend-prod.daws82s.online

expense-dev or expense-prod
create separate buckets and dynamodb tables for dev and prod environments

For dev:

terraform init -backend-config=dev/backend.tf -reconfigure
terraform plan -var-file=dev/dev.tfvars
terraform apply -auto-approve -var-file=dev/dev.tfvars
terraform destroy -auto-approve -var-file=dev/dev.tfvars

For Prod:
terraform init -backend-config=prod/backend.tf -reconfigure
terraform plan -var-file=prod/prod.tfvars
terraform apply -auto-approve -var-file=prod/prod.tfvars
terraform destroy -auto-approve -var-file=prod/prod.tfvars
-------------------------------------------------------------------------------------------------------------------------------------------------
session 27
-------------------------------------------------------------------------------------------------------------------------------------------------
Why terraform? advantages
variables
	variables.tf
	terraform.tfvars
	command line
	ENV variables
conditions -> expression ? "true-value" : "false-value"
loops --> count based, for each, dynamic
functions
data sources --> query existing information
output
locals --> store expressions in a variable
state and remote state with locking
provisioners --> local-exec and remote-exec
multiple environments --> tfvars

3 ec2, 3 r53 records
====================
expense-mysql-dev --> mysql-dev.daws82s.online
expense-backend-dev --> backend-dev.daws82s.online
expense-frontend-dev --> frontend-dev.daws82s.online
allow-tls-dev

expense-mysql-prod --> mysql-prod.daws82s.online
expense-backend-prod --> backend-prod.daws82s.online
expense-frontend-prod --> daws82s.online --> public IP
allow-tls-prod

variables and tfvars

diff bucket and diff dynamodb table  for diff environments

terraform init -reconfigure -backend-config=dev/backend.tf

if instance_name is frontend and environment is prod then name should be daws82s.online
instance_name-environment.daws82s.online

instance_name is frontend and environment is prod --> and condition

terraform workspaces
====================
terraform.workspace --variable--> prod

terraform workspace new  <workspacename>--- to create new work space
terraform workspace list -- to list workspaces
terraform workspace select <workspacename>  --  to switch workspace

terraform plan
terraform apply -auto-approve

lookup() function

1. tfvars
2. workspaces
3. maintain different repos for diff environment

terraform-expense-dev
terraform-expense-prod

disadvantages
================
should be very careful --> because same code to prod also, any mistake in dev causes confusion in prod
should have full expertise, too much testing

advantages
================
code reuse

disadvantages
================
multiple repos to manage
may be we need more employees

advantages
================
clear isolation between environments, no confusion


terraform modules
========================
variables, functions, ansible roles, locals

modules --> it is like functions, you can pass inputs you will get infra
code reuse
enforce standards and best practices
centralised place to updates

validation block to validate variables
-------------------------------------------------------------------------------------------------------------------------------------------------
session 28
-------------------------------------------------------------------------------------------------------------------------------------------------
reusability
maintainability
standards
consistent infra across organisation

VPC
=====
Virtual private cloud. A isolated project space where we can create services for a project. We wil have full control and access on this

on-premise data centers
physical space
physical security
networking
power
firewalls
linux admins
n/w admins
storage backups

We need proper society space to construct house. 

village --> name, pincode
street --> name, street number
house --> C/O name, house number. Main gate

VPC --> Virtual private cloud
CIDR
subnets
igw --> to provide internet connection to VPC
databases --> no outside person should have access to DB
frontend application --> public, anyone internet can open
routes
HA --> High availability
region --> min 2AZ(data center)

IP Address --> 2^32
0.0.0.0
.
.
.
.
255.255.255.255 --> each octate 8 bits

192.145.34.56

500082 --> It represents one entire village

CIDR --> Classless inter domain routing

10.0.0.0/16 -> 16 means first 2 octates are blocked for network bits. and ip address will start as below

IPs for 10.0.0.0/16

10.0.0.0
10.0.0.1
10.0.0.2
10.0.0.3
..
.
.
10.0.0.255

10.0.1.0
10.0.1.1
.
.
.
10.0.1.255

256*256 -> 65,536

subnet --> CIDR --> 10.0.0.0/24 --> 24 bits or 3 octates blocked. 256 IP addresses are possible
1a --> 10.0.0.0/24 --> 256 IP addresses
1b --> 10.0.1.0/24 --> 256 IP addresses

IPs for 10.0.0.0/24

10.0.0.0
10.0.0.1
10.0.0.2
.
.
.
10.0.0.255

IPs for 10.0.1.0/24

10.0.1.0
10.0.1.1
10.0.1.2
.
.
.
10.0.1.255

10.0.22.145 --> database 1b

10.0.0.0/28 -- blocks 28 bits only 4 bits are available and we can have only 2power4i.e., 16 IP addresses

192.145.34.56/32 -- means only one can be used

NAT --> if you want to enable egress internet to the servers in private subnet, you should create NAT gateway in public subnet and provide route to private subnets
if traffic coming to server --> ingress
if server is sending traffic to internet --> egress


static(elastic) IP --> 1000

VPC --> 10.0.0.0/16
IGW --> Attach to VPC
Public subnets --> 10.0.1.0/24 10.0.2.0/24
Public Subnets --> Public Route --> 0.0.0.0/0 --> IGW

Private Subnets --> 10.0.11.0/24 10.0.12.0/24
Private Subnets --> Private Route --> 0.0.0.0/0 --> NAT

Database Subnets --> 10.0.21.0/24 10.0.22.0/24
Database Subnets --> Database Route --> 0.0.0.0/0 --> NAT

for humans - or _
for programs _

joindevops --> expense, roboshop

project-name-environment

Project = expense
Environment = dev
Terraform = true
Name = expense-dev

40.25.35.98 --> Home N/W --> Google N/W

IP = N/W + Host ID

dnf install mysql-server --> my EC2 is requesting internet to provide mysql-server

-------------------------------------------------------------------------------------------------------------------------------------------------
session 29
-------------------------------------------------------------------------------------------------------------------------------------------------
VPC --> CIDR
Subnets
	public
	private
	database
IGW
Route table
	Public
	Private
	Database
NAT
Routes
	Public --> 0.0.0.0/0 --> IGW
	Private --> 0.0.0.0/0 --> NAT
	Database --> 0.0.0.0/0 --> NAT
	
2 subnets
data.aws_availability_zones.available.names

[
          + "us-east-1a",
          + "us-east-1b",
          + "us-east-1c",
          + "us-east-1d",
          + "us-east-1e",
          + "us-east-1f",
        ]

Peering
==========
village-A pincode
village-B pincode
pins should be different. road should be there between 2 villages.

by default 2 VPC in AWS are not connected.
VPC peering is the way of connecting 2 VPC. CIDR should be different. Routes also should be there between 2 VPC route tables.

same account, same region 2 VPC
same account, different region VPC
diff accounts, same region
diff accounts, diff region

expense-dev --> requestor
default --> acceptor

village-A --> village-B
village-B pincode is the destinatio
-------------------------------------------------------------------------------------------------------------------------------------------------
session 30
-------------------------------------------------------------------------------------------------------------------------------------------------
count = var.is_peering_required ? 1 : 0

if is_peering_required is true the count=1 and peering will be created
if is_peering_required is false the count=o and peering will not create

2 types of modules
1. custom
2. open source

custom

advantages
==========
we know what we created and what we want

disadvantages
==========
you need to write everything from the scratch

open source
===========
advantages
-----------
everything is ready

disadvantages
----------
we dont know what is inside
we have to wait for the fix if something is wrong, we must update when they updated.

referring  git terraform module
    #source = "github.com/DAWS-2025-82S/10-terraform-aws-vpc"
    source = "git::https://github.com/DAWS-2025-82S/10-terraform-aws-vpc.git?ref=main"

to get module updates
terraform get -update

AWS security group module 

# expense-dev-mysql
resource "aws_security_group" "main" {
  name        = local.sg_final_name
  description = var.sg_description
  vpc_id      = var.vpc_id
  
  ingress {
    from_port        = 80
    to_port          = 80
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  tags = merge(
    var.common_tags,
    var.sg_tags,
    {
        Name = local.sg_final_name
    }
  )
}


resource "aws_security_group_rule" "example" {
  type              = "ingress"
  from_port         = 0
  to_port           = 65535
  protocol          = "tcp"
  cidr_blocks       = [aws_vpc.example.cidr_block]
  ipv6_cidr_blocks  = [aws_vpc.example.ipv6_cidr_block]
  security_group_id = "sg-123456"
}

Our junior engineer developed a sg module with ingress rules from the user. Module users started using it, 
for another requirement they added new firewall seperately. When they run module again our module deleted newly added rule. 
So we decided not to add sg ingress rules in module development.
-------------------------------------------------------------------------------------------------------------------------------------------------
session 31
-------------------------------------------------------------------------------------------------------------------------------------------------
Expense project infra setup using modules

terraform list aws ssm parameter string list
use join function  -- to convert

["subnet-76dghye567vf","subnet-76dghye567vf"] --> terraform(List)

subnet-76dghye567vf,subnet-76dghye567vf --> AWS(StringList), Terraform(String)

We cannot connect to instances in private and database subnets from our local laptop because they will not have public IP
to connect to them we have to use Bastion/jump host
Bastion/jump host -- one instance in public subnet -- from this we can connect to private subnets

From bastion connect using private ip
ssh ec2-user@PrivateIP

stringList ---to--> aws list
string to list
use split function

Load Balancer
Target Group

AutoScaling
Launch Template

Load Balancer,Listener,Rule,Target Group,Instance
-------------------------------------------------------------------------------------------------------------------------------------------------
session 32
-------------------------------------------------------------------------------------------------------------------------------------------------
ALB --> Listener --> Evaluate Rules --> target group --> server
Application Load Balancer
terraform aws_alb

In AWS load balancer we cannot create listener with fixed response from AWS (always have to provide target group)
while it is possible in Terraform to provide fixed response(without target group)

https://github.com/terraform-aws-modules/terraform-aws-alb    -- opensource modules
Listener and Rules
aws_lb_listener

Project infra   -- infra that doesn't change ex: Load Balancer
Application infra -- infra that changes frequently ex: ec2

one time creation -- project infra

stateful vs statless
---------------------
state means data

database --- stateful
frontend and backend -- stateless

Access App Load balancer using Bastion host
terraform aws security group rule

aws_security_group_rule

bastion host --> app ALB
ALB SG Rule
port no: 80 --> bastion host IP

To hit/access load balancer we need to allow bastion IP and port in Private(APP) Load balancer inbound security group rule
But Bastion host IP may change so instead of bastion IP we will allow access from Bastion security group
so all the instances with Bastion security group will have access to ALB irresepctive of IP

Hit the load balancer from bastion instance to check connectivity
connect to Bastion from local and then
curl http://<Loadbalancer DNS Names>

create route53 record so that alb can be accessed without load balancer dns name

*.app-dev.raj82s.online  -- route 53 record
enable alias and then select load balancer

curl http://backend.app-dev.raj82s.online
curl http://something.app-dev.raj82s.online

-------------------------------------------------------------------------------------------------------------------------------------------------
session 33
-------------------------------------------------------------------------------------------------------------------------------------------------
VPN setup
VPN is forward proxy

Instead of accessing resources in private subnets with Bastion we can setup VPN
we can access http://backend.app-dev.raj82s.online only from Bastion
To access from your local browser setup VPN

VPN server setup should be done manually only but ec2 instance creation can be automated
VPN server configuration will start when connect for the first time with user openvpnas
The password can be configured with the setup for the first time

Security Group for VPN Server
ports 22, 443, 1194, 943 --> VPN ports
22 -- for ssh access
443 -- for access from browser
1194 & 943 -- VPN internal ports
security group rule for vpn to allow the ports

openvpn access server community -- image for VPN server
OpenVPN Access Server Community Image-fe8020db-5343-4c43-9e65-5ed4a825c931

connect to VPN server from terminal using below:
	ssh -i privatekey openvpnas@PublicIP
	
ssh -i D:/GitHub/DAWS-2025-82S/testvpnfromaws.pem openvpnas@52.91.49.147	

	Note: Change the permission of pem file to 600 

openvpnas -- VPN Server SSH Username

VPN Client credentials
----------------------
openvpn
Openvpn@123	

To Access VPN server from browser and  connect to client
--------------------------------------------------------
You can now continue configuring OpenVPN Access Server by
directing your Web browser to this URL:

https://52.91.49.147:943/admin

During normal operation, OpenVPN AS can be accessed via these URLs:
Admin  UI: https://52.91.49.147:943/admin
Client UI: https://52.91.49.147:943/
To login please use the "openvpn" account with  the password you specified during the setup.

OpenVPN Connect -- client software to connect to VPN server

aws_key_pair
file function -- to read file

DB
====
on-premise

DB installation
DB upgrades
DB backups
DB changes
DB Cluster setup
DB restoration test

RDS --> 1$/hours, multi-AZ(HA) 2$/hr

DB subnet group --> group of DB subnets

set up RDS(MYSQL) DB manually
RDS(Mysql DB Setup) using AWS
creating mysql db
creating db subnet group with database subnets
aws_db_subnet_group
security group for backend(mysql DB)

DB_HOST_URL: expense-dev.czn6yzxlcsiv.us-east-1.rds.amazonaws.com
root
ExpenseApp1
3306

connect RDS using Bastion
--------------------------
sudo dnf install mysql -y
mysql -h DB_HOST_URL -u root -pExpenseApp1


terraform-aws-rds
https://github.com/terraform-aws-modules/terraform-aws-rds

Note: Mysql password in AWS should be ExpenseApp1 without @

-------------------------------------------------------------------------------------------------------------------------------------------------
session 34
-------------------------------------------------------------------------------------------------------------------------------------------------
Architecture diagram with VPC,Availability Zones,subnets,Route Tables,Bastion,VPN,Load Balancer

VPN server full automation including user setup and VPN server setup can be done automatically using user_data option
This will avoid connecting to VPN server using below command and do user setup and VPN configuration
 ssh -i Private_KEY  openvpnas@IP

user_data 
 
For rds(MYSQL) follow the steps below
--------------------------------------
Create aws_db_subnet_group group
export it as aws ssm parameter
Allow the access to DB(security group) from Bastion(Bastion security group)
Allow the access to DB(security group) from VPN(VPN security group)
create rds resource using open source rds module  "terraform-aws-modules/rds/aws"
create  route 53 record

HeidiSQL
Install client tool HeidiSQL to connect MYSQL DB

connect to DB using rds endpoint

create  route 53 record since the endpoint is not user friendly
create CNAME record since the rds endpoint is another domain

mysql-dev.raj82s.online

For APP Load Balancer Alias Route 53 record(*.app-dev.raj82s.online) is created

Destroy all resources
----------------------
rajas@ManDev MINGW64 /d/GitHub/DAWS-2025-82S/repos/14-expense-infra-dev (main)
$  ls -dr *
50-app-alb/  40-rds/  30-vpn/  20-bastion/  10-sg/  00-vpc/

for i in $(ls -dr *);do cd $i;terraform destroy -auto-approve;cd ..;done

-------------------------------------------------------------------------------------------------------------------------------------------------
session 35
-------------------------------------------------------------------------------------------------------------------------------------------------
new release
=============
1. remove old code
2. download new code
3. restart the server

changes in existing servers
============================
20 servers --> downtime should be there

connect to the servers using ansible
fetch the ip using dynamic inventory
run ansible playbook against all the servers

another method
============================
provision new instance
configure it using ansible
	connect to it
	run the playbook

stop the instance
take AMI

update auto scaling group --> 5 old version application instances
Rolling update
provision one new instance with new AMI, delete one old version server
provision second new instance with new AMI, delete second old version server
.
.
provision fifth new instance with new AMI, delete fifth old version server

Launch template --> instance creation inputs
AMI
SG ID
Subnet
Which target group

null resource will not create any new resource, it is used to connect to the instances, copy the scripts, execute the scripts through provisioners.
It has a trigger attribute to take actions when something is changed like instance id.
 
Ansible environment specific variable files in roles
 
Ansible pull
ansible-pull  -i localhost, -U https://github.com/DAWS-2025-82S/15-expense-roles-terraform.git -e COMPONENT=backend -e ENVIRONMENT=$1

file provisioner 

terraform variables --> shell --> ansible
nslookup.io

1. instance creation
2. connect to server using connection block
3. copy the shell script into server using file provisioner block
	install ansible
	ansible-pull -i localhost, -U URL main.yaml -e COMPONENT=backend -e ENVIRONMENT=dev
4. remote-exec
	chmod +x backend.sh
	sudo sh backend.sh
5. ansible configures backend
6. stop instance
7. take the AMI
8. create target group
9. launch template --> AMI
10. ASG -->> launch template

cat -A script.sh
sed -i 's/\r//g' backend.sh
-------------------------------------------------------------------------------------------------------------------------------------------------
session 36
-------------------------------------------------------------------------------------------------------------------------------------------------
If there are less environment specific variables then instead of environment specific files change the variable directly.

To do healthcheck use the url below
curl http://localhost:8080/health

check the API endpoints
allow the port 8080 for VPN
curl http://PrivateIP:8080/transaction

stop the ec2 instance
aws_ec2_instance_state
Create AMI from ec2 instance
aws_ami_from_instance
delete the stopped instance using aws cli

create target group with target type as instances and 8080 port and health check-- without registering targets
create launch template using AMI Image
create auto scaling group with launch template and select the target group
Dynamic scaling policy
Attach target group to load balancer i.e., Add the Listener rule to the listener with condition
host-header backend.app-dev.raj82s.online and select target group

create security group from load balancer to backend on port 8080
aws_lb_target_group

http://backend.app-dev.raj82s.online/health
http://backend.app-dev.raj82s.online/transaction

With private IP use 8080 port and with domain use 80 since ALB listens on 80 port
http://10.0.12.74:8080/transaction
http://backend.app-dev.raj82s.online/transaction
aws_launch_template
aws_autoscaling_group
aws_lb_listener_rule
-------------------------------------------------------------------------------------------------------------------------------------------------
session 37
-------------------------------------------------------------------------------------------------------------------------------------------------
Backend Load Balancer and autoscaling -- recap

DM, He listens to client, his manager, etc.
DM --> Load balancer
client --> port no 80 http
Rules
	*.app-dev.daws82s.online --> LB
	fdhasdkfh.app-dev.daws82s.online --> Yes Iam LB (default fixed response)
	http://backend.app-dev.daws82s.online --> forward that to backend target group
	http://analytics.app-dev.daws82s.online --> forward that to analytics target group
	health check
		every 10sec
		2 health success --> instance is healthy
		2 health failure --> instance is failed
		
Autoscaling
==============
Terraform+Shell+Ansible+Autoscaling

HR --> JD --> Which team?
JD --> Launch template
AMI --> updated latest version backend AMI

install program runtime, create user, create app folder, download code, install dependencies, create sytemctl services, configure DB_URLS, restart application

terraform --> Instance launch --> copy file using provisioner --> connected to instance through remote-exec --> run playbook

stop the instance
take AMI
delete the instance

create backend target group
create launch template --> latest AMI ID, update launch template version
ASG --> launch template latest version, instance refresh
	rolling update
	new instance create, old instance delete

listener rule --> backend.app-dev.daws82s.online --> forward that to backend target group
ASG Policy --> if AVG CPU Utilisation is crossing 70% create instances
Scale out --> create new instances
Scale in --> remove instances

if instance id changes
trigger provisoner
stop instance
take AMI

launch template changes
ASG instance refresh

either host or context

amazon.daws82s.online --> amazon
daws82s.online/amazon --> path based or context based

m.facebook.com --> mobile target group
netbanking.icicibank.com
corporatebanking.icicibank.com

joindevops --> golden AMI

backend
	ansible+nodejs --> joindevops AMI + Ansible + nodejs
shipping
	ansible+java --> joindevops AMI + Ansible + java

host or context based forwarding
amazon.daws82s.online -- host
daws82s.online/amazon -- path or context

SSL/TLS certificates
----------------------
daws82s.online --> get certificates

certificate authority --> verisign, letsencrypt

.crt file --> domain, country, type of business, location, address, company name
private key

joindevops.com --> verisign
browser sends data to servers using encrypted key, joindevops has private key so it can decrypt

*.daws82s.online

SSL termination -- after hitting the public ALB we will terminate SSL

SSL certificate creation using AWS ACM
certificate validation

aws_acm_certificate  -- create ssl certificate
aws_route53_record -- to confirm the owner of the domain
aws_acm_certificate_validation -- validate certificate

create frontend(public) Load balancer for frontend using SSL certificate
security group for frontend(public) load balancer
listener for frontend load balancer with fixed response
create route 53 record for frontend ALB

https://frontend-dev.raj82s.online:443  --- to get fixed response from frontend(public) ALB


-------------------------------------------------------------------------------------------------------------------------------------------------
session 38 - 41 PENDING
-------------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------------
session 42
-------------------------------------------------------------------------------------------------------------------------------------------------
Refer for full details - https://github.com/DAWS-82S/notes/blob/main/session-42.txt
Physical Server vs VM vs Container
Old enterprise vs Monolithic vs Microservices
Docker
Docker installation
Adding user to the docker group

docker install --> docker group created by default
usermod -aG docker ec2-user
logout and login

VM --> AMI --> Instance
Docker --> Image --> Container
AMI ==> BaseOS + App runtime + User creation + Folder creation + config files + app code + dependency installation + start/restart app --> 2GB-4GB

Image ==> Bare min OS(10MB-500MB) + App runtime + User creation + Folder creation + config files + app code + dependency installation + start/restart app --> 150 - 500MB

docker images --> displays the images available

docker pull image-name  --- to download latest image from dockerhub
docker pull image-name:tag/version -- specific tag/version

docker images -a -q -- to get all images ids
docker rmi <Image-id/name> -- delete Image
docker rmi `docker images -a -q` -- to delete all images


docker ps --> displays running containers
docker ps -a --> displays all containers(runnin+stopped)

docker create <image> --> container will be created
docker start <container-id> -- to start the created container
docker rm <container-id>  -- delete stopped container
docker rm -f <container-id> -- delete running container

docker exec -it 1e74ed63d8d6 bash -- to connect to container and get  the interactive terminal/bash access
 
docker pull + create + start == docker run
docker run nginx
docker run -d nginx -- to run container in detached mode

Container will also have 65,535 ports
docker run -p host-port:container-port  --- docker will open host port and forward request to container

nginx
======
alpine OS + Install nginx --> nginx image

cd /usr/share/nginx/html
echo "I am fro Docker Container" > index.html

docker inspect
docker log

-------------------------------------------------------------------------------------------------------------------------------------------------
session 43
-------------------------------------------------------------------------------------------------------------------------------------------------
AWS EC2 instance for docker using terraform

How to create custom docker images?

Dockerfile --> used to create our custom images using instructions provided by docker

RHEL9 == Centos9 == Almalinux9
FROM
======
FROM <BASE-OS>:<version>

docker build -t from:1.0.0 . -- build image

docker build -t dath1/from:1.0.0 . -- build with docker hub username

how to build docker image?

docker build -t <image-name>:<version> . --> Dockerfile

docker build -t <docker-hub-username>/<image-name>:<version> .

docker login -u <username>

docker push <image-name>:<version>

RUN
======
RUN instruction used to install packages, configurations on top of base os. It executes at the time of image building

docker build -t run:1.0.0 .
docker run -d run:1.0.0 -- container will exit

docker run -itd run:1.0.0 -- container will not exit
docker ps

CMD
======
systemctl will not work in containers.. /etc/systemd/system/*.service

docker build -t cmd:1.0.0 .
docker run -d cmd:1.0.0 -- container will not exit because of cmd command of nginx
docker ps

docker run -d -p 8080:80 cmd:1.0.0

RUN vs CMD
=========
RUN instruction executes at the time of image building
CMD instruction executes at the time of container creation

COPY vs ADD
=========
COPY and ADD both copies the files to images. but ADD has 2 extra capabilities
1. copying from directly from internet to the image
2. extracts tar file directly into image

docker build -t copy:1.0.0 .
docker run -d -p 8080:80 copy:1.0.0
docker ps

docker build -t add:1.0.0 .
docker run -d -p 8080:80 add:1.0.0
docker ps


LABEL -- adds metadata to the image but can be used in filteration

docker build -t label:1.0.0 .
docker images -f "LABEL=project=expense"

EXPOSE -- let the users know/provide the info the ports container listens  but it will not open any port
docker build -t expose:1.0.0 .
docker inspect expose:1.0.0

ENV -- sets the environment variables

To check the env variables connect to container using exec command and give "env" command to list all environment variables

docker build -t env:1.0.0 .
docker run -d env:1.0.0  -- container will exit
docker run -d env:1.0.0 sleep 30 -- use sleep to stop the container exit

docker run -itd env:1.0.0 -- container will not exit
docker exec -it e01aceb bash
env

-------------------------------------------------------------------------------------------------------------------------------------------------
session 44
-------------------------------------------------------------------------------------------------------------------------------------------------
CMD vs ENTRYPOINT
==================
1. CMD instruction can be overriden
2. You can't override ENTRYPOINT --> ping google.com ping facebook.com. If you try to override entrypoint it will not override, but it will append
3. for best results we can use CMD and ENTRYPOINT together.
4. We can mention command in ENTRYPOINT, default options/inputs can be supplied through CMD. User can always override default options.
5. Only one CMD and one ENTRYPOINT should be used in Dockerfile

docker build -t entry:1.0.0 .
docker run entry:1.0.0 ping facebook.com

docker build -t entry:1.0.0 . -f entry-1.Dockerfile
docker run entry:1.0.0
docker ps -a --no-trunc
docker run entry:1.0.0 ping facebook.com


USER -> set the user of container -- this will not give the root user access
docker build -t user:1.0.0 .
docker run -d user:1.0.0

WORKDIR --> set the working directory of container/image

docker build -t work:1.0.0 .
docker run -d work:1.0.0

# docker build -t work:1.0.0 . --debug --no-cache --progress=plain

ARG
------
1. ENV variables can be access at the image building and in container also
2. ARG variables are only accessed inside image build, not in container
3. ARG can be the first instruction only to provide the version for base image. It can't be useful after FROM instruction

set the ARG value to ENV variable inside Dockerfile

docker build -t arg:1.0.0  . -f 02-ARG.Dockerfile
docker run -d arg:1.0.0  .

 to override the ARG in docker file use --build-arg
docker build -t arg:1.0.0 --progress=plain --no-cache --build-arg USERNAME=TestUser . -f 02-ARG.Dockerfile
-------------------------------------------------------------------------------------------------------------------------------------------------
session 45
-------------------------------------------------------------------------------------------------------------------------------------------------
Create Ec2 with 50 GB
Extend the volumes
how to resize aws ebs volumes

FROM --> should be the first instruction to represent base OS
RUN --> Used to configure or install packages
CMD --> executes at the time of container creation, this command should container running infinite time
COPY --> copies from local workspace to image
ADD --> same as copy but 2 extra capabilities, directly downloads from internet or untar directly
LABEL --> adds metadata, used for filter. key value pairs
EXPOSE --> doc purpose, tell the users about ports opened by container
ENV --> sets the env variables in the container, we can use at build time also
ENTRYPOINT --> cant' override, CMD can provide default args, we can always override default args
USER --> set the user to run container
WORKDIR --> sets the working directory for container/image
ARG --> build time variables, in an exceptional case can be first instruction to supply base os version

ONBUILD
=======
we can set some conditions when some user is using our image

nginx --> alamlinux:9, install nginx, removes index.html

we will force the users to keep index.html in their workspace for mandatory
docker build -t onbuild:1.0.0 .
docker build -t onbuild-test:1.0.0 .

expense project setup using docker
-----------------------
Use mysql official image 8.0 version

setup mysql
------------
docker build -t mysql:1.0.0 .
docker run -d --name mysql mysql:1.0.0

ifconfig

docker exec -it mysql bash
mysql -u root -pExpenseApp@1
show databases;
use transactions;
show tables;

# ifconfig
# docker inspect mysql  # to get container IP

setup backend
---------------
# set DB_HOST="msql container name" because the container IP changes if contianer is restarted

docker build -t backend:1.0.0 .
docker run -d --name backend backend:1.0.0

backend app cannot connect to mysql DB because of docker default network 

if you are using default n/w docker containers can't communicate with each other

For containers to communicate we need to create network 

docker network create expense
docker network ls

There are 2 types of nettworks in docker bridge and host
default network is bridge

disconnect the containers from default network and add them to the network created

docker network disconnect <NetworkName> <ContainerName>
docker network connect <NetworkName> <ContainerName>

docker network disconnect bridge mysql
docker network connect expense mysql

docker rm backend
docker run -d --name backend --network expense backend:1.0.0  # run the container with expense network
docker inspect backend

docker exec -it backend bash
telnet mysql 3306

setup frontend
----------------
if you are using default n/w docker containers can't communicate with each other

docker build -t frontend:1.0.0 .
docker run -d -p 80:80 --name frontend --network expense frontend:1.0.0

docker exec -it frontend bash
cat /etc/nginx/nginx.conf
-------------------------------------------------------------------------------------------------------------------------------------------------
session 46
-------------------------------------------------------------------------------------------------------------------------------------------------
Host Network
------------
In Host network container will use host ip and ports
Bridge network will create virtual bridge network and containers will be attached to bridge network we can communicate between containers

For host network use localhost instead of container name(dns)

docker build -t mysql:1.0.0 .
docker run -d --name mysql --network host mysql:1.0.0

netstat -ntlp

docker build -t backend:1.0.0 .
docker run -d --name backend --network host backend:1.0.0

docker build -t frontend:1.0.0 .
docker run -d --name frontend --network host frontend:1.0.0

Host network is less secure and ports will exhaust

Overlay network
---------------
It is used for multiple docker hosts
we will not use this we will use kubernetes

Volumes
-------
docker containers are ephemeral. ephemeral means temporary
once we remove container it removes data also

docker run -d -p 80:80 nginx
docker inspect 783
docker will store data in MergedDir
cd to MergedDir("MergedDir": /var/lib/docker/overlay2/9e776cc1385bc3e88be8e0052ea2a48b16b66c61be5ec0de45c2d5b91e58effe/merged)

Delete the container and try cd to MergedDir the directory cannot be accessed because docker will delete it when container is deleted.

docker rm -f 783
cd to MergedDir

you can create one directory  for volume in host and map it to container. even if we delete container, data will not be lost, you can remount it..

mkdir nginx-data
docker run -d -p 80:80 -v /home/rajasekhar/nginx-data:/usr/share/nginx/html nginx
docker exec -it 7ed bash
cd /usr/share/nginx/html
echo "<h1>Hello Volumes</h1>" > index.html

check: http://<IP>:80

Delete the container and try to mount the volume again
docker rm -f 7ed
docker run -d -p 80:80 -v /home/ec2-user/nginx-data:/usr/share/nginx/html nginx

check: http://<IP>:80

cd nginx-data
echo "<h1>Iam using  Volumes</h1>" > volumes.

check: http://172.22.168.203/volumes.html

The directoy we created is called as unnamed volumes
we created directory, so we have to manage it...

unnamed volumes
docker volumes --> you have to create volumes with docker commands, so docker can manage, we no need to worry of creation and managing

docker volume create nginx-data-volume
docker volume ls
docker inspect nginx-data-volume

docker run -d -p 80:80 -v nginx-data-volume:/usr/share/nginx/html nginx


manually running containers
===========================
1. we need to know the dependency
2. we need to make sure network creation and volume creation
3. while removing we need to remove in anti dependency order..
4. manually run docker run commands

docker-compose.yaml

docker compose up -d
docker compose down

mysql
backend --> trying to connect mysql, before it is completely up, so we need to delay few sec

optimising docker files
-----------------------
use alpine image
remove root access with USER instruction

cd
echo "tets secret" > secret.txt
docker run -d  -v /:/app-data nginx
docker exec -it 634s bash
cd app-data
cd home/rajasekhar
cat secret.txt

From container system files will be accessed to restrict this use USER instruction

docker build -t backend:1.0.0 .

docker build -t frontend:1.0.0 .

docker compose up -d
docker compose down

docker exec -it  backend sh
id  # command to check the user

docker exec -it frontend sh

-------------------------------------------------------------------------------------------------------------------------------------------------
session 47
-------------------------------------------------------------------------------------------------------------------------------------------------
docker build -t dath1/mysql:1.0.0 .

docker build -t dath1/backend:1.0.0 .

docker build -t dath1/frontend:1.0.0 .

docker login -u dath1
docker push dath1/mysql:1.0.0
docker push dath1/backend:1.0.0
docker push dath1/frontend:1.0.0

create Dockerfile
build image
push to docker hub

Download the images from dockerhub
Change image in compose file with docker hub image name
docker-compose.yaml
	image: joindevops/mysql:1.0.0
	
docker compose up -d
docker compose down
	
Image layers
=============
Optise image layers

export DOCKER_BUILDKIT=0
docker build -t dath1/mysql:1.0.0 .

 export DOCKER_BUILDKIT=0
 docker build -t layer-test:1.0.0 .
 try this multiple times
 docker push layer-test:1.0.0 .

Docker maintains the images as layers, each and every instruction is one layer. docker creates
1. intermediate container from instruction-1
2. docker runs 2nd instruction on top of IC-1. then docker saves this another layer
3. docker saves this container as another image layer. create intermediate container out of it IC-2
4. Now docker runs 3rd instruction in IC-2 container. docker saves this as another layer
5. docker creates intermediate container from this layer as IC-3

How do you optimise docker layers?
==================================
1. less number of layers faster builds, because number of intermediate containers are less
you can club multiple instructions into single instruction
2. keep the frequently changing instructions at the bottom of Dockerfile

Multi stage builds
===================
Java
JDK --> Java development kit
JRE --> Java runtime environment

JDK > JRE
JRE is subset of JDK

JDK = JRE + Extra libraries

while installing some libraries, OS adds extra space to HD. We will take only that jar file output and copy it another Dockerfile where only jre runs...

We can have 2 dockerfiles one is for builder, we can copy the output from builder into 2nd dockerfile and run it, we can save some image space using this

Docker multistage builds are primarily used to create smaller, more optimized container images by separating the build environment from the runtime environment.
 We can have one as builder and one as runner, copy the desired output from builder to runner. docker removes builder automatically

1. build the image --> use docker
2. run the image as container --> use kubernetes

pg have 100 rooms, what if water is stopped...

if owner have say 5pg, 

what is underlying docker server is crashed. We need to maintain multiple docker hosts. 
You need some orchestrator to manage all the docker hosts... docker swarm is docker native orchestrator

kubernetes is the popular container orchestrator tool..

autoscaling of containers --> 
HA --> run containers in multiple servers
reliability --> orchestrator shifts the container to another host if one host is down
kubernetes n/w and DNS is more stronger than docker swarm
kubernetes integrates with cloud providers
storage is better than in dockerswarm



-------------------------------------------------------------------------------------------------------------------------------------------------
session 48
-------------------------------------------------------------------------------------------------------------------------------------------------
Kubernetes

1. create one linux server as workstation
2. install docker to build images
3. run aws configure to provide authentication
4. install eksctl to create and manage EKS cluster
5. install kubectl to work with eks cluster

EKS cluster creation

We wiil use same EC2 instance used for docker to setup eksctl,kubectl

sudo tail -f /var/log/cloud-init-output.log -- to check user data output

eksctl installation -- For cluster creation
kubectl installation  -- For cluster  interaction(To work with cluster)

check kubernets.txt file for kubectl and eksctl installation

Add eks.yaml file in 16-Docker project -- To create EKS cluster

eksctl create cluster --config-file=eks.yaml

aws configure

AWS cloudformation -- Just like terraform infra as code - AWS native service - will create resouces for cluster

3 types of instances in AWS
---------------------------
On demand instances
spot instances
reserved instances

ondemand --> creating server on the spot, high cost
reserved --> cost is less because you are reserving for longterm
spot --> 70-90% discount hardware available now...when our customers require we will take back your hardware with 2min notice.

kubectl get nodes -- to get the no of nodes in the cluster

everyting in kubernetes is called as resource/object

namespace 
---------- 
isolated project space similar to VPC where you can control resources to your project
default namespace is created along with cluster creation

kubectl get namespace -- to get the namespaces

Kind -- kind of resource
apiVersion  -- kubernetes version
metadata 
  name -- resource name
  
spec used for other resouces  

kubectl create -f 01-namespace.yaml -- will throw error if the resource already exists
kubectl apply -f 01-namespace.yaml  -- will throw warning not error if the resource already exists
kubectl delete -f 01-namespace.yaml -- delete the resource

pod
====
docker --> image --> container
k8s    --> image --> pod

pod vs container
----------------
pod is the smallest deployable unit in k8s .pod can have multiple containers
all conatiners in pod share the same ip and storage
multiple containers are useful in few applications like shipping the logs through sidecars

kubectl apply -f 02-pod.yaml
kubectl get pods -- to get the list of pods
kubectl get pods -n <namespace> -- to get the list of pods in namespace

kubectl exec -it <pod-name> -- bash
kubectl exec -it nginx -- bash  # To connect to pod

kubectl apply -f 03-multicontainer.yaml
kubectl get pods
CrashLoopBackOff -- error when failed to create container

kubectl pod describe <pod-name>  -- to get the details of pod like which container is failing  in a pod

# If one container failed we need to delete the pod and create it again
kubectl delete -f 03-multicontainer.yaml

kubectl exec -it <pod-name> -- bash

kubectl exec -it multi-containers -- bash

curl  localhost

By default it will connect to first container
To connect to second contianer

kubectl exec -it <pod-name> -c <container-name> -- bash

kubectl exec -it multi-containers -c almalinux -- bash

curl localhost

curl output will be same because "all conatiners in pod share the same ip and storage"

pod or container is ephemeral

when pod is deleted application logs created by first container will also get deleted 
so we will create second container(sidecar container) to push the logs to ELK to reduce the load on first container

labels
-------
labels are used to provide metadata and can be used to attach other resources

kubectl apply -f 04-labels.yaml

annotations
------------
annotations are similar to labels 
labels can be used to select internal resources 
annotations can be used to select external resources
also in annotations we can give special characters and used in ingress controller
special characters cannot be used in labels

kubectl apply -f 05-annotations.yaml

-------------------------------------------------------------------------------------------------------------------------------------------------
session 49
-------------------------------------------------------------------------------------------------------------------------------------------------
env
To set environment variables
kubectl apply -f 06-env.yaml
kubectl exec -it env -- bash
# env

resources
 requests
 limits
sets the min and max RAM and CPU for container        
kubectl apply -f 07-resources.yaml
kubectl top pod  --- to check the resources consumed by pod

ConfigMap
  data
To provide configurations as env variables

kubectl apply -f 08-config-map.yaml
kubectl get configmap
kubectl describle configmap myconfig

Referring ConfigMap to set as env variables

envFrom
 configMapRef

 kubectl apply -f 09-pod-config.yaml
 kubectl exec -it pod-config -- bash  
 env
 
 Update config map data and restart pod
kubectl apply -f 08-config-map.yaml

 kubectl delete -f 09-pod-config.yaml
 kubectl apply -f 09-pod-config.yaml
 kubectl exec -it pod-config -- bash  
 env

secrets
 data
secret types
  opaque
  
$  echo -n "testsecret" | base64
dGVzdHNlY3JldA==

$ echo "dGVzdHNlY3JldA==" | base64 --decode
testsecret
kubectl apply -f 10-secret.yaml
kubectl get secret
kubectl describe secret dotfile-secret

Referring secret to set as env variables

envFrom
 secretRef

 kubectl apply -f 11-pod-secret.yaml
 kubectl exec -it pod-secret -- bash  
 env 
 
Service
=========
service is used for 
1. load balancing
2. pod to pod communication
3. as DNS between pods

Ip address of pod will changes after deletion and creation of pod
Instead of IP we will create service to access the pod or pod to pod communication with service name

service will be cretad with clusterip
kubectl delete pods --all --all-namespaces   -- to delete all pods

kubectl apply -f 12-service.yaml

kubectl get svc --- to get service info -- gives cluster IP and also node port

kubectl get pod -o wide -- to get pod ip and also the node where pod is running

start other container to check service access from other container

kubectl apply -f 03-multicontainer.yaml

kubectl exec -it multi-containers -- bash

then curl <servicename>

curl nginx

apt update
apt install dnsutils -y

nslookup nginx

multi-container --> curl nginx --> 
3 types of services
1. cluster IP --> default, only works internally i.e., pod to pod
2. NodePort --> external requests
3. LoadBalancer --> only works with cloud providers. here service open classic LB. 

clusterip will not work in internet, works only in kubernetes i.e., only works internally i.e., pod to pod

Node port
---------
Node port works from internet
Node port automatically creates cluster IP and creates new node port

kubectl delete -f 12-service.yaml
kubectl apply -f 13-node-port.yaml

kubectl get svc --- to get service info -- gives cluster IP and also node port

kubectl get pod -o wide -- to get pod ip and also the node where pod is running

check the private ip address of node from aws ui
copy  the node public ip address and access with node port from browser(open port in security group)

If we acess the node port with any node ip it will work because kubernetes will send the request to the correct pod

In NodePort type
when user send request with public ip(any instance/node public IP) ip:nodeport
then kubernetes will forward the request to cluster ip service
then cluster ip service will forward the request to the pod
When we open port in security group Kubernetes will open node port on all instances(nodes) so we the url will work with any instance ip and node port

# test with url
# http:<AnyInstancePublicIP>:<NodePort>

kubectl delete -f 13-node-port.yaml

kubectl apply -f 14-load-balancer.yaml

In Load Balancer type
request will come from user to Load balancer listening on port 80 
then LB will forward to node port to any one of the instances(nodes) in the cluster
then from any one of the instances(node) request goes to cluster ip
then finally request will be sent from cluster ip to the pod

# test with url
# http:<LB_DNS_Name>:<NodePort>

# create Route53 record for LB DNS name with domain name to access the url with domain name

-------------------------------------------------------------------------------------------------------------------------------------------------
session 50
-------------------------------------------------------------------------------------------------------------------------------------------------
replicaset - can be used for creating multiple pods
Pod is subset of replicaset
pod name = replicaset-name-random 5 digit

To create multiple pods we have to change pod name and run the 02-pod.yaml file
metadata:
  name: nginx2
  
Instead of creatng multiple pods manually we can use replicaset 

 kubectl apply -f 15-replica-set.yaml

kubectl get replicaset

kubectl get pods -o wide

watch kubectl get pods # in new terminal

if we delete one pod, replicateset will automatically creates new pod

If we run 02-pod.yaml again with same image nothing will change
If we change image name and run 02-pod.yaml file image will be updated

check the image with below command
kubectl describe pod nginx -n expense

If image is changed in replicaset with another version ,replicaset will not update the image

To achieve this we need to use deployment

Deployment 
----------
Deployment yaml is same as replicaset except that kind is Deployment

kubectl apply -f 16-deployment.yaml

kubectl get deployment

kubectl get rs or kubectl get replicaset

kubectl get pods

To verify the replicaset updates the pods with another image version
change the image name from nginx:1.14.2 to nginx:latest

kubectl apply -f 16-deployment.yaml

check deployment with image and service
---------------------------------------
docker build -t dath1/deployment:1.0 .
docker push dath1/deployment:1.0

then run the 17-deployment-service.yaml to test the deployment

Test with url http://<LB_DNS_NAME>:80

change the verion in index.html and build the image again to  test the deployment

docker build -t dath1/deployment:2.0 .
docker push dath1/deployment:2.0

build the image from DEPLOYMENT folder

kubectl apply -f 17-deployment-service.yaml
kubectl get svc
kubectl get deployment
kubectl get rs or kubectl get replicaset
kubectl get pods

Test with url http://<LB_DNS_NAME>:80

To verify the replicaset updates the pods with another image version
change the image name from 1.0 to 2.0

kubectl apply -f 16-deployment.yaml

Test with url http://<LB_DNS_NAME>:80

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

Commands to check deployment status and rollback
-------------------------------------------------
kubectl rollout history deployment/nginx  -- to check the deployment history
kubectl rollout history deployment/nginx --revision=2  -- to check revision 2 history

kubectl rollout undo deployment/nginx --> roll back to immidiate previous version
kubectl rollout undo deployment/nginx --to-revision=2 ---> roll back to specific verison

kubectl get deployment nginx   -- to check the deployment status
kubectl rollout status deployment/nginx  -- to check the deployment status

Expense project using k8s
--------------------------
To avoid mentioning the namespace name (-n namespacename) in  kubectl commands install the tool kubens

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

kubens <namespace_name>  -- set the name of namespace
kubens expense

kubectl apply -f 01-namespace.yaml

mysql
kubectl apply -f manifest.yaml 

kubectl get pods -n expense
kubectl exec -it mysql-djshdsh -n expense -- bash
mysql -u root -pExpenseApp@1
show databases;
use transactions;
show tables;
   

backend
kubectl apply -f manifest.yaml    

frontend
change port 80 to 8080 in nginx.conf file in Dockerfile and build the image in 18-expense-docker repo
remove /etc/nginx/conf.d/default.conf file which has port 80

docker build -t dath1/frontend:1.1.0 .
docker push dath1/frontend:1.1.0

kubectl apply -f manifest.yaml   

kubectl logs frontend-54c88dc9ff-8xdnk 

kubernetes won't allow system ports for non root user
0-1024 are system ports -- container has to run with root user to access system ports
Non root users cannot access this ports

Either change the user to root or change the port in frontend nginx.conf from 80 to 8080

docker build -t dath1/frontend:1.2.0 .
docker push dath1/frontend:1.2.0

Test the app with url
http://<LB_DNS_NAME>:80

add route 53 record for LB DNS NAme and test the url with dns name
-------------------------------------------------------------------------------------------------------------------------------------------------
session 51
-------------------------------------------------------------------------------------------------------------------------------------------------

Volumes
============
everything in k8 is ephemeral. pods, nodes, master node all are ephemeral. So it is not good to store DB inside K8

Storage administration --> manage all the disks related to servers
Backup every day
Restore testing
N/w to storage

EBS -> Elastic block storage  like harddisk
EFS -> Elastic file system    like google storage

If server is in AZ us-east-1b, EBS also should be here. EBS is less latency. EBS is well suitable for DB and OS

conditions for EBS
---------------------------
1. node and EBS volume should be in same AZ
2. drivers install( to access EBS from EKS)
3. EC2 worker nodes should have permission to work with EBS volumes

static and dynamic provisioning
=================================
static means we have to create disk(volume) manually
Dynamic means EKS will create the Disk(volume)

EBS static
==========
1.we have to create disk(volume) manually in same AZ as in worker nodes 
2.drivers install(ebs csi driver) using kustomize
	https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/install.md
	kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.40"
	
	kubectl get pods -n kube-system --  check the drivers installed as pods	

3.select the worker node security tabs
	click on Iam roles
	Add permission and Attach "AmazonEBSCSIDriverPolicy" to manage volumes

4.Mount the volume to the pod using PV and PVC
	
everything in k8 is resource/object. if you can treat or manage the volumes from k8 objects any k8 engineer can work on this..

PV --> Persistant volume, represents or wrapper of physical volume
PVC --> Claiming the volume
Storage class

PV is wrapper object in kubernetes to represent physical volume like EBS or EFS

kid --> mother --> father --> wallet
Pod	-->  PVC --> PV -->  volume

https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/static-provisioning/manifests/pv.yaml  -- for creating PV

access modes
============
ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod,

ReadWriteOnce - At a time only one node can read and write with multiple pods

ReadWriteOncePod -- for only one pod

Lifecycle policies(i.e., what will happen to data if the pod is deleted)
=================

Retain -- data and volume will not delete
delete -- data and volume will be deleted
recycle -- data will delete but not volume

kubectl apply -f 01-ebs-static-pv.yaml
kubectl get pv

create PVC  -- to claim the volume
https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/static-provisioning/manifests/claim.yaml -- ex to create PVC

kubectl apply -f 02-ebs-static-pvc.yaml
kubectl get pvc

Pod should claim that PVC

all the files/data of pod stored in /usr/share/nginx/html will be stored in volume

kubectl get pods

kubectl get nodes --show-labels  ----> to get nodeSelector label

if pod schedules(created) in 1a and volume is in 1b, pod status will be in pending for continuously.

scheduler(nodeSelector) --> schedules your pods on to appropriate nodes

kubectl apply -f 02-ebs-static-pvc.yaml

kubectl exec -it ebs-static -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from EBS static</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

Delete all resources(PV,PVC,pod,service)
kubectl delete -f 02-ebs-static-pvc.yaml
kubectl delete -f 01-ebs-static-pv.yaml

all resources got deleted but not volume and data
try recreate the pod and verify the data in volume is not deleted

kubectl apply -f 01-ebs-static-pv.yaml
kubectl apply -f 02-ebs-static-pvc.yaml

check : http://<LB_DNS_NAAME>:80
-------------------------------------------------------------------------------------------------------------------------------------------------
session 52
-------------------------------------------------------------------------------------------------------------------------------------------------
permanent storage
=================
1. SAN - Storage Area Network
2. K8 Admin
3. Expense DevOps Engineer --> they get access to only expense namespace

Expense DevOps engineer send a mail to SAN with the manager approval. SAN will take their manager approval and create storage for us.

We will forward disk details to k8 admin and ask them to create PV.

In kubernetes there are 2 type of resources

kubectl api-resources

1. namespace level
2. cluster level -- we will not have access to create clusterlevel resources so we will send request to create PV which is cluster level resource

they create PV and send the name for us..

then we will create PVC, pod to access the storage

Kid --> Mom --> Dad --> Wallet
Pod --> PVC --> PV  --> Storage

EBS Static
============
1. install drivers
2. give permissions to worker nodes
3. create volume
4. create PV, PVC and cliam through pod
5. EC2 and EBS should be in same az incase of EBS provisioning

kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.40"  -- driver install

watch kubectl get pods # to monitor pods

EBS dynamic
============

Kid --> Mom --> UPI
Pod --> PVC --> SC

StorageClass --> this object is responsible for dynamic provisioning. it will create external storage(volume) and PV automatically

kubectl get sc

1. install drivers
2. give permissions to worker nodes
3. create storage class

https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/dynamic-provisioning/manifests/storageclass.yaml -- sc example

kubectl apply -f 03-ebs-sc.yaml
kubectl get sc

 StorageClass should be empty for static provisionin(EBS Static)
 Provide StorageClass name for dynamic provisioning(EBS Dynamic)
kubectl apply -f 04-ebs-dynamic.yaml

kubectl get pvc
kubectl get pv
kubectl get pods
kubectl get svc

check : http://<LB_DNS_NAME>:80   --- no output

kubectl exec -it ebs-dynamic -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Dynamic EBS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

kubectl apply -f 04-ebs-dynamic.yaml

Volume will not delete we have to delete it manually

EFS --> Elastic file system
-----------------------------
1. EBS should be in same AZ as in EC2, EFS can be anywhere in n/w
2. EBS is speed compare to EFS
3. EBS is used OS disk and databases, EFS can be used for file storages
4. EBS size is fixed. EFS will be scaled automatically
5. EFS is based on NFS(Network File Sharing) protocol. 2049 port
6. EBS and EFS should be mounted to any instance to use.
7. You can have any filesystem attached to EBS, but EFS use NFS, we can't change.

EFS static
=============
1. install drivers(Link - https://github.com/kubernetes-sigs/aws-efs-csi-driver)
kubectl apply -k \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-2.1"
	
   	kubectl get pods -n kube-system --  check the efs csi drivers installed as pods	
	
2. give permissions
    select the worker node security tabs
	click on Iam roles
	Add permission and Attach "AmazonEFSCSIDriverPolicy" to manage volumes
3. create EFS
4. open port no 2049 on EFS to allow traffic from EC2 worker nodesi.e., add worker node security group to EFS security inbound rules with NFS protocol
5. create PV, PVC and POD

https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/specs/pv.yaml -- ex for pv

ex for PV
https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/specs/pv.yaml

kubectl apply -f 05-efs-pv.yaml
 
kubctl get pv


 kubectl apply -f 06-efs-pvc.yaml
 kubectl get pvc   
 kubectl get pods

kubectl exec -it efs-static -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Static EFS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

 kubectl delete -f 06-efs-pvc.yaml
 kubectl delete -f 05-efs-pv.yaml

Try creating again the data in EFS wiil be shown
kubectl apply -f 05-efs-pv.yaml
 kubectl apply -f 06-efs-pvc.yaml

check : http://<LB_DNS_NAAME>:80


EFS Dynamic
=============
1. create storage class
2. install drivers
3. give permissions

# For Dynamic EFs also we need to create EFS and provide the ID in fileSystemId

kubectl apply -f 07-efs-sc.yaml
kubectl get sc

# kubectl apply -f 08-efs-dynamic.yaml
# kubectl get pv
# kubectl get pvc

For Dynamic EFS  access points will be created

kubectl get pods

kubectl exec -it efs-dynamic -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Dynamic EFS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

 kubectl delete -f 08-efs-dynamic.yaml
 kubectl delete -f 07-efs-sc.yaml

Try creating again the data in EFS wiil be shown
kubectl apply -f 07-efs-sc.yaml
 kubectl apply -f 08-efs-dynamic.yaml
check : http://<LB_DNS_NAAME>:80

data is not coming which is working static EFS -- need to cross check

StatefulSet vs Deployment
==============
1. Deployment is for stateless applications, generally frontend and backend
2. StatefulSet is for stateful applications usually databases.
3. StatefulSet will have headless service, deployment will not have headless service
4. PV, PVC are mandatory for statefulset
5. pods in statefulset create in orderly manner with static names. statefulset-0, statefulset-1
-------------------------------------------------------------------------------------------------------------------------------------------------
session 53
-------------------------------------------------------------------------------------------------------------------------------------------------
Deployment
==========
kubectl apply -f 17-deployment-service.yaml

kubectl get svc
kubectl get deployment
kubectl get rs or kubectl get replicaset
kubectl get pods
kubectl get pods -o wide 

kubectl exec -it podname -- bash

when you hit nslookup on service name, you will get ClusterIP as address

apt update -y
apt install dnsutils -y
nslookup nginx


Statefulset
===========
You need to attach headless service to statefulset. Why?

what is headless service? headless service will have no ClusterIP. It will be attached to statefulset

PV and PVC are mandatory for statefulset

Create Dynamic EBS
----------------
1. install drivers
2. give permissions to worker nodes
3. create storage class

kubectl apply -f volumes/03-ebs-sc.yaml

kubectl apply -f 18-statefulset.yaml

kubectl get svc

	For statefulset, service will be created with ClusterIP as None

kubectl get pods

kubectl get pv,pvc

pods in statefulset will be created in orderly manner i.e., one by one not all at a time like in Deployment
deployment pods names are choosen as random. but statefulset pods names are unique. <statefulset-name>-0, <statefulset-name>-1

For deployment pods will be created with the format below

<Deployment_Name>-<RS_RandomNumber>-<Pod_RandomNumber>

For statefulset pods will be created with the format below

<statefulset-name>-0, <statefulset-name>-1

why statefulset have same names, pods preserve their identity?

stateful set creates pods with same old names after deletion

nginx-0 delete pod, 
statefulset creates immediately another pod nginx-0  with same name and it should attach to its own storage through naming convention. persistentvolumeclaim/www-nginx-0
i.e., persistentvolumeclaim/<volumeName>-<PodName>

kubectl delete pod nginx-o

after deletion new pod will be created and will be attached to the same volume

kubectl exec -it nginx-0 -- bash

apt install dnsutils -y
nslookup nginx-headless

kubectl get pods -o wide 

nslookup will give pod ip address not ClusterIP address which is shown for deployment
Giving pod ip address on nslookup is exactly required for databases
When create request comes to one worker node,to sync that data to other worker nodes in the cluster it should know the other nodes available in the cluster.
To get other nodes info it will nslookup on headless service

So headless service is required in statefulset so that  it can update to other nodes using pod ipadresses.


Deployment vs Statefulset
============================
1. Deployment is for stateless applications like frontend, backend, etc. Statefulset is for DB related applications like MySQL, ELK, Prometheus, Queue apps, etc.
2. Statefulset requires headless service and normal service to be attached. Deployment will not have stateless service
3. PV and PVC are mandatory to statefulset, they create individual storages for each pod. PV and PVC for deployment creates single storage
4. Statefulset pods will be created in orderly manner. Deployment pods will be created parellely
5. Deployment pods names are choosen random. Statefulset pods keep the same identity. like <statefulset-name>-0, -1, etc.

For statefulset pods names will be unique not random because each pod in statefulset have seperate storage i.e., PV and PVC and
if a pod is deleted it will be recreated with same old name not random name so that new pod can attach to the same storage/volume again

kubectl delete -f 18-statefulset.yaml

Statefulset requires normal service also for load balancing and  headless service  for clustering

Headless service and Normal service are same except headless service has clusterIP: None

kubectl delete -f volumes/03-ebs-sc.yaml

kubectl delete -f 18-statefulset.yaml

kubectl delete pv,pvc --all -A

# If we delete the PV and PVC  with below command
#   kubectl delete pv,pvc --all -A

# New Volumes will be created again

kubectl apply -f volumes/03-ebs-sc.yaml
kubectl apply -f 18-statefulset.yaml

# If we delete only storage class and statefulset new volumes are not created

cd 21-k8-expense-volumes  --- expense project using volumes
=========================
kubectl apply -f 01-namespace.yaml

Create Dynamic EBS
----------------
1. install drivers
2. give permissions to worker nodes
3. create storage class

mysql
------
  kubectl apply -f manifest.yaml 

use kubens  to set namespace

k9s installation
----------------
https://github.com/derailed/k9s

curl -sS https://webinstall.dev/k9s | bash  --- command to install k9s

Open new tab and Type k9s 

Type shift+:(pod or service or namespace)

Type shift+:+pod and then 
type s to get pod shell access
type l to get pod logs


mysql -u root -pExpenseApp@1
show databases;
use transactions;
show tables;

backend
--------
  kubectl apply -f manifest.yaml 

frontend
--------
  kubectl apply -f manifest.yaml  

Mount nginx.conf file as volume using ConfigMap  

Autoscaling
=============
Vertical, Horizontal

Vertical scaling --> Increasing resources in of same server---> 2CPU, 4GB, 20GB HD --> 4CPU 8GB 100GB HD
Horizontal scaling --> create another server with 2CPU, 4GB, 20GB HD and add to LB

There is down time in Vertical scaling and no downtime in horizontal scaling

4 individual houses, 

HPA in K8
------------
HPA - Horizontal Pod Autoscaler
We should metrics server installed in k8

kubectl top pod

kubectl top pod -n expense

kubectl top hpa -n expense

while true;do curl <ALB_URN>;done  --- this will not send parallel requests

Apache Bench
------------
Tool  to send parallel requests

sudo yum install httpd-tools -yaml
ab --help
ab -n 50000 -c 100 <ALB_URL>  --- / at the end is required


kubectl apply -f 01-namespace.yaml
kubectl apply -f 03-ebs-sc.yaml
kubectl apply -f 01-mysql/manifest.yaml
kubectl apply -f 02-backend/manifest.yaml
kubectl apply -f 03-frontend/manifest.yaml 

kubectl delete -f 01-mysql/manifest.yaml
kubectl delete -f 02-backend/manifest.yaml
kubectl delete -f 03-frontend/manifest.yaml 
kubectl delete -f 03-ebs-sc.yaml

kubectl delete pv,pvc --all -A
If we delete the PV and PVC  with below command
  kubectl delete pv,pvc --all -A

New Volumes will be created again

If we delete only storage class new volumes are not created
and pods will connect to existing volumes

Test the app 
http://<LB_URN>:80/

ab -n 50000 -c 100 http://afdb66a07d5594006b4825856aa3011c-210927200.us-east-1.elb.amazonaws.com/
-------------------------------------------------------------------------------------------------------------------------------------------------
session 54 -- HELM PENDING
-------------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------------
session 55
-------------------------------------------------------------------------------------------------------------------------------------------------
Selectors
==============
Scheduler --> it will decide where to run your pod

You can use any of the following methods to choose where Kubernetes schedules specific Pods:

nodeSelector field matching against node labels
Affinity and anti-affinity
nodeName field
Pod topology spread constraints

nodeSelector:
	az: 1a
	
taints and tolerations
affinity and anti-affinity

kubectl apply -f 01-node-selector.yaml

If workes nodes are in 1a and 1b zone but the nodeselector is given as 1c then
Pod will not create and it will be in Pending status

kubectl delete -f 01-node-selector.yaml

Errors in K8
==============
ErrImagePull --> if node is not able to pull the image
CrashLoopBackOff --> Container is unable to start
Pending --> worker node not availalbe in that AZ, PVC is not bound to PV
ContainerCreating --> PV, PVC

taint --> paint or pollute

if we taint the node, scheduler will not schedule any pods on to that node

kubectl get nodes
kubectl get nodes --show-labels
kubectl taint nodes ip-192-168-43-212.ec2.internal hardware=gpu:NoSchedule

kubectl apply -f 01-taint-node-selector.yaml

17.182 --> 1b
43.212 --> 1a and tainted
51.139 --> 1a

NoSchedule --> order  -- will not schedule any pods in future and cureent pods will be running
PreferNoSchedule --> request
NoExecute --> taint the node which has already few pods running -- deletes the existing pods

kubectl get pods -o wide
kubectl taint nodes ip-192-168-43-239.ec2.internal hardware=gpu:NoExecute
kubectl get pods -o wide

Worker nodes are tainted when dedicated worker nodes are required 

toleration --> allow -- the pods to run on tainted nodes

kubectl apply -f 02-toleration.yaml

command to get the nodes with taints

kubectl get nodes -o json | jq '.items[].spec'
kubectl get nodes -o json | jq -r '.items[] | select(.spec.taints) | .metadata.name'

a dedicated worker nodes are there project savings bank project, it means any other project related pods will not be scheduled here

tolerations will not 100% guarantee that pods will run on the tainted nodes.. 
because of resources and other pods in the node

try schedule the pods to tainted nodes without toleration configuration then
the pod will not work and it will be in Pending status

kubectl apply -f 03-tolerations.yaml

Untaint nodes
--------------
kubectl taint nodes ip-192-168-43-212.ec2.internal hardware=gpu:NoSchedule-
kubectl taint nodes ip-192-168-43-239.ec2.internal hardware=gpu:NoExecute-

taint the nodes again and remove nodeSelector configuration

kubectl taint nodes ip-192-168-43-212.ec2.internal hardware=gpu:NoExecute
kubectl taint nodes ip-192-168-43-239.ec2.internal hardware=gpu:NoExecute

kubectl apply -f 04-tolerations.yaml

Pod will be scheduled to other untainted node since there is no nodeSelector configuration

Affinity
--------
tolerations will not 100% guarantee that pods will run on the tainted nodes.. 
because of resources and other pods in the node 

pod may run in other node

to force the pod to run on specific node we ill use affinity

nodeSelector vs affinity
-------------------------
nodeSelector is one option but affinity has more options than nodeSelector

nodeSelector has only key value pair but affinity has multiple options
You can use the operator field to specify a logical operator for Kubernetes to use 
when interpreting the rules. You can use In, NotIn, Exists, DoesNotExist, Gt and Lt.

Node affinity is conceptually similar to nodeSelector, 
allowing you to constrain which nodes your Pod can be scheduled on based on node labels.

kubectl apply -f 05-node-affinity.yaml

kubectl get pods

Pod will be in Pending status Add the labels to the nodes

Node Labels
-----------
kubectl get nodes
kubectl label nodes <Node_Name> hardware=gpu

kubectl label nodes ip-192-168-32-164.ec2.internal hardware=gpu
kubectl label nodes ip-192-168-33-103.ec2.internal hardware=gpu

After adding label the pod will be in running status

kubectl get node

situation
==========
node is tainted --> can scheduler run the pod on that node? --> no
node is tainted --> pod asks for toleration --> can scheduler run the pod --> may be(if resources are not free or scheduler decided another node) --> Running
node is tainted --> pod asks for toleration and nodeSelector --> If tainted have resources(Running) --> if tainted nodes don't have any resources(Pending)

requiredDuringSchedulingIgnoredDuringExecution --> Schedule the pod and execute --> hard rules, labels must be availalbe while scheduling

If pod labels are changed between scheduling and execution scheduler will ignore the changes at the time of execution

preferredDuringSchedulingIgnoredDuringExecution --> Soft rule --> if labels are not availalbe then consider schedule on the node..

AntiAffinity
------------
opposite to affinity uses NotIn operatori.e., schedules the pod in the node which doesn't match criteria

kubectl apply -f 06-anti-affinity.yaml

podAffinity
-----------
In pod affinity both the pods will run in the same node
Pod affinity will be used for Cache

kubectl apply -f 07-pod-affinity.yaml
kubectl get pods -o wide

podAntiAffinity
---------------
In pod antiaffinity both the pods will not run in the same node

kubectl apply -f 08-pod-affinity.yaml
kubectl get pods -o wide

backend --> DB 
backend --> Cache --> DB --> Cache -> 10 days rice store

backend --> node-1
Cache --> node-2

traffic should come out from node-1 and enter into node-2 and then Cache pod

pod-1 --> 17.182
pod-2 --> 

Ingress Controller
===================
Classic --> Old generation
ALB --> New generation --> host based routing

LoadBalancer service in kubernetes will create classic Load Balancer which will not support advanced rules
so we will use ingress contoller which provides new features similar to ALB

netbanking.hdfcbank.com --> netbanking target groups
sms.hdfcbank.com --> sms banking target group

url path hdfcbank.com/netbanking --> netbanking

by using ingress controller, we can expose application running in K8 to outside world.

setup ingress controller
use ingress resources --> routing rules

Ingress controller is called as AWS Load Balancer Controller

https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/deploy/installation/

-------------------------------------------------------------------------------------------------------------------------------------------------
session 56
--------------------------------------------------------------------------------------------------------------------------------
R53 --> ALB --> Listener --> Rule --> Target Group(VM)/Target Group(Pod)

Ingress controller is called as AWS Load Balancer Controller in AWS

https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/deploy/installation/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/ingress/annotations/

Steps to create ingress controller in AWS
--------------------------------------------
eksctl utils associate-iam-oidc-provider \
    --region <region-code> \
    --cluster <your-cluster-name> \
    --approve

Install Dirvers and Set IAM policy 
Refer ReadMe.MD file	in 25-k8-ingress repo

To check Load balancer drivers which will create,manage and automate Load Balancers

kubectl get pods -n kube-system
	
daws82s.online

app1.daws82s.online --> app1 related pods
app2.daws82s.online --> app2 related pods

Ingress resource
=================
Ingress Controller is used to provide external access to the applications running in kubernetes.
 We can set the routing rules through ingress resource either path based or host based. 
 in EKS ingress resource can create ALB, Listener, Rule and target group. 
 Ingress is attached to service, so it fetch the pods and add them to target group

app1(25-k8-ingress/app1)
------------------------
Build image for app1 and push to docker hub

docker build -t dath1/app1:1.0.0 .
dokcer login -u dath1
docker push dath1/app1:1.0.0

set the annotaions in ingress to create LB
create certificate for the domain  *.raj82s.online and copy the certificate arn for https

kubectl apply -f manifest.yaml
kubectl get ingress

ADD dns record for the *.raj82s.online with LB DNS name

app2(25-k8-ingress/app2)
--------------------------
Build image for app2 and push to docker hub

docker build -t dath1/app2:1.0.0 .
dokcer login -u dath1
docker push dath1/app2:1.0.0

Ingres will check if there is LoadBalancer in joindevops group and 
it will not create LoadBalancer again
it will just add the rules

kubectl apply -f manifest.yaml
kubectl get ingress
 

RBAC --> Role based access control
======
Authentication and Authorization

Nouns and Verbs

Nouns --> what are the resources we have
Verbs --> What actions you can take on those resources

Role and RoleBinding used to provide access to namespace level resources
ClusterRole and ClusterRoleBinding used to provide access to cluster level resources

IAM
Nouns --> EC2, VPC, R53, CDN, EKS, IoT, etc.

Fresher --> deleteEC2 --> no
CRUD
Fresher --> readEC2
Junior --> createEC2
Senior --> creaEC2, readEC2, updateEC2
Team lead --> deleteEC2

User, Role, Rolebinding

expense-trainee --> read access to expense namespace
role should be bound to user --> through rolebinding
EKS is one platform, AWS is another platform
k8 has its own RBAC, AWS has its own

AWS integrates IAM service as authentication mechanism to EKS

a user should describe EKS to connect it..

Create EKS describe policy for user  suresh and also create user

1. create IAM user and provide describeEKSCluster access
2. we need to create role and rolebinding resources
3. aws-auth configmap need to be configured to connect EKS and IAM


Create IAM User suresh
-----------------------
create IAM policy with "ExpenseEKSDescribe" name in AWS
Add the EKS services and provide the "DescribeCluster" permissions/actions user can perform
	Provide Region
	Provide cluster name
	ARN "arn:aws:eks:us-east-1:311141550186:cluster/expense" added automatically
create suresh user
Attach IAM policy to the user
create access key for suresh account
create new EC2 instance for user suresh and follow the steps below
	aws configure
	aws eks update-kubeconfig --region us-east-1 --name expense  -- update aws eks config to get cluster access
	aws sts get-caller-identity
	Install kubectl -- to interact with cluster

26-k8-rbac  -- project setup

kubectl apply -f 01-role.yaml

kubectl get role
kubectl get rolebinding

Edit aws-auth configmap( Grant IAM Users access to kubernetes with configMap)
-----------------------
kubectl get configmap aws-auth -n kube-system -o yaml

copy the output to aws-auth.yaml file and add AWS suresh user and ARN configurtaion to map EKS user with AWS user

https://docs.aws.amazon.com/eks/latest/userguide/auth-configmap.html  -- edit aws auth configmap

kubectl get configmap aws-auth -n kube-system -o yaml

mail suresh about his config is done, he can login

Create new Ec2 instance and follow the below steps

aws configure
aws configure --profile suresh
aws sts get-caller-identity
aws sts get-caller-identity --profile suresh  ---to check the user

https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html -- aws eks update kubeconfig to get cluster access

to get cluster access update aws eks config use the below command
-----------------------------------------------------------------
aws eks update-kubeconfig --region us-east-1 --name expense

cat .kube/config 

check the access

kubectl get pods
kubectl get pods -n kube-system
kubectl get nodes 
kubectl get services

Changes the resources access to * to access all resources

-------------------------------------------------------------------------------------------------------------------------------------------------
session 57
--------------------------------------------------------------------------------------------------------------------------------
Role and RoleBinding used to provide access to namespace level resources
ClusterRole and ClusterRoleBinding used to provide access to cluster level resources

Follow the same steps used for role and added cluster role config to access cluster level resources
 uid in 02-aws-auth.yaml is optional
kubectl api-resources

give cluster level access for nodes
kubectl apply -f 03-clusterrole.yaml

ServiceAccount
================
Service Account:  default
when you create namespace or every namespace will have default sa
sa is a non human account, it is pod identity with which it can connect with api server and get access to external services

kubectl apply -f 01-namespace.yaml
kubectl apply -f 02-pod.yaml
kubectl get pods -n expense 
kubectl describe pods nginx -n expense -- check there is default service account

Pod will create LB,rules and targetgroups in Ingress using service accounts.

pod should access aws secret manager
-------------------------------------
Kuberentes secrets are encoded but not encrypted
AWS secrets are encrypted

1. make sure oidc provider exist
eksctl utils associate-iam-oidc-provider \
    --region us-east-1 \
    --cluster expense \
    --approve

2. create secret and policy in AWS
2a)create secret "expense/mysql/creds" in AWS with key value pair
   key		value
   ----------------
   username	admin
   password	admin123
   
2b)Create IAM policy "ExpenseMySQLSecretRead" with "SecretManager" service with read "GetSecretValue" Permission and 
give access to specific resource i.e., secret created

	Edit ARN and provide below
		Resource region
		copy and paste ARN of secret created in ARN field
		secret field will be automatically filled
3. we need to map sa with IAM policy(Update with kubernetes namespace and policy arn created in aws in below command)
	For internal resources create role and roldebinding for service account
	For external resources access attach IAM policy to service account
	
eksctl create iamserviceaccount \
--cluster=expense \
--namespace=expense \
--name=expense-mysql-secret \
--attach-policy-arn=arn:aws:iam::311141550186:policy/ExpenseMySQLSecretRead \
--override-existing-serviceaccounts \
--region us-east-1 \
--approve

this command will create IAM role with policy ExpenseMySQLSecretRead and integrate with EKS SA

kubectl get sa -n expense
kubectl describe sa expense-mysql-secret -n expense
kubectl get sa expense-mysql-secret -n expense -o yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::315069654700:role/eksctl-expense-addon-iamserviceaccount-expens-Role1-XUPebrTfkrjM
  creationTimestamp: "2025-03-17T02:22:46Z"
  labels:
    app.kubernetes.io/managed-by: eksctl
  name: expense-mysql-secret
  namespace: expense
  resourceVersion: "7864"
  uid: 8c97086a-9360-4a82-b77f-142fe47ce1c2
  
Verify Role "eksctl-expense-addon-iamserviceaccount-expens-Role1-XUPebrTfkrjM" will be created in AWS and 
attached to Service Account  

InitContainers
==================
InitContainers are used for setup the requirements for pod, for example backend pod can check whether database connections working fine or not
InitContainers containers can fetch the secrets for pod before it starts

InitContainers are special containers run before main container runs, we can use them to make sure dependent services are running fine before our main application starts and fetch the secrets from secretmanager for main container. they always run to completion. you can run one or many init containers, they executed in sequence

InitContainers can do heavy lifting for main container so that main container is less in size and limit attack surface of main container by not installing more tools in it..

kubectl apply -f 20-init-container.yaml

aws secretsmanager get-secret-value \
    --secret-id expense/mysql/creds --query SecretString --output text
	
Create Pod to get secretes with init contianer	

volumes --> external volumes --->EBS,EFS and configMap as volume

internal volumes --> emptyDir and hostPath

emptyDir --> pod temporary storage, accessible until pod lives. all containers in the pod can access
init container stores the secret in emptyDir and completes. then main container can acess this

kubectl apply -f 19-sa.yaml
kubectl get pods -n expense
kubectl exec -it sa-demo -n expense -- bash
cat /secrets/.env

To check the pods access running in default namespace from the pod in expense namespace after connecting to sa-demo pod created using 19-sa.yaml
-------------------------------------------------------------------------------------------------
kubectl exec -it sa-demo-2 -n expense -- bash
Install kubectl
kubectl get pods
kubectl get pods -n default

kubectl get secrets
kubectl get secrets -n default

you will get permission error to access this again we need to use sa with role and rolebinding
exit the pod

Not only to users, role and rolebinding can be created for ServiceAccount also

kubectl get sa expense-mysql-secret -n expense -o yaml

Add role and role binding with Kind as ServiceAccount in 21-sa.yaml
kubectl apply -f 21-sa.yaml
kubectl exec -it sa-demo-2 -n expense -- bash
Install kubectl
kubectl get pods
kubectl get pods -n default

kubectl get secrets
kubectl get secrets -n default


what is serviceaccount?

serviceaccount is non human user account, sa is the pod identity with which it can access internal resources or external services. 
we can map sa with cloud provider IAM roles and policies. we can use sa to fetch secrets from secretmanager

what are initcontainers?

these are special containers run before main container run. 
we can run one or many init containers, all containers run in sequence. 
init containers will come to completion, we use them to check the dependencies are running fine before main container starts 
another example can be fetching the secrets and provide to main container.  
they do heavy lifting keeping the main container lightweight and 
less attack surface by not installing utility tools in main container.

what is emptyDir?
it is pod temporary storage exist until the pod lives, all containers inside pod can acess emptyDir volumes.

OIDC provider
sa --> IAM Role --> IAM Policy (external roles)
sa --> Role and RoleBinding (Internal resources access)





