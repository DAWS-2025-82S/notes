eksctl
kubectl
namespace
pod
labels
annotations
env
resources
configMapRef
secrets
services
3 types of services
	1. cluster IP --> default, only works internally i.e., pod to pod
	2. NodePort --> external requests
	3. LoadBalancer --> only works with cloud providers. here service open classic LB. 
replicaset - can be used for creating multiple pods
deployment -- to update the image in replicaset
kubens -- tool to set namespace

eksctl -- AWS command line tool to create and manage EKS cluster 
For real projects terraform is used to create EKS cluster
eksctl installation -- For cluster creation
kubectl installation  -- For cluster  interaction
We wiil use same EC2 instance used for docker to setup eksctl,kubectl

https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html
https://eksctl.io/installation/

https://github.com/eksctl-io/eksctl/blob/main/examples/01-simple-cluster.yaml

sudo tail -f /var/log/cloud-init-output.log -- to check user data output

https://kubernetes.io/  -- official doc and can be used for CKA exam

1. create one linux server as workstation
2. install docker to build images
3. run aws configure to provide authentication
4. install eksctl to create and manage EKS cluster
5. install kubectl to work with eks cluster

eksctl installation
-------------------
curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.32.0/2024-12-20/bin/linux/amd64/kubectl.sha256
chmod +x ./kubectl
sudo mv kubectl /usr/local/bin/kubectl

# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH
curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
# (Optional) Verify checksum
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo mv /tmp/eksctl /usr/local/bin

From ExampleConfigs select simple cluster

https://github.com/eksctl-io/eksctl/blob/main/examples/01-simple-cluster.yaml

eks.yaml in 16-Docker -- To create cluster

eksctl create cluster --config-file=eks.yaml
eksctl delete cluster --config-file=eks.yaml

aws configure

AWS cloudformation -- Just like terraform infra as code - AWS native service - will create resouces for cluster

kubectl get nodes -- to get the no of nodes in the cluster

namespace 
---------- 
isolated project space similar to VPC where you can control resources to your project

default namespace is created along with cluster creation

kubectl get namespace -- to get the namespaces

Kind -- kind of resource
apiVersion  -- kubernetes version
metadata 
  name -- resource name
  
spec used for other resouces 

kubectl create -f 01-namespace.yaml -- will throw error if the resource alrady exists
kubectl apply -f 01-namespace.yaml  -- will throw warning not error if the resource alrady exists
kubectl delete -f 01-namespace.yaml -- delete the resource


pod
====
docker --> image --> container
k8s    --> image --> pod

pod vs container
----------------
pod is the smallest deployable unit in k8s .pod can have multiple containers
all containers in pod share the same ip and storage
multiple containers are useful in few applications like shipping the logs through sidecars

kubectl apply -f 02-pod.yaml
kubectl get pods -- to get the list of pods
kubectl get pods -n <namespace> -- to get the list of pods in namespace

kubectl exec -it <pod-name> -- bash
kubectl exec -it nginx -- bash  # To connect to pod

To connect pod in expense namespace
kubectl exec -it nginx -n expense -- bash

kubectl apply -f 03-multicontainer.yaml
kubectl get pods
CrashLoopBackOff -- error when failed to create container

kubectl pod describe <pod-name>  -- to get the details of pod like which container is failing  in a pod

# If one container failed we need to delete the pod and create it again
kubectl delete -f 03-multicontainer.yaml

kubectl exec -it <pod-name> -- bash

kubectl exec -it multi-containers -- bash

curl  localhost

By default it will connect to first container
To connect to second contianer

kubectl exec -it <pod-name> -c <container-name> -- bash

kubectl exec -it multi-containers -c almalinux -- bash

curl localhost

curl output will be same because "all conatiners in pod share the same ip and storage"

pod or container is ephemeral

when pod is deleted application logs created by first container will also get deleted 
so we will create second container(sidecar container) to push the logs to ELK to reduce the load on first container

labels
-------
labels are used to provide metadata and can be used to attach other resources

kubectl apply -f 04-labels.yaml

annotations
------------
annotations are similar to labels 
labels can be used to select internal resources 
annotations can be used to select external resources
also in annotations we can give special characters and used in ingress controller
special characters cannot be used in labels

kubectl apply -f 05-annotations.yaml

--------------------------------------------------------------------------------------------------------
env
To set environment variables
kubectl apply -f 06-env.yaml
kubectl exec -it env -- bash
# env

resources
 requests
 limits
sets the min and max RAM and CPU for container        
kubectl apply -f 07-resources.yaml
kubectl top pod  --- to check the resources consumed by pod

ConfigMap
  data
To provide configurations as env variables

kubectl apply -f 08-config-map.yaml
kubectl get configmap
kubectl describle configmap myconfig

Referring ConfigMap to set as env variables

envFrom
 configMapRef

 kubectl apply -f 09-pod-config.yaml
 kubectl exec -it pod-config -- bash  
 env
 
 Update config map data and restart pod
kubectl apply -f 08-config-map.yaml

 kubectl delete -f 09-pod-config.yaml
 kubectl apply -f 09-pod-config.yaml
 kubectl exec -it pod-config -- bash  
 env

secrets
 data
secret types
  opaque
  
$  echo -n "testsecret" | base64
dGVzdHNlY3JldA==

$ echo "dGVzdHNlY3JldA==" | base64 --decode
testsecret
kubectl apply -f 10-secret.yaml
kubectl get secret
kubectl describe secret dotfile-secret

Referring secret to set as env variables

envFrom
 secretRef

 kubectl apply -f 11-pod-secret.yaml
 kubectl exec -it pod-secret -- bash  
 env 
 
Service
=========
service is used for 
1. load balancing
2. pod to pod communication
3. as DNS between pods

Ip address of pod will changes after deletion and creation of pod
Instead of IP we will create service to access the pod or pod to pod communication with service name

service will be cretad with clusterip
kubectl delete pods --all --all-namespaces   -- to delete all pods
kubectl delete pods --all -A

kubectl get pods --all-namespaces
kubectl get pods -A

kubectl apply -f 12-service.yaml

kubectl get svc --- to get service info -- gives cluster IP and also node port

kubectl get pod -o wide -- to get pod ip and also the node where pod is running

start other container to check service access from other container

kubectl apply -f 03-multicontainer.yaml

kubectl exec -it multi-containers -- bash

then curl <servicename>

curl nginx

apt update
apt install dnsutils -y

nslookup nginx

multi-container --> curl nginx --> 
3 types of services
1. cluster IP --> default, only works internally i.e., pod to pod
2. NodePort --> external requests
3. LoadBalancer --> only works with cloud providers. here service open classic LB. 

clusterip will not work in internet, works only in kubernetes i.e., only works internally i.e., pod to pod

Node port
---------
Node port works from internet
Node port automatically creates cluster IP and creates new node port

kubectl delete -f 12-service.yaml
kubectl apply -f 13-node-port.yaml

kubectl get svc --- to get service info -- gives cluster IP and also node port

kubectl get pod -o wide -- to get pod ip and also the node where pod is running

check the private ip address of node from aws ui
copy  the node public ip address and access with node port from browser(open port in security group)

If we acess the node port with any node ip it will work because kubernetes will send the request to the correct pod

In NodePort type
when user send request with public ip(any instance/node public IP) ip:nodeport
then kubernetes will forward the request to cluster ip service
then cluster ip service will forward the request to the pod
When we open port in security group Kubernetes will open node port on all instances(nodes) so we the url will work with any instance ip and node port

# test with url
# http:<AnyInstancePublicIP>:<NodePort>

kubectl delete -f 13-node-port.yaml

kubectl apply -f 14-load-balancer.yaml

In Load Balancer type
request will come from user to Load balancer listening on port 80 
then LB will forward to node port to any one of the instances(nodes) in the cluster
then from any one of the instances(node) request goes to cluster ip
then finally request will be sent from cluster ip to the pod

# test with url
# http:<LB_DNS_Name>:<80>

# create Route53 record for LB DNS name with domain name to access the url with domain name

replicaset - can be used for creating multiple pods
Pod is subset of replicaset
pod name = replicaset-name-random 5 digit

To create multiple pods we have to change pod name and run the 02-pod.yaml file
metadata:
  name: nginx2
  
Instead of creatng multiple pods manually we can use replicaset 

 kubectl apply -f 15-replica-set.yaml

kubectl get replicaset

kubectl get pods -o wide

watch kubectl get pods # in new terminal

if we delete one pod, replicateset will automatically creates new pod

If we run 02-pod.yaml again with same image nothing will change
If we change image name and run 02-pod.yaml file image will be updated

check the image with below command
kubectl describe pod nginx -n expense

If image is changed in replicaset with another version ,replicaset will not update the image

To achieve this we need to use deployment

Deployment 
----------
Deployment yaml is same as replicaset except that kind is Deployment

kubectl apply -f 16-deployment.yaml

kubectl get deployment

kubectl get rs or kubectl get replicaset

kubectl get pods

To verify the replicaset updates the pods with another image version
change the image name from nginx:1.14.2 to nginx:latest

kubectl apply -f 16-deployment.yaml

check deployment with image and service
---------------------------------------
docker build -t dath1/deployment:1.0 .
docker push dath1/deployment:1.0

then run the 17-deployment-service.yaml to test the deployment

Test with url http://<LB_DNS_NAME>:80

change the verion in index.html and build the image again to  test the deployment

docker build -t dath1/deployment:2.0 .
docker push dath1/deployment:2.0

build the image from DEPLOYMENT folder

kubectl apply -f 17-deployment-service.yaml
kubectl get svc
kubectl get deployment
kubectl get rs or kubectl get replicaset
kubectl get pods

Test with url http://<LB_DNS_NAME>:80

To verify the replicaset updates the pods with another image version
change the image name from 1.0 to 2.0

kubectl apply -f 16-deployment.yaml

Test with url http://<LB_DNS_NAME>:

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

Commands to check deployment status and rollback
-------------------------------------------------
kubectl rollout history deployment/nginx  -- to check the deployment history
kubectl rollout history deployment/nginx --revision=2  -- to check revision 2 history

kubectl rollout undo deployment/nginx --> roll back to immidiate previous version
kubectl rollout undo deployment/nginx --to-revision=2 ---> roll back to specific verison

kubectl get deployment nginx   -- to check the deployment status
kubectl rollout status deployment/nginx  -- to check the deployment status

Expense project using k8s
--------------------------
To avoid mentioning the namespace name (-n namespacename) in  kubectl commands install the tool kubens

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

kubens <namespace_name>  -- set the name of namespace
kubens expense

kubectl apply -f 01-namespace.yaml

mysql
kubectl apply -f manifest.yaml 

kubectl get pods -n expense
kubectl exec -it mysql-djshdsh -n expense -- bash
mysql -u root -pExpenseApp@1
show databases;
use transactions;
show tables;
   

backend
kubectl apply -f manifest.yaml    

frontend
change port 80 to 8080 in nginx.conf file in Dockerfile and build the image in 18-expense-docker repo
remove /etc/nginx/conf.d/default.conf file which has port 80

docker build -t dath1/frontend:1.1.0 .
docker push dath1/frontend:1.1.0

kubectl apply -f manifest.yaml   

kubectl logs frontend-54c88dc9ff-8xdnk 

kubernetes won't allow system ports for non root user
0-1024 are system ports -- container has to run with root user to access system ports
Non root users cannot access this ports

Either change the user to root or change the port in frontend nginx.conf from 80 to 8080

docker build -t dath1/frontend:1.2.0 .
docker push dath1/frontend:1.2.0

Test the app with url
http://<LB_DNS_NAME>:80

add route 53 record for LB DNS NAme and test the url with dns name



------------------------------------------------------------------------------------------
CrashLoopBackoff - unable to create pod from image i.e., unable to keep the pod running continuously
ErrorImagePull - unable to pull the image





