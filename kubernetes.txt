eksctl  -- tool to install cluster
kubectl
nodes
namespace
pod
labels
annotations
env
resources
configMapRef
secrets
services
3 types of services
	1. cluster IP --> default, only works internally i.e., pod to pod
	2. NodePort --> external requests
	3. LoadBalancer --> only works with cloud providers. here service open classic LB. 
replicaset - can be used for creating multiple pods
deployment -- to update the image in replicaset
kubens -- tool to set namespace
volumes 
EBS
  static
  dynamic

https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/install.md -- link for ebs csi driver install using kustomzie
PV -- Pv is wrapper object in kubernetes to represent physical volume like EBS or EFS
PVC
Storageclass -- Dynamic EBS -- creates PV and volume automatically no need to create manually
api-resources
1. namespace level
2. cluster level
EFS
  static
  dynamic
statefulset 
statefulset vs deployment
k9s installation -- https://github.com/derailed/k9s
21-k8-expense-volumes  -- project
selector
  taints
  tolerations
Affinity
 node affinity
 node anti affinity
 pod affinity
 pod anti affinity
Ingress
  LB
  TargetGroup
  
RBAC
  Role
  RoleBinding
  ClusterRole
  ClusterRoleBinding
  
Just me and Opensource -- youtube channel for kubernetes

eksctl -- AWS command line tool to create and manage EKS cluster 
For real projects terraform is used to create EKS cluster
eksctl installation -- For cluster creation
kubectl installation  -- For cluster  interaction
We wiil use same EC2 instance used for docker to setup eksctl,kubectl

https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html
https://eksctl.io/installation/

https://github.com/eksctl-io/eksctl/blob/main/examples/01-simple-cluster.yaml

sudo tail -f /var/log/cloud-init-output.log -- to check user data output

https://kubernetes.io/  -- official doc and can be used for CKA exam

1. create one linux server as workstation
2. install docker to build images
3. run aws configure to provide authentication
4. install eksctl to create and manage EKS cluster
5. install kubectl to work with eks cluster

local
-----
cd /d/GitHub/DAWS-2025-82S/repos/16-docker
terraform apply -auto-approve

 terraform destroy -auto-approve
 
copy Ip and ssh to Ec2
 
ssh ec2-user@IP
git clone https://github.com/DAWS-2025-82S/16-docker.git
git clone https://github.com/DAWS-2025-82S/19-k8s-resources.git

eksctl installation
-------------------
curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.32.0/2024-12-20/bin/linux/amd64/kubectl.sha256
chmod +x ./kubectl
sudo mv kubectl /usr/local/bin/kubectl

# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH
curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
# (Optional) Verify checksum
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo mv /tmp/eksctl /usr/local/bin

From ExampleConfigs select simple cluster

https://github.com/eksctl-io/eksctl/blob/main/examples/01-simple-cluster.yaml

eks.yaml in 16-Docker -- To create cluster

eksctl create cluster --config-file=eks.yaml
eksctl delete cluster --config-file=eks.yaml

aws configure

AWS cloudformation -- Just like terraform infra as code - AWS native service - will create resouces for cluster

kubectl get nodes -- to get the no of nodes in the cluster

namespace 
---------- 
isolated project space similar to VPC where you can control resources to your project

default namespace is created along with cluster creation

kubectl get namespace -- to get the namespaces

Kind -- kind of resource
apiVersion  -- kubernetes version
metadata 
  name -- resource name
  
spec used for other resouces 

kubectl create -f 01-namespace.yaml -- will throw error if the resource alrady exists
kubectl apply -f 01-namespace.yaml  -- will throw warning not error if the resource alrady exists
kubectl delete -f 01-namespace.yaml -- delete the resource


pod
====
docker --> image --> container
k8s    --> image --> pod

pod vs container
----------------
pod is the smallest deployable unit in k8s .pod can have multiple containers
all containers in pod share the same ip and storage
multiple containers are useful in few applications like shipping the logs through sidecars

kubectl apply -f 02-pod.yaml
kubectl get pods -- to get the list of pods
kubectl get pods -n <namespace> -- to get the list of pods in namespace

kubectl exec -it <pod-name> -- bash
kubectl exec -it nginx -- bash  # To connect to pod

To connect pod in expense namespace
kubectl exec -it nginx -n expense -- bash

kubectl apply -f 03-multicontainer.yaml
kubectl get pods
CrashLoopBackOff -- error when failed to create container

kubectl describe pod <pod-name>  -- to get the details of pod like which container is failing  in a pod

# If one container failed we need to delete the pod and create it again
kubectl delete -f 03-multicontainer.yaml

kubectl exec -it <pod-name> -- bash

kubectl exec -it multi-containers -- bash

curl  localhost

By default it will connect to first container
To connect to second contianer

kubectl exec -it <pod-name> -c <container-name> -- bash

kubectl exec -it multi-containers -c almalinux -- bash

curl localhost

curl output will be same because "all conatiners in pod share the same ip and storage"

pod or container is ephemeral

when pod is deleted application logs created by first container will also get deleted 
so we will create second container(sidecar container) to push the logs to ELK to reduce the load on first container

labels
-------
labels are used to provide metadata and can be used to attach other resources

kubectl apply -f 04-labels.yaml

annotations
------------
annotations are similar to labels 
labels can be used to select internal resources 
annotations can be used to select external resources
also in annotations we can give special characters and used in ingress controller
special characters cannot be used in labels

kubectl apply -f 05-annotations.yaml

--------------------------------------------------------------------------------------------------------
env
To set environment variables
kubectl apply -f 06-env.yaml
kubectl exec -it env -- bash
# env

resources
 requests
 limits
sets the min and max RAM and CPU for container 
       
kubectl apply -f 07-resources.yaml
kubectl top pod  --- to check the resources consumed by pod

ConfigMap
  data
To provide configurations as env variables

kubectl apply -f 08-config-map.yaml
kubectl get configmap
kubectl describle configmap myconfig

Referring ConfigMap to set as env variables

envFrom
 configMapRef

 kubectl apply -f 09-pod-config.yaml
 kubectl exec -it pod-config -- bash  
 env
 
 Update config map data and restart pod
kubectl apply -f 08-config-map.yaml

 kubectl delete -f 09-pod-config.yaml
 kubectl apply -f 09-pod-config.yaml
 kubectl exec -it pod-config -- bash  
 env

secrets
 data
secret types
  opaque
  
$  echo -n "testsecret" | base64
dGVzdHNlY3JldA==

$ echo "dGVzdHNlY3JldA==" | base64 --decode
testsecret
kubectl apply -f 10-secret.yaml
kubectl get secret
kubectl describe secret dotfile-secret

Referring secret to set as env variables

envFrom
 secretRef

 kubectl apply -f 11-pod-secret.yaml
 kubectl exec -it pod-secret -- bash  
 env 
 
Service
=========
service is used for 
1. load balancing
2. pod to pod communication
3. as DNS between pods

Ip address of pod will changes after deletion and creation of pod
Instead of IP we will create service to access the pod or pod to pod communication with service name

service will be created with clusterip
kubectl delete pods --all --all-namespaces   -- to delete all pods
kubectl delete pods --all -A

kubectl get pods --all-namespaces
kubectl get pods -A

kubectl apply -f 12-service.yaml

kubectl get svc --- to get service info -- gives cluster IP and also node port

kubectl get pod -o wide -- to get pod ip and also the node where pod is running

start other container to check service access from other container

kubectl apply -f 03-multicontainer.yaml

kubectl exec -it multi-containers -- bash

then curl <servicename>

curl nginx

apt update
apt install dnsutils -y

nslookup nginx

multi-container --> curl nginx --> 
3 types of services
1. cluster IP --> default, only works internally i.e., pod to pod
2. NodePort --> external requests
3. LoadBalancer --> only works with cloud providers. here service open classic LB. 

clusterip will not work in internet, works only in kubernetes i.e., only works internally i.e., pod to pod

Node port
---------
Node port works from internet
Node port automatically creates cluster IP and creates new node port

kubectl delete -f 12-service.yaml
kubectl apply -f 13-node-port.yaml

kubectl get svc --- to get service info -- gives cluster IP and also node port

kubectl get pod -o wide -- to get pod ip and also the node where pod is running

check the private ip address of node from aws ui
copy  the node public ip address and access with node port from browser(open port in security group)

If we acess the node port with any node ip it will work because kubernetes will send the request to the correct pod

In NodePort type
when user send request with public ip(any instance/node public IP) ip:nodeport
then kubernetes will forward the request to cluster ip service
then cluster ip service will forward the request to the pod
When we open port in security group Kubernetes will open node port on all instances(nodes) so we the url will work with any instance ip and node port

# test with url
# http:<AnyInstancePublicIP>:<NodePort>

kubectl delete -f 13-node-port.yaml

kubectl apply -f 14-load-balancer.yaml

In Load Balancer type
request will come from user to Load balancer listening on port 80 
then LB will forward to node port to any one of the instances(nodes) in the cluster
then from any one of the instances(node) request goes to cluster ip
then finally request will be sent from cluster ip to the pod

# test with url
# http:<LB_DNS_Name>:<80>

# For LB service type no need to mention node port and open in secuirty group

# create Route53 record for LB DNS name with domain name to access the url with domain name

replicaset - can be used for creating multiple pods
Pod is subset of replicaset
pod name = replicaset-name-random 5 digit

To create multiple pods we have to change pod name and run the 02-pod.yaml file
metadata:
  name: nginx2
  
Instead of creatng multiple pods manually we can use replicaset 

 kubectl apply -f 15-replica-set.yaml

kubectl get replicaset

kubectl get pods -o wide

watch kubectl get pods # in new terminal

if we delete one pod, replicateset will automatically creates new pod

If we run 02-pod.yaml again with same image nothing will change
If we change image name and run 02-pod.yaml file image will be updated

check the image with below command
kubectl describe pod nginx -n expense

If image is changed in replicaset with another version ,replicaset will not update the image

To achieve this we need to use deployment

Deployment 
----------
Deployment yaml is same as replicaset except that kind is Deployment

kubectl apply -f 16-deployment.yaml

kubectl get deployment

kubectl get rs or kubectl get replicaset

kubectl get pods

To verify the replicaset updates the pods with another image version
change the image name from nginx:1.14.2 to nginx:latest

kubectl apply -f 16-deployment.yaml

check deployment with image and service
---------------------------------------
docker build -t dath1/deployment:1.0 .
docker push dath1/deployment:1.0

then run the 17-deployment-service.yaml to test the deployment

Test with url http://<LB_DNS_NAME>:80

change the verion in index.html and build the image again to  test the deployment

docker build -t dath1/deployment:2.0 .
docker push dath1/deployment:2.0

build the image from DEPLOYMENT folder

kubectl apply -f 17-deployment-service.yaml
kubectl get svc
kubectl get deployment
kubectl get rs or kubectl get replicaset
kubectl get pods

Test with url http://<LB_DNS_NAME>:80

To verify the replicaset updates the pods with another image version
change the image name from 1.0 to 2.0

kubectl apply -f 16-deployment.yaml

Test with url http://<LB_DNS_NAME>:80

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

Commands to check deployment status and rollback
-------------------------------------------------
kubectl rollout history deployment/nginx  -- to check the deployment history
kubectl rollout history deployment/nginx --revision=2  -- to check revision 2 history

kubectl rollout undo deployment/nginx --> roll back to immediate previous version
kubectl rollout undo deployment/nginx --to-revision=2 ---> roll back to specific verison

kubectl get deployment nginx   -- to check the deployment status
kubectl rollout status deployment/nginx  -- to check the deployment status

Expense project using k8s
--------------------------
To avoid mentioning the namespace name (-n namespacename) in  kubectl commands install the tool kubens

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

kubens <namespace_name>  -- set the name of namespace
kubens expense

kubectl apply -f 01-namespace.yaml

mysql
kubectl apply -f manifest.yaml 

kubectl get pods -n expense
kubectl exec -it mysql-djshdsh -n expense -- bash
mysql -u root -pExpenseApp@1
show databases;
use transactions;
show tables;
   

backend
kubectl apply -f manifest.yaml    

frontend
change port 80 to 8080 in nginx.conf file in Dockerfile and build the image in 18-expense-docker repo
remove /etc/nginx/conf.d/default.conf file which has port 80

docker build -t dath1/frontend:1.1.0 .
docker push dath1/frontend:1.1.0

kubectl apply -f manifest.yaml   

kubectl logs frontend-54c88dc9ff-8xdnk 

kubernetes won't allow system ports for non root user
0-1024 are system ports -- container has to run with root user to access system ports
Non root users cannot access this ports

Either change the user to root or change the port in frontend nginx.conf from 80 to 8080

docker build -t dath1/frontend:1.2.0 .
docker push dath1/frontend:1.2.0

Test the app with url
http://<LB_DNS_NAME>:80

add route 53 record for LB DNS NAme and test the url with dns name

Volumes
============
everything in k8 is ephemeral. pods, nodes, master node all are ephemeral. So it is not good to store DB inside K8

Storage administration --> manage all the disks related to servers
Backup every day
Restore testing
N/w to storage

EBS -> Elastic block storage  like harddisk
EFS -> Elastic file system    like google storage

If server is in AZ us-east-1b, EBS also should be here. EBS is less latency. EBS is well suitable for DB and OS

conditions for EBS
---------------------------
1. node and EBS volume should be in same AZ
2. drivers install( to access EBS from EKS)
3. EC2 worker nodes should have permission to work with EBS volumes

static and dynamic provisioning
=================================
static means we have to create disk(volume) manually
Dynamic means EKS will create the Disk(volume)

EBS static
==========
1.we have to create disk(volume) manually in same AZ as in worker nodes 
2.drivers install(ebs csi driver) using kustomize
	https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/install.md
	kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.40"
	
	kubectl get pods -n kube-system --  check the drivers installed as pods	

3.select the worker node security tabs
	click on Iam roles
	Add permission and Attach "AmazonEBSCSIDriverPolicy" to manage volumes

4.Mount the volume to the pod using PV and PVC
	
everything in k8 is resource/object. if you can treat or manage the volumes from k8 objects any k8 engineer can work on this..

PV --> Persistant volume, represents or wrapper of physical volume
PVC --> Claiming the volume
Storage class

PV is wrapper object in kubernetes to represent physical volume like EBS or EFS

kid --> mother --> father --> wallet
Pod	-->  PVC --> PV -->  volume

https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/static-provisioning/manifests/pv.yaml  -- for creating PV

access modes
============
ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod,

ReadWriteOnce - At a time only one node can read and write with multiple pods

ReadWriteOncePod -- for only one pod

Lifecycle policies(i.e., what will happen to data if the pod is deleted)
=================

Retain -- data and volume will not delete
delete -- data and volume will be deleted
recycle -- data will delete but not volume

kubectl apply -f 01-ebs-static-pv.yaml
kubectl get pv

create PVC  -- to claim the volume
https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/static-provisioning/manifests/claim.yaml -- ex to create PVC

kubectl apply -f 02-ebs-static-pvc.yaml
kubectl get pvc

Pod should claim that PVC

all the files/data of pod stored in /usr/share/nginx/html will be stored in volume

kubectl get pods

kubectl get nodes --show-labels  ----> to get nodeSelector label

if pod schedules(created) in 1a and volume is in 1b, pod status will be in pending for continuously.

scheduler(nodeSelector) --> schedules your pods on to appropriate nodes

kubectl apply -f 02-ebs-static-pvc.yaml

kubectl exec -it ebs-static -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from EBS static</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

Delete all resources(PV,PVC,pod,service)
kubectl delete -f 02-ebs-static-pvc.yaml
kubectl delete -f 01-ebs-static-pv.yaml

all resources got deleted but not volume and data
try recreate the pod and verify the data in volume is not deleted

kubectl apply -f 01-ebs-static-pv.yaml
kubectl apply -f 02-ebs-static-pvc.yaml

check : http://<LB_DNS_NAAME>:80

permanent storage
=================
1. SAN - Storage Area Network
2. K8 Admin
3. Expense DevOps Engineer --> they get access to only expense namespace

Expense DevOps engineer send a mail to SAN with the manager approval. SAN will take their manager approval and create storage for us.

We will forward disk details to k8 admin and ask them to create PV.

In kubernetes there are 2 type of resources

kubectl api-resources

1. namespace level
2. cluster level -- we will not have access to create clusterlevel resources so we will send request to create PV which is cluster level resource

they create PV and send the name for us..

then we will create PVC, pod to access the storage

Kid --> Mom --> Dad --> Wallet
Pod --> PVC --> PV  --> Storage

EBS Static
============
1. install drivers
2. give permissions to worker nodes
3. create volume
4. create PV, PVC and cliam through pod
5. EC2 and EBS should be in same az incase of EBS provisioning

kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.40"  -- driver install

watch kubectl get pods # to monitor pods

EBS dynamic
============

Kid --> Mom --> UPI
Pod --> PVC --> SC

StorageClass --> this object is responsible for dynamic provisioning. it will create external storage(volume) and PV automatically

kubectl get sc

1. install drivers
2. give permissions to worker nodes
3. create storage class

https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/dynamic-provisioning/manifests/storageclass.yaml -- sc example

kubectl apply -f 03-ebs-sc.yaml
kubectl get sc

 StorageClass should be empty for static provisionin(EBS Static)
 Provide StorageClass name for dynamic provisioning(EBD Dynamic)
kubectl apply -f 04-ebs-dynamic.yaml

kubectl get pvc
kubectl get pv
kubectl get pods
kubectl get svc

check : http://<LB_DNS_NAME>:80   --- no output

kubectl exec -it ebs-dynamic -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Dynamic EBS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

kubectl apply -f 04-ebs-dynamic.yaml

Volume will not delete we have to delete it manually

EFS --> Elastic file system
-----------------------------
1. EBS should be in same AZ as in EC2, EFS can be anywhere in n/w
2. EBS is speed compare to EFS
3. EBS is used OS disk and databases, EFS can be used for file storages
4. EBS size is fixed. EFS will be scaled automatically
5. EFS is based on NFS(Network File Sharing) protocol. 2049 port
6. EBS and EFS should be mounted to any instance to use.
7. You can have any filesystem attached to EBS, but EFS use NFS, we can't change.

EFS static
=============
1. install drivers(Link - https://github.com/kubernetes-sigs/aws-efs-csi-driver)
kubectl apply -k \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-2.1"
	
   	kubectl get pods -n kube-system --  check the efs csi drivers installed as pods	
	
2. give permissions
    select the worker node security tabs
	click on Iam roles
	Add permission and Attach "AmazonEFSCSIDriverPolicy" to manage volumes
3. create EFS
4. open port no 2049 on EFS to allow traffic from EC2 worker nodesi.e., add worker node security group to EFS security inbound rules with NFS protocol
5. create PV, PVC and POD

https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/specs/pv.yaml -- ex for pv

ex for PV
https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/specs/pv.yaml

kubectl apply -f 05-efs-pv.yaml
 
kubctl get pv


 kubectl apply -f 06-efs-pvc.yaml
 kubectl get pvc   
 kubectl get pods

kubectl exec -it efs-static -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Static EFS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

 kubectl delete -f 06-efs-pvc.yaml
 kubectl delete -f 05-efs-pv.yaml

Try creating again the data in EFS wiil be shown
kubectl apply -f 05-efs-pv.yaml
 kubectl apply -f 06-efs-pvc.yaml

check : http://<LB_DNS_NAAME>:80


EFS Dynamic
=============
1. create storage class
2. install drivers
3. give permissions

# For Dynamic EFs also we need to create EFS and provide the ID in fileSystemId

kubectl apply -f 07-efs-sc.yaml
kubectl get sc

# kubectl apply -f 08-efs-dynamic.yaml
# kubectl get pv
# kubectl get pvc

For Dynamic EFS  access points will be created

kubectl get pods

kubectl exec -it efs-dynamic -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Dynamic EFS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

 kubectl delete -f 08-efs-dynamic.yaml
 kubectl delete -f 07-efs-sc.yaml

Try creating again the data in EFS wiil be shown
kubectl apply -f 07-efs-sc.yaml
 kubectl apply -f 08-efs-dynamic.yaml
check : http://<LB_DNS_NAAME>:80

data is not coming which is working static EFS -- need to cross check

StatefulSet vs Deployment
==============
1. Deployment is for stateless applications, generally frontend and backend
2. StatefulSet is for stateful applications usually databases.
3. StatefulSet will have headless service, deployment will not have headless service
4. PV, PVC are mandatory for statefulset
5. pods in statefulset create in orderly manner with static names. statefulset-0, statefulset-1
-------------------------------------------------------------------------------------------------------------------------------------------------
zsession 53
-------------------------------------------------------------------------------------------------------------------------------------------------
Deployment
==========
kubectl apply -f 17-deployment-service.yaml

kubectl get svc
kubectl get deployment
kubectl get rs or kubectl get replicaset
kubectl get pods
kubectl get pods -o wide 

kubectl exec -it podname -- bash

when you hit nslookup on service name, you will get ClusterIP as address

apt update -y
apt install dnsutils -y
nslookup nginx


Statefulset
===========
You need to attach headless service to statefulset. Why?

what is headless service? headless service will have no ClusterIP. It will be attached to statefulset

PV and PVC are mandatory for statefulset

Create Dynamic EBS
----------------
1. install drivers
2. give permissions to worker nodes
3. create storage class

kubectl apply -f volumes/03-ebs-sc.yaml

kubectl apply -f 18-statefulset.yaml

kubectl get svc

	For statefulset, service will be created with ClusterIP as None

kubectl get pods

kubectl get pv,pvc

pods in statefulset will be created in orderly manner i.e., one by one not all at a time like in Deployment
deployment pods names are choosen as random. but statefulset pods names are unique. <statefulset-name>-0, <statefulset-name>-1

For deployment pods will be created with the format below

<Deployment_Name>-<RS_RandomNumber>-<Pod_RandomNumber>

For statefulset pods will be created with the format below

<statefulset-name>-0, <statefulset-name>-1

why statefulset have same names, pods preserve their identity?

stateful set creates pods with same old names after deletion

nginx-0 delete pod, 
statefulset creates immediately another pod nginx-0  with same name and it should attach to its own storage through naming convention. persistentvolumeclaim/www-nginx-0
i.e., persistentvolumeclaim/<volumeName>-<PodName>

kubectl delete pod nginx-o

after deletion new pod will be created and will be attached to the same volume

kubectl exec -it nginx-0 -- bash

apt install dnsutils -y
nslookup nginx-headless

kubectl get pods -o wide 

nslookup will give pod ip address not ClusterIP address which is shown for deployment
Giving pod ip address on nslookup is exactly required for databases
When create request comes to one worker node,to sync that data to other worker nodes in the cluster it should know the other nodes available in the cluster.
To get other nodes info it will nslookup on headless service

So headless service is required in statefulset so that  it can update to other nodes using pod ipadresses.


Deployment vs Statefulset
============================
1. Deployment is for stateless applications like frontend, backend, etc. Statefulset is for DB related applications like MySQL, ELK, Prometheus, Queue apps, etc.
2. Statefulset requires headless service and normal service to be attached. Deployment will not have stateless service
3. PV and PVC are mandatory to statefulset, they create individual storages for each pod. PV and PVC for deployment creates single storage
4. Statefulset pods will be created in orderly manner. Deployment pods will be created parellely
5. Deployment pods names are choosen random. Statefulset pods keep the same identity. like <statefulset-name>-0, -1, etc.

For statefulset pods names will be unique not random because each pod in statefulset have seperate storage i.e., PV and PVC and
if a pod is deleted it will be recreated with same old name not random name so that new pod can attach to the same storage/volume again

kubectl delete -f 18-statefulset.yaml

Statefulset requires normal service also for load balancing and  headless service  for clustering

Headless service and Normal service are same except headless service has clusterIP: None

kubectl delete -f volumes/03-ebs-sc.yaml

kubectl delete -f 18-statefulset.yaml

kubectl delete pv,pvc --all -A

# If we delete the PV and PVC  with below command
#   kubectl delete pv,pvc --all -A

# New Volumes will be created again

kubectl apply -f volumes/03-ebs-sc.yaml
kubectl apply -f 18-statefulset.yaml

# If we delete only storage class and statefulset new volumes are not created

cd 21-k8-expense-volumes  --- expense project using volumes
=========================
kubectl apply -f 01-namespace.yaml

Create Dynamic EBS
----------------
1. install drivers
2. give permissions to worker nodes
3. create storage class

mysql
------
  kubectl apply -f manifest.yaml 

use kubens  to set namespace

k9s installation
----------------
https://github.com/derailed/k9s

curl -sS https://webinstall.dev/k9s | bash  --- command to install k9s

Open new tab and Type k9s 

Type shift+:(pod or service or namespace)

Type shift+:+pod and then 
type s to get pod shell access
type l to get pod logs


mysql -u root -pExpenseApp@1
show databases;
use transactions;
show tables;

backend
--------
  kubectl apply -f manifest.yaml 

frontend
--------
  kubectl apply -f manifest.yaml  

Mount nginx.conf file as volume using ConfigMap  

Autoscaling
=============
Vertical, Horizontal

Vertical scaling --> Increasing resources in of same server---> 2CPU, 4GB, 20GB HD --> 4CPU 8GB 100GB HD
Horizontal scaling --> create another server with 2CPU, 4GB, 20GB HD and add to LB

There is down time in Vertical scaling and no downtime in horizontal scaling

4 individual houses, 

HPA in K8
------------
HPA - Horizontal Pod Autoscaler
We should metrics server installed in k8

kubectl top pod

kubectl top pod -n expense

kubectl top hpa -n expense

while true;do curl <ALB_URN>;done  --- this will not send parallel requests

Apache Bench
------------
Tool  to send parallel requests

sudo yum install httpd-tools -yaml
ab --help
ab -n 50000 -c 100 <ALB_URL>  --- / at the end is required


kubectl apply -f 01-namespace.yaml
kubectl apply -f 03-ebs-sc.yaml
kubectl apply -f 01-mysql/manifest.yaml
kubectl apply -f 02-backend/manifest.yaml
kubectl apply -f 03-frontend/manifest.yaml 

kubectl delete -f 01-mysql/manifest.yaml
kubectl delete -f 02-backend/manifest.yaml
kubectl delete -f 03-frontend/manifest.yaml 
kubectl delete -f 03-ebs-sc.yaml

kubectl delete pv,pvc --all -A
If we delete the PV and PVC  with below command
  kubectl delete pv,pvc --all -A

New Volumes will be created again

If we delete only storage class new volumes are not created
and pods will connect to existing volumes

Test the app 
http://<LB_URN>:80/

ab -n 50000 -c 100 http://afdb66a07d5594006b4825856aa3011c-210927200.us-east-1.elb.amazonaws.com/
-------------------------------------------------------------------------------------------------------------------------------------------------
session 54 -- HELM PENDING
-------------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------------
session 55
-------------------------------------------------------------------------------------------------------------------------------------------------
Selectors
==============
Scheduler --> it will decide where to run your pod

You can use any of the following methods to choose where Kubernetes schedules specific Pods:

nodeSelector field matching against node labels
Affinity and anti-affinity
nodeName field
Pod topology spread constraints

nodeSelector:
	az: 1a
	
taints and tolerations
affinity and anti-affinity

kubectl apply -f 01-node-selector.yaml

If workes nodes are in 1a and 1b zone but the nodeselector is given as 1c then
Pod will not create and it will be in Pending status

kubectl delete -f 01-node-selector.yaml

Errors in K8
==============
ErrImagePull --> if node is not able to pull the image
CrashLoopBackOff --> Container is unable to start
Pending --> worker node not availalbe in that AZ, PVC is not bound to PV
ContainerCreating --> PV, PVC

taint --> paint or pollute

if we taint the node, scheduler will not schedule any pods on to that node

kubectl get nodes
kubectl get nodes --show-labels
kubectl taint nodes ip-192-168-43-212.ec2.internal hardware=gpu:NoSchedule

kubectl apply -f 01-taint-node-selector.yaml

17.182 --> 1b
43.212 --> 1a and tainted
51.139 --> 1a

NoSchedule --> order  -- will not schedule any pods in future and cureent pods will be running
PreferNoSchedule --> request
NoExecute --> taint the node which has already few pods running -- deletes the existing pods

kubectl get pods -o wide
kubectl taint nodes ip-192-168-43-239.ec2.internal hardware=gpu:NoExecute
kubectl get pods -o wide

Worker nodes are tainted when dedicated worker nodes are required 

toleration --> allow -- the pods to run on tainted nodes

kubectl apply -f 02-toleration.yaml

command to get the nodes with taints

kubectl get nodes -o json | jq '.items[].spec'
kubectl get nodes -o json | jq -r '.items[] | select(.spec.taints) | .metadata.name'

a dedicated worker nodes are there project savings bank project, it means any other project related pods will not be scheduled here

tolerations will not 100% guarantee that pods will run on the tainted nodes.. 
because of resources and other pods in the node

try schedule the pods to tainted nodes without toleration configuration then
the pod will not work and it will be in Pending status

kubectl apply -f 03-tolerations.yaml

Untaint nodes
--------------
kubectl taint nodes ip-192-168-43-212.ec2.internal hardware=gpu:NoSchedule-
kubectl taint nodes ip-192-168-43-239.ec2.internal hardware=gpu:NoExecute-

taint the nodes again and remove nodeSelector configuration

kubectl taint nodes ip-192-168-43-212.ec2.internal hardware=gpu:NoExecute
kubectl taint nodes ip-192-168-43-239.ec2.internal hardware=gpu:NoExecute

kubectl apply -f 04-tolerations.yaml

Pod will be scheduled to other untainted node since there is no nodeSelector configuration

Affinity
--------
tolerations will not 100% guarantee that pods will run on the tainted nodes.. 
because of resources and other pods in the node 

pod may run in other node

to force the pod to run on specific node we ill use affinity

nodeSelector vs affinity
-------------------------
nodeSelector is one option but affinity has more options than nodeSelector

nodeSelector has only key value pair but affinity has multiple options
You can use the operator field to specify a logical operator for Kubernetes to use 
when interpreting the rules. You can use In, NotIn, Exists, DoesNotExist, Gt and Lt.

Node affinity is conceptually similar to nodeSelector, 
allowing you to constrain which nodes your Pod can be scheduled on based on node labels.

kubectl apply -f 05-node-affinity.yaml

kubectl get pods

Pod will be in Pending status Add the labels to the nodes

Node Labels
-----------
kubectl get nodes
kubectl label nodes <Node_Name> hardware=gpu

kubectl label nodes ip-192-168-32-164.ec2.internal hardware=gpu
kubectl label nodes ip-192-168-33-103.ec2.internal hardware=gpu

After adding label the pod will be in running status

kubectl get node

situation
==========
node is tainted --> can scheduler run the pod on that node? --> no
node is tainted --> pod asks for toleration --> can scheduler run the pod --> may be(if resources are not free or scheduler decided another node) --> Running
node is tainted --> pod asks for toleration and nodeSelector --> If tainted have resources(Running) --> if tainted nodes don't have any resources(Pending)

requiredDuringSchedulingIgnoredDuringExecution --> Schedule the pod and execute --> hard rules, labels must be availalbe while scheduling

If pod labels are changed between scheduling and execution scheduler will ignore the changes at the time of execution

preferredDuringSchedulingIgnoredDuringExecution --> Soft rule --> if labels are not availalbe then consider schedule on the node..

AntiAffinity
------------
opposite to affinity uses NotIn operatori.e., schedules the pod in the node which doesn't match criteria

kubectl apply -f 06-anti-affinity.yaml

podAffinity
-----------
In pod affinity both the pods will run in the same node
Pod affinity will be used for Cache

kubectl apply -f 07-pod-affinity.yaml
kubectl get pods -o wide

podAntiAffinity
---------------
In pod antiaffinity both the pods will not run in the same node

kubectl apply -f 08-pod-affinity.yaml
kubectl get pods -o wide

backend --> DB 
backend --> Cache --> DB --> Cache -> 10 days rice store

backend --> node-1
Cache --> node-2

traffic should come out from node-1 and enter into node-2 and then Cache pod

pod-1 --> 17.182
pod-2 --> 

Ingress Controller
===================
Classic --> Old generation
ALB --> New generation --> host based routing

LoadBalancer service in kubernetes will create classic Load Balancer which will not support advanced rules
so we will use ingress contoller which provides new features similar to ALB

netbanking.hdfcbank.com --> netbanking target groups
sms.hdfcbank.com --> sms banking target group

url path hdfcbank.com/netbanking --> netbanking

by using ingress controller, we can expose application running in K8 to outside world.

setup ingress controller
use ingress resources --> routing rules

Ingress controller is called as AWS Load Balancer Controller

https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/deploy/installation/

-------------------------------------------------------------------------------------------------------------------------------------------------
session 56
--------------------------------------------------------------------------------------------------------------------------------
R53 --> ALB --> Listener --> Rule --> Target Group(VM)/Target Group(Pod)

Ingress controller is called as AWS Load Balancer Controller in AWS

https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/deploy/installation/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/ingress/annotations/

Steps to create ingress controller in AWS
--------------------------------------------
eksctl utils associate-iam-oidc-provider \
    --region <region-code> \
    --cluster <your-cluster-name> \
    --approve

Install Dirvers and Set IAM policy 
Refer ReadMe.MD file	in 25-k8-ingress repo

To check Load balancer drivers which will create,manage and automate Load Balancers

kubectl get pods -n kube-system
	
daws82s.online

app1.daws82s.online --> app1 related pods
app2.daws82s.online --> app2 related pods

Ingress resource
=================
Ingress Controller is used to provide external access to the applications running in kubernetes.
 We can set the routing rules through ingress resource either path based or host based. 
 in EKS ingress resource can create ALB, Listener, Rule and target group. 
 Ingress is attached to service, so it fetch the pods and add them to target group

app1(25-k8-ingress/app1)
------------------------
Build image for app1 and push to docker hub

docker build -t dath1/app1:1.0.0 .
dokcer login -u dath1
docker push dath1/app1:1.0.0

set the annotaions in ingress to create LB
create certificate for the domain  *.raj82s.online and copy the certificate arn for https

kubectl apply -f manifest.yaml
kubectl get ingress

ADD dns record for the *.raj82s.online with LB DNS name

app2(25-k8-ingress/app2)
--------------------------
Build image for app2 and push to docker hub

docker build -t dath1/app2:1.0.0 .
dokcer login -u dath1
docker push dath1/app2:1.0.0

Ingres will check if there is LoadBalancer in joindevops group and 
it will not create LoadBalancer again
it will just add the rules

kubectl apply -f manifest.yaml
kubectl get ingress
 

RBAC --> Role based access control
======
Authentication and Authorization

Nouns and Verbs

Nouns --> what are the resources we have
Verbs --> What actions you can take on those resources

Role and RoleBinding used to provide access to namespace level resources
ClusterRole and ClusterRoleBinding used to provide access to cluster level resources

IAM
Nouns --> EC2, VPC, R53, CDN, EKS, IoT, etc.

Fresher --> deleteEC2 --> no
CRUD
Fresher --> readEC2
Junior --> createEC2
Senior --> creaEC2, readEC2, updateEC2
Team lead --> deleteEC2

User, Role, Rolebinding

expense-trainee --> read access to expense namespace
role should be bound to user --> through rolebinding
EKS is one platform, AWS is another platform
k8 has its own RBAC, AWS has its own

AWS integrates IAM service as authentication mechanism to EKS

a user should describe EKS to connect it..

Create EKS describe policy for user  suresh and also create user

1. create IAM user and provide describeEKSCluster access
2. we need to create role and rolebinding resources
3. aws-auth configmap need to be configured to connect EKS and IAM


Create IAM User suresh
-----------------------
create IAM policy with "ExpenseEKSDescribe" name in AWS
Add the EKS services and provide the "DescribeCluster" permissions/actions user can perform
	Provide Region
	Provide cluster name
	ARN "arn:aws:eks:us-east-1:311141550186:cluster/expense" added automatically
create suresh user
Attach IAM policy to the user
create access key for suresh account
create new EC2 instance for user suresh and follow the steps below
	aws configure
	aws eks update-kubeconfig --region us-east-1 --name expense  -- update aws eks config to get cluster access
	aws sts get-caller-identity
	Install kubectl -- to interact with cluster

26-k8-rbac  -- project setup

kubectl apply -f 01-role.yaml

kubectl get role
kubectl get rolebinding

Edit aws-auth configmap( Grant IAM Users access to kubernetes with configMap)
-----------------------
kubectl get configmap aws-auth -n kube-system -o yaml

copy the output to aws-auth.yaml file and add AWS suresh user and ARN configurtaion to map EKS user with AWS user

https://docs.aws.amazon.com/eks/latest/userguide/auth-configmap.html  -- edit aws auth configmap

kubectl get configmap aws-auth -n kube-system -o yaml

mail suresh about his config is done, he can login

Create new Ec2 instance and follow the below steps

aws configure
aws configure --profile suresh
aws sts get-caller-identity
aws sts get-caller-identity --profile suresh  ---to check the user

https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html -- aws eks update kubeconfig to get cluster access

to get cluster access update aws eks config use the below command
-----------------------------------------------------------------
aws eks update-kubeconfig --region us-east-1 --name expense

cat .kube/config 

check the access

kubectl get pods
kubectl get pods -n kube-system
kubectl get nodes 
kubectl get services

Changes the resources access to * to access all resources

-------------------------------------------------------------------------------------------------------------------------------------------------
session 57
--------------------------------------------------------------------------------------------------------------------------------
Role and RoleBinding used to provide access to namespace level resources
ClusterRole and ClusterRoleBinding used to provide access to cluster level resources

Follow the same steps used for role and added cluster role config to access cluster level resources
 uid in 02-aws-auth.yaml is optional
kubectl api-resources

give cluster level access for nodes
kubectl apply -f 03-clusterrole.yaml

ServiceAccount
================
Service Account:  default
when you create namespace or every namespace will have default sa
sa is a non human account, it is pod identity with which it can connect with api server and get access to external services

kubectl apply -f 01-namespace.yaml
kubectl apply -f 02-pod.yaml
kubectl get pods -n expense 
kubectl describe pods nginx -n expense -- check there is default service account

Pod will create LB,rules and targetgroups in Ingress using service accounts.

pod should access aws secret manager
-------------------------------------
Kuberentes secrets are encoded but not encrypted
AWS secrets are encrypted

1. make sure oidc provider exist
eksctl utils associate-iam-oidc-provider \
    --region us-east-1 \
    --cluster expense \
    --approve

2. create secret and policy in AWS
2a)create secret "expense/mysql/creds" in AWS with key value pair
   key		value
   ----------------
   username	admin
   password	admin123
   
2b)Create IAM policy "ExpenseMySQLSecretRead" with "SecretManager" service with read "GetSecretValue" Permission and 
give access to specific resource i.e., secret created

	Edit ARN and provide below
		Resource region
		copy and paste ARN of secret created in ARN field
		secret field will be automatically filled
3. we need to map sa with IAM policy(Update with kubernetes namespace and policy arn created in aws in below command)
	For internal resources create role and roldebinding for service account
	For external resources access attach IAM policy to service account
	
eksctl create iamserviceaccount \
--cluster=expense \
--namespace=expense \
--name=expense-mysql-secret \
--attach-policy-arn=arn:aws:iam::311141550186:policy/ExpenseMySQLSecretRead \
--override-existing-serviceaccounts \
--region us-east-1 \
--approve

this command will create IAM role with policy ExpenseMySQLSecretRead and integrate with EKS SA

kubectl get sa -n expense
kubectl describe sa expense-mysql-secret -n expense
kubectl get sa expense-mysql-secret -n expense -o yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::315069654700:role/eksctl-expense-addon-iamserviceaccount-expens-Role1-XUPebrTfkrjM
  creationTimestamp: "2025-03-17T02:22:46Z"
  labels:
    app.kubernetes.io/managed-by: eksctl
  name: expense-mysql-secret
  namespace: expense
  resourceVersion: "7864"
  uid: 8c97086a-9360-4a82-b77f-142fe47ce1c2
  
Verify Role "eksctl-expense-addon-iamserviceaccount-expens-Role1-XUPebrTfkrjM" will be created in AWS and 
attached to Service Account  

InitContainers
==================
InitContainers are used for setup the requirements for pod, for example backend pod can check whether database connections working fine or not
InitContainers containers can fetch the secrets for pod before it starts

InitContainers are special containers run before main container runs, we can use them to make sure dependent services are running fine before our main application starts and fetch the secrets from secretmanager for main container. they always run to completion. you can run one or many init containers, they executed in sequence

InitContainers can do heavy lifting for main container so that main container is less in size and limit attack surface of main container by not installing more tools in it..

kubectl apply -f 20-init-container.yaml

aws secretsmanager get-secret-value \
    --secret-id expense/mysql/creds --query SecretString --output text
	
Create Pod to get secretes with init contianer	

volumes --> external volumes --->EBS,EFS and configMap as volume

internal volumes --> emptyDir and hostPath

emptyDir --> pod temporary storage, accessible until pod lives. all containers in the pod can access
init container stores the secret in emptyDir and completes. then main container can acess this

kubectl apply -f 19-sa.yaml
kubectl get pods -n expense
kubectl exec -it sa-demo -n expense -- bash
cat /secrets/.env

To check the pods access running in default namespace from the pod in expense namespace after connecting to sa-demo pod created using 19-sa.yaml
-------------------------------------------------------------------------------------------------
kubectl exec -it sa-demo-2 -n expense -- bash
Install kubectl
kubectl get pods
kubectl get pods -n default

kubectl get secrets
kubectl get secrets -n default

you will get permission error to access this again we need to use sa with role and rolebinding
exit the pod

Not only to users, role and rolebinding can be created for ServiceAccount also

kubectl get sa expense-mysql-secret -n expense -o yaml

Add role and role binding with Kind as ServiceAccount in 21-sa.yaml
kubectl apply -f 21-sa.yaml
kubectl exec -it sa-demo-2 -n expense -- bash
Install kubectl
kubectl get pods
kubectl get pods -n default

kubectl get secrets
kubectl get secrets -n default


what is serviceaccount?

serviceaccount is non human user account, sa is the pod identity with which it can access internal resources or external services. 
we can map sa with cloud provider IAM roles and policies. we can use sa to fetch secrets from secretmanager

what are initcontainers?

these are special containers run before main container run. 
we can run one or many init containers, all containers run in sequence. 
init containers will come to completion, we use them to check the dependencies are running fine before main container starts 
another example can be fetching the secrets and provide to main container.  
they do heavy lifting keeping the main container lightweight and 
less attack surface by not installing utility tools in main container.

what is emptyDir?
it is pod temporary storage exist until the pod lives, all containers inside pod can acess emptyDir volumes.

OIDC provider
sa --> IAM Role --> IAM Policy (external roles)
sa --> Role and RoleBinding (Internal resources access)

------------------------------------------------------------------------------------------
CrashLoopBackoff - unable to create pod from image i.e., unable to keep the pod running continuously
ErrorImagePull - unable to pull the image
Pending -- pod in pending status if pod and volume are in different zones AZ.

Dynamic EBS and EFS both are not working
   EBS creating new volume everytime
   EFS deleting access  points and creating new 

on what basis mapping between EKS and AWS user done?Is it username?   







