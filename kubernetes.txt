eksctl  -- tool to install cluster
kubectl
nodes
namespace
pod
labels
annotations
env
resources
configMapRef
secrets
services
3 types of services
	1. cluster IP --> default, only works internally i.e., pod to pod
	2. NodePort --> external requests
	3. LoadBalancer --> only works with cloud providers. here service open classic LB. 
replicaset - can be used for creating multiple pods
deployment -- to update the image in replicaset
kubens -- tool to set namespace
volumes 
EBS
  static
  dynamic

https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/install.md -- link for ebs csi driver install using kustomzie
PV -- Pv is wrapper object in kubernetes to represent physical volume like EBS or EFS
PVC
Storageclass -- Dynamic EBS -- creates PV and volume automatically no need to create manually
api-resources
1. namespace level
2. cluster level
EFS
  static
  dynamic


eksctl -- AWS command line tool to create and manage EKS cluster 
For real projects terraform is used to create EKS cluster
eksctl installation -- For cluster creation
kubectl installation  -- For cluster  interaction
We wiil use same EC2 instance used for docker to setup eksctl,kubectl

https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html
https://eksctl.io/installation/

https://github.com/eksctl-io/eksctl/blob/main/examples/01-simple-cluster.yaml

sudo tail -f /var/log/cloud-init-output.log -- to check user data output

https://kubernetes.io/  -- official doc and can be used for CKA exam

1. create one linux server as workstation
2. install docker to build images
3. run aws configure to provide authentication
4. install eksctl to create and manage EKS cluster
5. install kubectl to work with eks cluster

eksctl installation
-------------------
curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.32.0/2024-12-20/bin/linux/amd64/kubectl.sha256
chmod +x ./kubectl
sudo mv kubectl /usr/local/bin/kubectl

# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH
curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
# (Optional) Verify checksum
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo mv /tmp/eksctl /usr/local/bin

From ExampleConfigs select simple cluster

https://github.com/eksctl-io/eksctl/blob/main/examples/01-simple-cluster.yaml

eks.yaml in 16-Docker -- To create cluster

eksctl create cluster --config-file=eks.yaml
eksctl delete cluster --config-file=eks.yaml

aws configure

AWS cloudformation -- Just like terraform infra as code - AWS native service - will create resouces for cluster

kubectl get nodes -- to get the no of nodes in the cluster

namespace 
---------- 
isolated project space similar to VPC where you can control resources to your project

default namespace is created along with cluster creation

kubectl get namespace -- to get the namespaces

Kind -- kind of resource
apiVersion  -- kubernetes version
metadata 
  name -- resource name
  
spec used for other resouces 

kubectl create -f 01-namespace.yaml -- will throw error if the resource alrady exists
kubectl apply -f 01-namespace.yaml  -- will throw warning not error if the resource alrady exists
kubectl delete -f 01-namespace.yaml -- delete the resource


pod
====
docker --> image --> container
k8s    --> image --> pod

pod vs container
----------------
pod is the smallest deployable unit in k8s .pod can have multiple containers
all containers in pod share the same ip and storage
multiple containers are useful in few applications like shipping the logs through sidecars

kubectl apply -f 02-pod.yaml
kubectl get pods -- to get the list of pods
kubectl get pods -n <namespace> -- to get the list of pods in namespace

kubectl exec -it <pod-name> -- bash
kubectl exec -it nginx -- bash  # To connect to pod

To connect pod in expense namespace
kubectl exec -it nginx -n expense -- bash

kubectl apply -f 03-multicontainer.yaml
kubectl get pods
CrashLoopBackOff -- error when failed to create container

kubectl describe pod <pod-name>  -- to get the details of pod like which container is failing  in a pod

# If one container failed we need to delete the pod and create it again
kubectl delete -f 03-multicontainer.yaml

kubectl exec -it <pod-name> -- bash

kubectl exec -it multi-containers -- bash

curl  localhost

By default it will connect to first container
To connect to second contianer

kubectl exec -it <pod-name> -c <container-name> -- bash

kubectl exec -it multi-containers -c almalinux -- bash

curl localhost

curl output will be same because "all conatiners in pod share the same ip and storage"

pod or container is ephemeral

when pod is deleted application logs created by first container will also get deleted 
so we will create second container(sidecar container) to push the logs to ELK to reduce the load on first container

labels
-------
labels are used to provide metadata and can be used to attach other resources

kubectl apply -f 04-labels.yaml

annotations
------------
annotations are similar to labels 
labels can be used to select internal resources 
annotations can be used to select external resources
also in annotations we can give special characters and used in ingress controller
special characters cannot be used in labels

kubectl apply -f 05-annotations.yaml

--------------------------------------------------------------------------------------------------------
env
To set environment variables
kubectl apply -f 06-env.yaml
kubectl exec -it env -- bash
# env

resources
 requests
 limits
sets the min and max RAM and CPU for container        
kubectl apply -f 07-resources.yaml
kubectl top pod  --- to check the resources consumed by pod

ConfigMap
  data
To provide configurations as env variables

kubectl apply -f 08-config-map.yaml
kubectl get configmap
kubectl describle configmap myconfig

Referring ConfigMap to set as env variables

envFrom
 configMapRef

 kubectl apply -f 09-pod-config.yaml
 kubectl exec -it pod-config -- bash  
 env
 
 Update config map data and restart pod
kubectl apply -f 08-config-map.yaml

 kubectl delete -f 09-pod-config.yaml
 kubectl apply -f 09-pod-config.yaml
 kubectl exec -it pod-config -- bash  
 env

secrets
 data
secret types
  opaque
  
$  echo -n "testsecret" | base64
dGVzdHNlY3JldA==

$ echo "dGVzdHNlY3JldA==" | base64 --decode
testsecret
kubectl apply -f 10-secret.yaml
kubectl get secret
kubectl describe secret dotfile-secret

Referring secret to set as env variables

envFrom
 secretRef

 kubectl apply -f 11-pod-secret.yaml
 kubectl exec -it pod-secret -- bash  
 env 
 
Service
=========
service is used for 
1. load balancing
2. pod to pod communication
3. as DNS between pods

Ip address of pod will changes after deletion and creation of pod
Instead of IP we will create service to access the pod or pod to pod communication with service name

service will be cretad with clusterip
kubectl delete pods --all --all-namespaces   -- to delete all pods
kubectl delete pods --all -A

kubectl get pods --all-namespaces
kubectl get pods -A

kubectl apply -f 12-service.yaml

kubectl get svc --- to get service info -- gives cluster IP and also node port

kubectl get pod -o wide -- to get pod ip and also the node where pod is running

start other container to check service access from other container

kubectl apply -f 03-multicontainer.yaml

kubectl exec -it multi-containers -- bash

then curl <servicename>

curl nginx

apt update
apt install dnsutils -y

nslookup nginx

multi-container --> curl nginx --> 
3 types of services
1. cluster IP --> default, only works internally i.e., pod to pod
2. NodePort --> external requests
3. LoadBalancer --> only works with cloud providers. here service open classic LB. 

clusterip will not work in internet, works only in kubernetes i.e., only works internally i.e., pod to pod

Node port
---------
Node port works from internet
Node port automatically creates cluster IP and creates new node port

kubectl delete -f 12-service.yaml
kubectl apply -f 13-node-port.yaml

kubectl get svc --- to get service info -- gives cluster IP and also node port

kubectl get pod -o wide -- to get pod ip and also the node where pod is running

check the private ip address of node from aws ui
copy  the node public ip address and access with node port from browser(open port in security group)

If we acess the node port with any node ip it will work because kubernetes will send the request to the correct pod

In NodePort type
when user send request with public ip(any instance/node public IP) ip:nodeport
then kubernetes will forward the request to cluster ip service
then cluster ip service will forward the request to the pod
When we open port in security group Kubernetes will open node port on all instances(nodes) so we the url will work with any instance ip and node port

# test with url
# http:<AnyInstancePublicIP>:<NodePort>

kubectl delete -f 13-node-port.yaml

kubectl apply -f 14-load-balancer.yaml

In Load Balancer type
request will come from user to Load balancer listening on port 80 
then LB will forward to node port to any one of the instances(nodes) in the cluster
then from any one of the instances(node) request goes to cluster ip
then finally request will be sent from cluster ip to the pod

# test with url
# http:<LB_DNS_Name>:<80>

# For LB service type no need to mention node port and open in secuirty group

# create Route53 record for LB DNS name with domain name to access the url with domain name

replicaset - can be used for creating multiple pods
Pod is subset of replicaset
pod name = replicaset-name-random 5 digit

To create multiple pods we have to change pod name and run the 02-pod.yaml file
metadata:
  name: nginx2
  
Instead of creatng multiple pods manually we can use replicaset 

 kubectl apply -f 15-replica-set.yaml

kubectl get replicaset

kubectl get pods -o wide

watch kubectl get pods # in new terminal

if we delete one pod, replicateset will automatically creates new pod

If we run 02-pod.yaml again with same image nothing will change
If we change image name and run 02-pod.yaml file image will be updated

check the image with below command
kubectl describe pod nginx -n expense

If image is changed in replicaset with another version ,replicaset will not update the image

To achieve this we need to use deployment

Deployment 
----------
Deployment yaml is same as replicaset except that kind is Deployment

kubectl apply -f 16-deployment.yaml

kubectl get deployment

kubectl get rs or kubectl get replicaset

kubectl get pods

To verify the replicaset updates the pods with another image version
change the image name from nginx:1.14.2 to nginx:latest

kubectl apply -f 16-deployment.yaml

check deployment with image and service
---------------------------------------
docker build -t dath1/deployment:1.0 .
docker push dath1/deployment:1.0

then run the 17-deployment-service.yaml to test the deployment

Test with url http://<LB_DNS_NAME>:80

change the verion in index.html and build the image again to  test the deployment

docker build -t dath1/deployment:2.0 .
docker push dath1/deployment:2.0

build the image from DEPLOYMENT folder

kubectl apply -f 17-deployment-service.yaml
kubectl get svc
kubectl get deployment
kubectl get rs or kubectl get replicaset
kubectl get pods

Test with url http://<LB_DNS_NAME>:80

To verify the replicaset updates the pods with another image version
change the image name from 1.0 to 2.0

kubectl apply -f 16-deployment.yaml

Test with url http://<LB_DNS_NAME>:

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

Commands to check deployment status and rollback
-------------------------------------------------
kubectl rollout history deployment/nginx  -- to check the deployment history
kubectl rollout history deployment/nginx --revision=2  -- to check revision 2 history

kubectl rollout undo deployment/nginx --> roll back to immidiate previous version
kubectl rollout undo deployment/nginx --to-revision=2 ---> roll back to specific verison

kubectl get deployment nginx   -- to check the deployment status
kubectl rollout status deployment/nginx  -- to check the deployment status

Expense project using k8s
--------------------------
To avoid mentioning the namespace name (-n namespacename) in  kubectl commands install the tool kubens

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

kubens <namespace_name>  -- set the name of namespace
kubens expense

kubectl apply -f 01-namespace.yaml

mysql
kubectl apply -f manifest.yaml 

kubectl get pods -n expense
kubectl exec -it mysql-djshdsh -n expense -- bash
mysql -u root -pExpenseApp@1
show databases;
use transactions;
show tables;
   

backend
kubectl apply -f manifest.yaml    

frontend
change port 80 to 8080 in nginx.conf file in Dockerfile and build the image in 18-expense-docker repo
remove /etc/nginx/conf.d/default.conf file which has port 80

docker build -t dath1/frontend:1.1.0 .
docker push dath1/frontend:1.1.0

kubectl apply -f manifest.yaml   

kubectl logs frontend-54c88dc9ff-8xdnk 

kubernetes won't allow system ports for non root user
0-1024 are system ports -- container has to run with root user to access system ports
Non root users cannot access this ports

Either change the user to root or change the port in frontend nginx.conf from 80 to 8080

docker build -t dath1/frontend:1.2.0 .
docker push dath1/frontend:1.2.0

Test the app with url
http://<LB_DNS_NAME>:80

add route 53 record for LB DNS NAme and test the url with dns name

Volumes
============
everything in k8 is ephemeral. pods, nodes, master node all are ephemeral. So it is not good to store DB inside K8

Storage administration --> manage all the disks related to servers
Backup every day
Restore testing
N/w to storage

EBS -> Elastic block storage  like harddisk
EFS -> Elastic file system    like google storage

If server is in AZ us-east-1b, EBS also should be here. EBS is less latency. EBS is well suitable for DB and OS

conditions for EBS
---------------------------
1. node and EBS volume should be in same AZ
2. drivers install( to access EBS from EKS)
3. EC2 worker nodes should have permission to work with EBS volumes

static and dynamic provisioning
=================================
static means we have to create disk(volume) manually
Dynamic means EKS will create the Disk(volume)

EBS static
==========
1.we have to create disk(volume) manually in same AZ as in worker nodes 
2.drivers install(ebs csi driver) using kustomize
	https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/install.md
	kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.40"
	
	kubectl get pods -n kube-system --  check the drivers installed as pods	

3.select the worker node security tabs
	click on Iam roles
	Add permission and Attach "AmazonEBSCSIDriverPolicy" to manage volumes

4.Mount the volume to the pod using PV and PVC
	
everything in k8 is resource/object. if you can treat or manage the volumes from k8 objects any k8 engineer can work on this..

PV --> Persistant volume, represents or wrapper of physical volume
PVC --> Claiming the volume
Storage class

PV is wrapper object in kubernetes to represent physical volume like EBS or EFS

kid --> mother --> father --> wallet
Pod	-->  PVC --> PV -->  volume

https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/static-provisioning/manifests/pv.yaml  -- for creating PV

access modes
============
ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod,

ReadWriteOnce - At a time only one node can read and write with multiple pods

ReadWriteOncePod -- for only one pod

Lifecycle policies(i.e., what will happen to data if the pod is deleted)
=================

Retain -- data and volume will not delete
delete -- data and volume will be deleted
recycle -- data will delete but not volume

kubectl apply -f 01-ebs-static-pv.yaml
kubectl get pv

create PVC  -- to claim the volume
https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/static-provisioning/manifests/claim.yaml -- ex to create PVC

kubectl apply -f 02-ebs-static-pvc.yaml
kubectl get pvc

Pod should claim that PVC

all the files/data of pod stored in /usr/share/nginx/html will be stored in volume

kubectl get pods

kubectl get nodes --show-labels  ----> to get nodeSelector label

if pod schedules(created) in 1a and volume is in 1b, pod status will be in pending for continuously.

scheduler(nodeSelector) --> schedules your pods on to appropriate nodes

kubectl apply -f 02-ebs-static-pvc.yaml

kubectl exec -it ebs-static -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from EBS static</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

Delete all resources(PV,PVC,pod,service)
kubectl delete -f 02-ebs-static-pvc.yaml
kubectl delete -f 01-ebs-static-pv.yaml

all resources got deleted but not volume and data
try recreate the pod and verify the data in volume is not deleted

kubectl apply -f 01-ebs-static-pv.yaml
kubectl apply -f 02-ebs-static-pvc.yaml

check : http://<LB_DNS_NAAME>:80

permanent storage
=================
1. SAN - Storage Area Network
2. K8 Admin
3. Expense DevOps Engineer --> they get access to only expense namespace

Expense DevOps engineer send a mail to SAN with the manager approval. SAN will take their manager approval and create storage for us.

We will forward disk details to k8 admin and ask them to create PV.

In kubernetes there are 2 type of resources

kubectl api-resources

1. namespace level
2. cluster level -- we will not have access to create clusterlevel resources so we will send request to create PV which is cluster level resource

they create PV and send the name for us..

then we will create PVC, pod to access the storage

Kid --> Mom --> Dad --> Wallet
Pod --> PVC --> PV  --> Storage

EBS Static
============
1. install drivers
2. give permissions to worker nodes
3. create volume
4. create PV, PVC and cliam through pod
5. EC2 and EBS should be in same az incase of EBS provisioning

kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.40"  -- driver install

watch kubectl get pods # to monitor pods

EBS dynamic
============

Kid --> Mom --> UPI
Pod --> PVC --> SC

StorageClass --> this object is responsible for dynamic provisioning. it will create external storage(volume) and PV automatically

kubectl get sc

1. install drivers
2. give permissions to worker nodes
3. create storage class

https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/dynamic-provisioning/manifests/storageclass.yaml -- sc example

kubectl apply -f 03-ebs-sc.yaml
kubectl get sc

 StorageClass should be empty for static provisionin(EBS Static)
 Provide StorageClass name for dynamic provisioning(EBD Dynamic)
kubectl apply -f 04-ebs-dynamic.yaml

kubectl get pvc
kubectl get pv
kubectl get pods
kubectl get svc

check : http://<LB_DNS_NAME>:80   --- no output

kubectl exec -it ebs-dynamic -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Dynamic EBS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

kubectl apply -f 04-ebs-dynamic.yaml

Volume will not delete we have to delete it manually

EFS --> Elastic file system
-----------------------------
1. EBS should be in same AZ as in EC2, EFS can be anywhere in n/w
2. EBS is speed compare to EFS
3. EBS is used OS disk and databases, EFS can be used for file storages
4. EBS size is fixed. EFS will be scaled automatically
5. EFS is based on NFS(Network File Sharing) protocol. 2049 port
6. EBS and EFS should be mounted to any instance to use.
7. You can have any filesystem attached to EBS, but EFS use NFS, we can't change.

EFS static
=============
1. install drivers(Link - https://github.com/kubernetes-sigs/aws-efs-csi-driver)
kubectl apply -k \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-2.1"
	
   	kubectl get pods -n kube-system --  check the efs csi drivers installed as pods	
	
2. give permissions
    select the worker node security tabs
	click on Iam roles
	Add permission and Attach "AmazonEFSCSIDriverPolicy" to manage volumes
3. create EFS
4. open port no 2049 on EFS to allow traffic from EC2 worker nodesi.e., add worker node security group to EFS security inbound rules with NFS protocol
5. create PV, PVC and POD

https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/specs/pv.yaml -- ex for pv

ex for PV
https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/specs/pv.yaml

kubectl apply -f 05-efs-pv.yaml
 
kubctl get pv


 kubectl apply -f 06-efs-pvc.yaml
 kubectl get pvc   
 kubectl get pods

kubectl exec -it efs-static -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Static EFS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

 kubectl delete -f 06-efs-pvc.yaml
 kubectl delete -f 05-efs-pv.yaml

Try creating again the data in EFS wiil be shown
kubectl apply -f 05-efs-pv.yaml
 kubectl apply -f 06-efs-pvc.yaml

check : http://<LB_DNS_NAAME>:80


EFS Dynamic
=============
1. create storage class
2. install drivers
3. give permissions

# For Dynamic EFs also we need to create EFS and provide the ID in fileSystemId

kubectl apply -f 07-efs-sc.yaml
kubectl get sc

# kubectl apply -f 08-efs-dynamic.yaml
# kubectl get pv
# kubectl get pvc

For Dynamic EFS  access points will be created

kubectl get pods

kubectl exec -it efs-dynamic -- bash
cd /usr/share/nginx/html
echo "<h1>Hello from Dynamic EFS Volume</h1>" > index.html

check : http://<LB_DNS_NAAME>:80

 kubectl delete -f 08-efs-dynamic.yaml
 kubectl delete -f 07-efs-sc.yaml

Try creating again the data in EFS wiil be shown
kubectl apply -f 07-efs-sc.yaml
 kubectl apply -f 08-efs-dynamic.yaml
check : http://<LB_DNS_NAAME>:80

data is not coming which is working static EFS -- need to cross check

StatefulSet vs Deployment
==============
1. Deployment is for stateless applications, generally frontend and backend
2. StatefulSet is for stateful applications usually databases.
3. StatefulSet will have headless service, deployment will not have headless service
4. PV, PVC are mandatory for statefulset
5. pods in statefulset create in orderly manner with static names. statefulset-0, statefulset-1
------------------------------------------------------------------------------------------
CrashLoopBackoff - unable to create pod from image i.e., unable to keep the pod running continuously
ErrorImagePull - unable to pull the image
Pending -- pod in pending status if pod and volume are in different zones AZ.





